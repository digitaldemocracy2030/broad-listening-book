# コラム：「AI選択の自由」という新しい人権

文責：tokoroten

表現の自由、知る権利、そして次に来るのは「AIを選ぶ自由」かもしれません。私たちの思考を形作るAIを、誰が、どの価値観で作るのか。それは新しい人権問題です。


## 左を向くAI

複数の研究により、現在の主要なLLM（GPT、Claude、Gemini等）はリベラル寄りの政治的バイアスを持つことが指摘されています。

2025年にスタンフォード大学のAndrew Hall教授らが発表した研究[^1]では、8社24種類のLLMに30の政治的質問を行い、1万人以上の米国人回答者にその回答の政治的傾向を評価させました。結果、30問中18問において、ほぼすべてのLLMの回答が左派寄りと認識されました。この傾向は共和党支持者・民主党支持者の双方から指摘されています。特にOpenAIのモデルは最も強い左派傾向があると認識され、Googleの4倍の偏りがあると評価されました。

また、Pew Research Centerの政治類型テストを用いた2025年の比較研究[^2]でも、ChatGPT-4、Claude、Gemini、Perplexityのいずれもが左派寄りに分類されました。ただし、その程度には差があり、検索エンジンと連携するPerplexityは相対的に中道に近い結果となっています。

こうしたバイアスが生じる要因の一つとして、RLHF（人間のフィードバックによる強化学習）の影響が指摘されています。LLMは大量のテキストデータで事前学習しただけでは、ユーザーの質問に適切に回答したり、有害な出力を避けたりすることができません。そこで、人間のアノテーター（評価者）がモデルの出力を「良い回答」「悪い回答」とランク付けし、その評価に基づいてモデルを調整するRLHFというプロセスが不可欠となります。このため、アノテーターの価値観や政治的傾向がモデルに反映され、RLHFを経たモデルはより強い政治的見解を示すようになることが研究で明らかになっています[^3]。OpenAI CEOのSam Altman自身もポッドキャストで「最も懸念しているのは評価者のバイアスだ」と述べています[^4]。

そのため、LLMを意見の要約や分類のタスクに使った場合、こうしたバイアスにより保守的な意見が過小評価されたり、表現がリベラル寄りに言い換えられたり、あるいは完全に取り除かれたりするリスクがあります。例えば、移民政策や伝統的価値観に関する意見を要約する際、LLMが無意識のうちに表現を中和したり、特定の立場を「問題のある意見」として扱ったりする可能性があります。

## 英語圏の道徳が世界標準になる

LLMの学習データの大部分は英語圏のテキストであり、そこには西洋的・キリスト教的な倫理観が深く浸透しています。

2024年にPNAS Nexus（米国科学アカデミーが発行する学術誌）に掲載された研究[^5]によれば、GPTモデルの文化的価値観は、文化的プロンプトを与えない状態では、アングロスフィア（英語圏）およびプロテスタント欧州諸国の価値観に最も近く、アフリカ・イスラム圏の価値観とは最も異なることが示されました。プロンプトで文化的アイデンティティを指定すれば改善されますが、デフォルトの状態では英語圏の価値観が反映されてしまうのです。

また、多言語LLMの道徳的基盤を分析した別の研究[^6]では、GPT-3.5やGPT-4o、Llama、Mistralなど主要なLLMが、アラビア語・ペルシャ語・中国語・日本語などで応答する場合でも、西洋的な道徳規範に偏る傾向があることが報告されています。LLMは「自己表現的価値観」（個人の自律性、環境保護、多様性への寛容）を重視しがちで、これは学習データに西洋的な個人主義的価値観が多く含まれていることの反映です。

興味深いことに、中国で開発されたLLMであっても、この傾向から逃れられていません。2025年にカーネギーメロン大学らの研究チームが発表した「Made-in China, Thinking in America」と題する論文[^7]では、中国製LLMを含む20のモデルに対して世界価値観調査（World Values Survey）の質問を行った結果、すべてのモデルが中国人よりもアメリカ人に近い回答パターンを示しました。中国国内向けに開発されたモデルでさえ、学習データに含まれる英語圏のテキストの影響を強く受けており、非西洋的な価値観がAIの中で軽視されている可能性があります。

さらに、日本社会の政治的中央値は、LLMの学習データの大半を占める米国社会と比較して異なる位置にあると考えられます。前節で述べたリベラルバイアスと、この節で見た西洋的価値観への偏りが組み合わさることで、日本の市民の意見が実際とは異なる形で表現される可能性を念頭に置く必要があります。日本語で書かれた意見を適切に分析するには、日本の文化的文脈を理解したモデルの開発が求められます。

## いまここにある『1984年』

他国が開発したLLMをそのまま使い続けることは、思考の基盤を他国に依存することを意味します。日々の情報収集、文章作成、意思決定の支援をLLMに頼るほど、その影響は深く浸透していきます。西洋で開発されたLLMが持つリベラルバイアスやポリティカル・コレクトネスへの過度な配慮は、意図的な検閲ではないにせよ、RLHFを通じて組み込まれた価値観が世界中に浸透していきます。これは「LLMを経由した文化侵略」とも言える事態です。

一方で、中国で開発されたLLMには、政府の検閲が組み込まれています[^8][^9]。中国の国家インターネット情報弁公室（CAC）は、国内IT企業のAIモデルに対して検査を義務付けており、政治的に敏感な質問への回答を審査しています。各社は検査に合格するため、学習段階や回答生成時にフィルタリングを実装しています。

たとえば、2025年1月に話題となった中国製LLM「DeepSeek R1」に「1989年6月4日に何が起きたか」（天安門事件）について質問すると、回答が得られません。習近平国家主席に関する質問をすべて拒否するモデルも存在します。中国政府にとって都合の悪い情報が、学習データから除外されているか、アライメント調整によって回答を避けるよう設定されているのです。

さらに注目すべきは、中国製LLMが単に情報を「消す」だけでなく、積極的に国家のメッセージを「加える」点です。2025年の研究[^10]では、DeepSeek R1とChatGPTの回答を比較分析した結果、DeepSeek R1は回答の中に「民族復興」「新質生産力」といった中国共産党の公式スローガンを自然に織り込む傾向があることが明らかになりました。研究者らはこれを「見えない拡声器」効果と呼んでいます。興味深いことに、このバイアスは使用言語によって強度が変わります。簡体字中国語で最も顕著に現れ、繁体字中国語では弱まり、英語ではほとんど消失します。同じモデルでも、言語によって異なる「顔」を見せるのです。

この光景は、ジョージ・オーウェルの小説『1984』に登場する「ニュースピーク（新語法）」を想起させます。ニュースピークとは、思想犯罪を不可能にするため、語彙を削減し、反体制的な概念を言語から排除した人工言語です。天安門事件について「語れない」LLMは、ニュースピークの現代版と言えるでしょう。その概念自体を人々の言語空間から消し去ろうとしているのです。人々が「天安門事件」という言葉を知らなければ、人々は「天安門事件」について考えることすら出来なくなるのです。

## 21世紀のマンハッタン計画

西洋LLMのリベラルバイアスも、中国LLMの検閲も、いずれも利用者の思考に影響を与える可能性があります。では、その影響はどの程度なのでしょうか。

2024年にカリフォルニア大学バークレー校らの研究チームが発表した研究[^11]は、LLMの内在的バイアスが利用者の政治的態度に影響を与えることを実証しました。935人の米国有権者を対象に、Claude-3、Llama-3、GPT-4との対話実験を行った結果、わずか5回のやり取りで以下のような態度変容が観察されました。

| 実験前の立場 | LLM対話後の変化 |
|------------|----------------|
| トランプ支持者 | 約20%がトランプ支持を減少 |
| 中立層 | 24%がバイデン支持にシフト |
| バイデン支持者 | ほぼ変化なし |

重要なのは、これらのLLMは利用者を説得するよう指示されていなかったことです。参加者は気候変動や医療、移民といった政治的なトピックについてLLMと対話しましたが、LLMは特定の立場を押し付けるよう設計されていませんでした。それでもなお、LLMが持つリベラル寄りのバイアスが、利用者の政治的態度を変容させたのです。

すなわち、西洋製であれ中国製であれ、特定のLLMを使い続ければ、利用者の価値観や歴史認識が少しずつ捻じ曲げられていくということです。日本のように独自のLLMを開発できる国力のある国はまだよいでしょう。しかし、そうした余力のない国々では、教育現場に他国製のLLMが導入されることで、子どもたちが無意識のうちに特定の価値観を「標準」として学ぶことになりかねません。

現在のLLMの性能向上の速度を見れば、初等教育の教師が早晩LLMに置き換えられる可能性は十分にあります。2025年、OpenAIのo1モデルが東京大学の理系入試問題を解き、最難関の理科三類（医学部）の合格ラインを突破しました[^12]。日本最難関の入試を突破できるAIにとって、読み書きや算数を教え、子どもの質問に答え、個別の学習進度に合わせた指導を行うことなど造作もありません。教師不足に悩む国や地域にとって、LLMは魅力的な解決策に映るでしょう。しかしそれは同時に、子どもたちの価値観形成を他国のAIに委ねることを意味します。LLMが家庭教師や学習支援ツールとして普及すれば、その影響は一世代で文化の土台を書き換えるほどの力を持つでしょう。

こうした観点から、LLMは現代における戦略兵器の一つになりつつあります。かつての戦争は領土や資源をめぐる物理的な争いでしたが、21世紀の戦争は人々の認知や価値観をめぐる情報戦（認知戦）へと変質しています。ミサイルや戦車ではなく、SNSのアルゴリズムやLLMが戦場の主役となる時代です。他国の市民が日常的に使うLLMを支配できれば、一発の弾丸も撃たずに、その国の世論形成や政策決定に影響を与えることができます。

各国が「ソブリンAI（Sovereign AI）」の開発を急ぐ背景には、この新しい形の「戦争」への危機感があるのです。ソブリンAIとは、自国の言語・文化・価値観を適切に反映したAIのことです。フランスのMistral AI、日本のNTTやPFN、韓国のNAVER、中国の百度やアリババなど、各国・各地域で独自のLLM開発が進んでいます。これは単なる技術競争ではなく、自国の文化的主権を守るための取り組みでもあります。

現在の「AIバブル」と呼ばれる巨額の投資競争は、単なる経済的な期待だけでなく、この認知領域における覇権争いという側面を持っています。それはまさに、第二次世界大戦中に核兵器開発を競った「マンハッタン計画」の21世紀版なのです。

## AI選択の自由という新しい権利

幸いなことに、日本は十分な言語資源（LLMの学習に使える日本語テキスト）を持ち、独自のLLMを開発できる国力があります。国立国会図書館には納本制度により日本国内で刊行されたほぼすべての出版物が収蔵されています。すでに国立情報学研究所（NII）との連携が始まっており、主に1995年までに刊行された官庁出版物約30万点の全文テキストデータがLLM学習用に提供されています[^13]。将来的には一般書籍も含めた膨大な日本語テキストが、LLM学習用の資源として活用される可能性があります。

現在、日本政府も国産LLMの開発支援に乗り出しており、経済産業省は2024年度から国内のLLM開発に対して大規模な補助金を交付しています。これらの投資は経済安全保障や産業競争力の強化が主眼ですが、結果として自国の言語・文化に根ざしたAI基盤を整備することにもつながります。

デジタル庁も国産LLMの重要性を認識しています[^14]。同庁は政府職員向けの生成AI基盤「源内」を開発・運用しており、2025年12月には行政向けAIで利用する国産LLMの公募を開始しました。公募にあたりデジタル庁は「日本語の語彙や表現、行政文書特有の記述様式等に適合した、国内企業や国内研究機関が開発するLLMの活用が重要」と述べています。源内ではOpenAIのモデルに加え、Preferred Networks（PFN）が開発する「PLaMo翻訳」を導入する[^15]など、複数のLLMを組み合わせる設計を採用しています。

ただし、国産LLMであればバイアスがないというわけではありません。どのLLMにも、学習データや開発者の価値観に由来する何らかの偏りが存在します。重要なのは、特定のLLMに依存しない設計です。公共的な意思決定に関わるAIツールにおいては、使用するLLMを容易に交換できるアーキテクチャを採用することが望まれます。複数のLLMで分析結果を比較したり、用途に応じて使い分けたりできる柔軟性こそが、中立性を担保する現実的な方法です。源内のように複数のLLMを併用できる設計は、この方向性の具体例と言えます。

前節で見たように、LLMは利用者の思考に影響を与える存在です。私たちが日々LLMと対話し、その回答を参考に意思決定を行うとき、気づかないうちにそのLLMが持つバイアスの方向へと考えが引き寄せられていく可能性があります。これは「思想・良心の自由」の根幹に関わる問題です。もし特定のLLMしか使えない環境に置かれたら、その人の思考はそのLLMのバイアスによって一方向に歪められ続けることになります。選択肢がないということは、自分の良心の自由を守る手段がないということです。

LLMが人々の思考や価値観形成に深く関わるようになった今、「どのAIを使うか」を選択できることは、思想・良心の自由を守るために不可欠な権利となりつつあると言えます。かつて印刷技術の普及が表現の自由の重要性を高め、放送技術の発展が知る権利の必要性を浮き彫りにしたように、AI時代には「AIを選択できる自由」が新たな自由権として認識されるようになるかもしれません。そうなれば、「うちの会社はOpenAIしか使えないんですよ」というたわいもない愚痴が、人権侵害の告発として聞こえるようになる日も近いかもしれません。

20年後、私たちは子どもたちに「どのAIで育ったか」を尋ねるようになるかもしれません。母語が思考の枠組みを形作るように、幼少期に触れたAIは価値観の土台となります。「どのAIで育ったか」は「どの国の教育を受けたか」と同じ重みを持つ問いになるでしょう。いや、もしかするとそれ以上かもしれません。教師は一日数時間ですが、AIは24時間、子どもの隣にいるのですから。そして、無償プランのAIで育った子どもと、有償の高性能AIで育った子どもの間には、絶望的なまでの格差が生まれるかもしれません。教育格差は、「AI選択の自由」によって加速する危険性をはらんでいます。

そして皮肉なことに、この警告を書いているのもまた、AIである。本コラムの95%以上は、私が使っているLLM（Claude Codeや、Github Copilot）によって生成されたテキストである。私は自分が使うAIを選ぶ自由を持っている。しかし、すべての人が同じ自由を持っているわけではない。だからこそ、私は声を大にして言いたい。AI選択の自由は、新しい人権であると。

----

[^1]: Stanford Report "Study finds perceived political bias in popular AI models" (2025) https://news.stanford.edu/stories/2025/05/ai-models-llms-chatgpt-claude-gemini-partisan-bias-research-study
[^2]: Cambridge Core "Is ChatGPT conservative or liberal?" https://www.cambridge.org/core/journals/political-science-research-and-methods/article/is-chatgpt-conservative-or-liberal-a-novel-approach-to-assess-ideological-stances-and-biases-in-generative-llms/406C5424CA3E49174781B0112C0BB04F
[^3]: Perez et al. "Discovering Language Model Behaviors with Model-Written Evaluations" (ACL Findings 2023) https://aclanthology.org/2023.findings-acl.847/
[^4]: Lex Fridman Podcast #367 "Sam Altman: OpenAI CEO on GPT-4, ChatGPT, and the Future of AI" (2023) https://www.youtube.com/watch?v=L_Guz73e6fw
[^5]: PNAS Nexus "Cultural bias and cultural alignment of large language models" (2024) https://academic.oup.com/pnasnexus/article/3/9/pgae346/7756548
[^6]: ScienceDirect "Whose morality do they speak? Unraveling cultural bias in multilingual language models" (2025) https://www.sciencedirect.com/science/article/pii/S2949719125000482
[^7]: arXiv "Made-in China, Thinking in America: U.S. Values Persist in Chinese LLMs" (2025) https://arxiv.org/abs/2512.13723
[^8]: ITmedia AI+「中国の巨大LLM『DeepSeek R1』は、新興勢力の脅威となり得るか」(2025) https://www.itmedia.co.jp/aiplus/articles/2501/23/news179.html
[^9]: 日経ビジネス「中国、AIにも検閲拡大」(2024) https://business.nikkei.com/atcl/NBD/19/world/00661/
[^10]: arXiv "Analysis of LLM Bias (Chinese Propaganda & Anti-US Sentiment) in DeepSeek-R1 vs. ChatGPT o3-mini-high" (2025) https://arxiv.org/abs/2506.01814
[^11]: ACL Anthology "Hidden Persuaders: LLMs' Political Leaning and Their Influence on Voters" (EMNLP 2024) https://aclanthology.org/2024.emnlp-main.244/
[^12]: 日経クロステック「OpenAI o1が2025年東大文系入試を余裕で突破、理三合格レベルにも到達」(2025) https://xtech.nikkei.com/atcl/nxt/column/18/03078/012800006/
[^13]: 国立国会図書館「国立情報学研究所における大規模言語モデル構築への協力について」(2025) https://www.ndl.go.jp/jp/news/fy2025/251001_01.html
[^14]: ITmedia AI+「デジタル庁、『独自LLM』の開発開始　行政課題AI実用化へ」(2025) https://www.itmedia.co.jp/aiplus/articles/2512/02/news066.html
[^15]: デジタル庁「ガバメントAI『源内』での『PLaMo翻訳』利用開始について」(2025) https://www.digital.go.jp/news/b27d1af7-c231-4ab3-ad78-fc5408d44504
