# 第11章 ブロードリスニング要素技術解説

本章では、ブロードリスニングを支える基盤技術について解説する。広聴AIをはじめとするブロードリスニングツールは、複数の先端技術を組み合わせることで実現されている。データサイエンスの素養がないプログラマーでも理解できるよう、各技術が「何をインプットして、どのようなアウトプットを行い、何に使えるのか」という水準で説明していく。

## 11.1 ブロードリスニングに至る技術ツリー

ブロードリスニングは、比較的最近登場した技術の組み合わせによって成り立っている。主要な構成技術とその登場年は以下の通りである。

- **UMAP**（2018年）：高次元データを低次元に圧縮する次元削減アルゴリズム
- **BERT**（2018年）：文脈を考慮した単語・文章のベクトル化技術
- **LLM**（2020年）：大規模言語モデル。GPT-3の論文とAPIは2020年、ChatGPTは2022年11月に登場

これらのアルゴリズムを組み合わせることで、「大量の意見を分析、集約、要約する技術」が構築されている。

![ブロードリスニングに至る技術ツリー](images/chapter11/slide_11.png)

技術ツリーを見ると、「同じ」「似ている」という概念をコンピュータで扱うための長い歴史があることがわかる。シノニム辞書から始まり、Word2Vec、BERTへと発展し、最終的にLLMによる意味理解・文書生成へとつながっている。一方で、クラスタリングや次元圧縮といった統計的手法も、コサイン類似度という「距離が近い」という概念を通じてブロードリスニングに貢献している。

## 11.2 コンピュータにおける「同じ」とは何か

### 11.2.1 文字は数値である

コンピュータの中では、文字は数値として取り扱われる。JavaScriptやPythonで「デジタル民主主義」という文字列をエンコードすると、それぞれの文字がバイト列（数値の配列）として表現される。

```python
>>> [c for c in "デジタル民主主義".encode("utf-8")]
[227, 131, 135, 227, 130, 184, 227, 130, 191, 227, 131, 171, 230, 176,
 145, 228, 184, 187, 228, 184, 187, 231, 190, 169]
```

このように、日本語の文字列も内部的にはすべて数値として扱われている。UTF-8エンコーディングでは、日本語の1文字は通常3バイトの数値で表現される。「デ」という文字は[227, 131, 135]という3つの数値で表現され、「ジ」は[227, 130, 184]となる。この数値の並びが、コンピュータにとっての「文字」の実体である。

### 11.2.2 「猫」と「ねこ」問題

人間は「猫」と「ねこ」を同一のものと認識する。しかし、コンピュータにとってこれらはまったく別のもの（別の数値）として認識される。

```python
"猫".encode("utf-8").hex()    # => 'e78cab'
"ねこ".encode("utf-8").hex()  # => 'e381ade38193'
```

`0xE78CAB`と`0xE381ADE38193`は似ているだろうか。数値として見れば、まったく異なる値である。では、どのようにすればコンピュータは「猫」と「ねこ」を同じものとして扱えるようになるのだろうか。

この問題は日本語特有のものではない。英語でも「color」と「colour」、「organize」と「organise」のようなスペルの違いがあり、コンピュータにとっては異なる文字列として認識される。さらに深刻なのは、「car」と「automobile」のような完全に異なる綴りで同じ意味を持つ単語である。これらをどのように同一視するかが、自然言語処理の根本的な課題となってきた。

### 11.2.3 シノニム（同義語）辞書の整備

従来のアプローチは、対応表（シノニム辞書）を人力で整備することだった。

- 「パソコン」「PC」
- 「自動車」「車」「クルマ」「カー」
- 「携帯電話」「スマートフォン」「スマホ」
- 「エアコン」「クーラー」
- 「顧客」「クライアント」「カスタマー」

日本語では、漢字の開き、ひらがな、カタカナくらいは機械的に変換ができる。しかし、それ以上は対応表を地道に整備する必要があった。昔の検索エンジンではシノニム辞書の整備に多大な労力が費やされていたが、人間がすべてを網羅するには限界がある。そこで、この作業を機械に任せたいという要求が生まれた。

この人力での辞書整備には、いくつかの本質的な問題があった。

1. **網羅性の限界**：新しい言葉が次々と生まれる中で、すべての同義語を網羅することは不可能である
2. **文脈の無視**：「ネコ」は動物のことだけでなく、建設現場では一輪車を指す。単純な対応表ではこの文脈による意味の違いを捉えられない
3. **スケーラビリティの問題**：言語ごとに辞書を整備する必要があり、多言語対応には膨大なコストがかかる

これらの課題を解決するために登場したのが、次節で紹介するWord2Vecである。

## 11.3 Word2Vecの発明からBERTへ：意味のベクトル化

### 11.3.1 Word2Vec（2013年）

Word2Vecは、「同じような前後文脈で使われ方をしている単語は、同じような意味なのだろう」という考え方に基づくアルゴリズムである。この考え方は言語学では「分布仮説（distributional hypothesis）」と呼ばれ、1950年代から提唱されていた。Word2Vecはこの仮説を大規模データと機械学習で実現したものである。標準実装では、対象の単語の前後5ワードを利用する。

例えば、「カワイイ○○が『にゃー』と鳴いた」という文章があったとする。○○には「猫」や「ねこ」が入りうる。これにより、「猫」と「ねこ」は同じものを指していそうだとわかる。

また、「子供の△△はとてもカワイイ」という文章では、△△には「猫」だけでなく「犬」や「鶏」なども入りうる。これにより、「猫」「犬」「鶏」は近い概念だとわかる。

Word2Vecの画期的な点は、人間が明示的に「猫とねこは同じ」と教えなくても、大量のテキストデータから自動的にその関係性を学習できることだった。Googleが開発し、2013年に論文が公開されると、自然言語処理の分野に革命をもたらした。

### 11.3.2 トークナイズと形態素解析

人は単語単位で意味を処理するので、意味単位に切り出してコンピュータで取り扱ったほうが効率的である。英語ではスペースで単語が区切られているので、その単位で取り扱えばよい。日本語では形態素解析を行って、単語単位で切り出すことが多い。

```python
from janome.tokenizer import Tokenizer
tok = Tokenizer()
text = "人は単語単位で意味を処理するので、意味単位に切り出してコンピュータで取り扱ったほうが効率的"
tokens = tok.tokenize(text)
print(",".join([token.surface for token in tokens]))
# => 人,は,単語,単位,で,意味,を,処理,する,ので,、,意味,単位,に,切り出し,て,コンピュータ,で,取り扱っ,た,ほう,が,効率,的
```

### 11.3.3 ベクトル化（分散表現）

単語の「同じ」や「近しい」を表現するにはどうしたらいいだろうか。Word2Vecのアプローチは、単語を多次元上の点だと考え、その点の間の距離を考えることである。同じような使われ方をしている単語は、近しい位置になるように自動調整される。

Word2Vecのあるモデルを使うと、「猫」という単語が300次元のベクトルに変換される。

```python
model["猫"]
# => array([-0.0938, -0.5717, -0.0722,  0.1387,  0.0142, -0.1156,  0.4258,
#           -0.4551, -0.2258,  0.5309,  0.1736, -0.1482,  0.0961,  0.1825,
#           ... (300次元分続く)
```

300次元というのは、300個の数値の組み合わせで単語の意味を表現するということである。人間には300次元の空間を直感的に理解することは難しいが、コンピュータにとっては単なる数値の配列であり、容易に計算できる。

ベクトル化することで、文字列の完全一致でなくとも、コンピュータはうまく取り扱えるようになった。「猫」に近い単語を計算することが可能になったのである。

```python
model.most_similar("猫")
# => [('ネコ', 0.7426), ('ねこ', 0.6689), ('仔猫', 0.6409),
#     ('ウサギ', 0.6322), ('子猫', 0.6317), ('犬', 0.6310), ...]
```

### 11.3.4 コサイン類似度

多次元空間において「近い」とは何だろうか。一般にはコサイン類似度を利用する。

角度はベクトルの内積の定義から計算できる。

```
A・B = |A| |B| cosθ  （高校数学Bの範囲）
cosθ = A・B / |A| |B|
```

cosθは似ていれば1に近く、違うなら-1に近くなる（θ=0°のとき1、θ=180°のとき-1をとる）。わざわざcosの逆関数を使って角度を求めなくとも、cosθを求めるだけで類似性の判断はできる。

```python
cat1 = model["猫"]
cat2 = model["ネコ"]

# word2vecの類似度計算
similarity1 = model.similarity("猫", "ネコ")

# 手動での類似度計算
similarity2 = 0
for v1, v2 in zip(cat1, cat2):
    similarity2 += v1 * v2
similarity2 /= (sum(v1 ** 2 for v1 in cat1) ** 0.5)
similarity2 /= (sum(v2 ** 2 for v2 in cat2) ** 0.5)

print(similarity1, similarity2)
# => 0.74265 0.7426499817057948
```

単語が適切な分散表現に変換できれば、意味の近さは内積の計算で求まる。分散表現によって言語学の問題が、空間幾何の問題に変換されたのである。幾何の問題に帰着すれば、ベクトル計算（四則演算）に帰着する。四則演算に帰着すれば、コンピュータで計算できる。

Word2Vecには、もう一つ有名な特性がある。単語のベクトルを足し引きすることで、意味の演算ができるのである。

```python
# 「王様」から「男」を引いて「女」を足すと「女王」に近くなる
model.most_similar(positive=['王様', '女'], negative=['男'])
# => [('女王', 0.72), ...]

# 「東京」から「日本」を引いて「フランス」を足すと「パリ」に近くなる
model.most_similar(positive=['東京', 'フランス'], negative=['日本'])
# => [('パリ', 0.68), ...]
```

この現象は、Word2Vecが単なる単語の類似性だけでなく、単語間の「関係性」もベクトル空間に埋め込んでいることを示している。「王様と女王の関係」と「男と女の関係」が、ベクトル空間上で同じ方向の差分として表現されているのである。

### 11.3.5 Word2Vecの限界

「bank」は文脈によって「川岸・土手」や「銀行」になる。Word2Vecでは単語単位でしか学習できていないので、以下の2つの文章の"bank"は同じ値になってしまう。

- 文1: "He sat by the bank of the river."（川岸）
- 文2: "She deposited money in the bank."（銀行）

前後文脈によって意味が異なる単語を適切に捉えられていない。日本語の「ネコ」も実は多義的である。土木作業員が「ネコを取ってきてくれ」と言ったら、それは一輪車のことを指している。

### 11.3.6 BERTの登場（2018年）

BERTは、ベクトル表現された単語の値を、前後のベクトルと共に計算を行うことで、その文脈にあった意味を持つ値になるように変換を行う。この「前後の文脈を考慮する」仕組みこそが、BERTの革新的な点である。

BERTは「Bidirectional Encoder Representations from Transformers」の略で、Googleが2018年に発表した。「Bidirectional（双方向）」という名前が示すように、ある単語の意味を理解するために、その単語の前後両方の文脈を同時に参照する。

上記の例では、riverやmoneyによってbankの意味が変化してくる。BERTの登場によって、Googleでは自然文による検索の精度が大きく改善したとされている（2019年頃の話）。

また、BERTは文章全体の意味も同時にベクトル化できる。文章のベクトル化自体は2014年頃から行われていた（doc2vec）が、性能が向上してうまく機能するようになったのはBERT以降である。

BERTの処理では、文章の開始地点には必ず[CLS]トークンが入れられる。計算後に[CLS]トークンの値は、文脈全体の意味を表現するベクトルに変換される。

![BERTの処理の流れ](images/chapter11/slide_27.png)

### 11.3.7 文脈ベクトルによる文章の類似度

文章をベクトル化できるようになったので、文章間の遠い、近いも計算できるようになった。以下の文章をベクトル化してコサイン類似度を観察してみよう。

**料理関連**
- トマトソースのパスタを作るのが好きです
- 私はイタリアンの料理が得意です
- スパゲッティカルボナーラは簡単においしく作れます

**天気関連**
- 今日は晴れて気持ちがいい天気です
- 明日の天気予報では雨が降るようです
- 週末は天気が良くなりそうで外出するのに最適です

**技術関連**
- 新しいスマートフォンは処理速度が速くなりました
- 最新のノートパソコンはバッテリー持ちが良いです
- ワイヤレスイヤホンの音質が向上しています

SentenceTransformerを用いて文章をベクトル化し、それぞれの文章間で相互にコサイン類似度を算出して距離行列化すると、同一ジャンルの文章はコサイン類似度が高くなっていることがわかる。同一ジャンル内でも「晴れ」と「雨」ではコサイン類似度は下がる傾向にある。

「ベクトル検索」のコアとなる技術が、この文脈ベクトルとコサイン類似度である。

従来のキーワード検索では、ユーザが入力した単語と完全一致または部分一致するドキュメントしか見つけられなかった。「犬の飼い方」で検索しても、「ペットの育て方」という文書は見つからない。しかしベクトル検索では、文章の「意味」を比較するため、表現が異なっていても意味的に近い文書を発見できる。

この技術は、RAG（Retrieval-Augmented Generation）の基盤としても使われている。LLMに回答させる際に、質問文に意味的に近いドキュメントをベクトル検索で取得し、それを参考情報としてLLMに渡すことで、より正確な回答を生成させることができる。

### 11.3.8 エンベディング（embedding）

エンベディングとは、言葉や単語の世界にあるものを、ベクトルの世界に変換することである。機械学習によって、同じような単語は高次元空間において近しい位置に配置される。これにより単語を演算可能なベクトルの空間に変換することができる。日本語では「埋め込み」と呼ばれることもある。

エンベディングの実装例としては、以下のようなものがある。

**OpenAI Embeddings API**
```python
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
    input="Your text string goes here",
    model="text-embedding-3-small"
)

print(response.data[0].embedding)
# => 1536次元のベクトルが返ってくる
```

**SentenceTransformers**
```python
from sentence_transformers import SentenceTransformer
model_name = "sentence-transformers/paraphrase-multilingual-mpnet"
emb_model = SentenceTransformer(model_name)
data = ["hello world"]
emb = emb_model.encode(data)
print(emb)
# => 768次元のベクトルが返ってくる
```

## 11.4 LLMの誕生とプロンプトエンジニアリング

### 11.4.1 BERTからGPTへ

BERTは文章を理解する方向性で開発された技術なので、文章生成には適していなかった。BERTは「穴埋め問題」を解くように設計されており、文章の途中にある欠けた単語を予測するのは得意だが、文章を続けて書くことは苦手だった。

GPTは単語の分散表現の列から次の単語を予測する方向に進化し、文章生成が可能になった。GPTは「Generative Pre-trained Transformer」の略で、OpenAIが開発した。名前に「Generative（生成的）」とあるように、文章を生成することを目的として設計されている。

GPTの処理では、自分よりも前のトークンのベクトルから値を更新し、最終的に次の単語の確率が得られる。例えば、"He sat by the bank of the ???"という文章に対して、次に来る単語の確率が計算される。

```
river   35%
stream  25%
creek   15%
pond     6%
lake     6%
canal    4%
...
```

### 11.4.2 GPTは「次の言葉を予測するAI」

過去の言葉から、次の言葉を予測することは、次の言葉を紡ぐことと同義である。処理の流れは以下の通りである。

1. 入力された文章をトークン化
2. トークンをベクトル化
3. ベクトルを計算して意味ベクトル化
4. 次のトークンを予測
5. 予測されたトークンから、ランダムに適当なものを選ぶ
6. トークンから単語に戻す

「繋がりそうな単語を繋いでいく」という、ただそれだけで、「知性」と呼んでも差し支えない能力が得られてしまった。

なぜ「次の単語を予測する」だけで知性が生まれるのだろうか。それは、正しく次の単語を予測するためには、文章の内容を「理解」している必要があるからである。

例えば、「日本の首都は」という文章の次に来る単語を正しく予測するには、「日本」という国の「首都」が「東京」であるという知識が必要になる。「ピタゴラスの定理によれば、直角三角形の斜辺の二乗は」という文章の続きを予測するには、数学的な知識が必要である。

大量のテキストで「次の単語の予測」を学習する過程で、LLMは言語のパターンだけでなく、世界についての膨大な知識を獲得した。これが「創発」と呼ばれる現象であり、単純なタスクの学習が、予期せぬ高度な能力を生み出したのである。

### 11.4.3 ChatGPTのトークナイズ

自然言語はそのままの形ではAIに入力できないので、内部的に数値に変換する。この変換がトークナイズである。

出現頻度に応じてトークンの割り当てが行われており、英語では頻出単語は1トークンになり、複雑な単語は途中で分割される。ChatGPTのトークナイザでは日本語の文書は、1文字が約1.3トークンになる（平易な文章では1文字1.1トークン程度）。

「日本」「自由」は出現頻度が高いので1トークンにまとめられている。一方、出現頻度の低い「挙」「孫」「諸」「恵」「禍」「権」「憲」は1文字が複数のトークンに分割される。

日本語が英語に比べて推論性能が低いのは、英語では「次の単語」を予測するが、日本語では「次の文字（の一部）」を予測しているからではないかという議論もある。

また、トークン数はAPI利用料金の計算基準にもなっている。同じ内容を伝える場合、日本語は英語よりも多くのトークンを消費するため、API利用コストが高くなる傾向がある。これは日本語でLLMを活用する際に考慮すべき実務的な課題の一つである。

### 11.4.4 プロンプトエンジニアリングの誕生

GPT-3のAPIは2020年9月から提供されていたが、そこまで話題にはならなかった。「次の言葉を予測させるための独り言」を入力できる人がほとんどいなかったからである。次の言葉を予測させることで課題を解決させるには、極めて特殊な構文を発明しなくてはならず、これができる人が限られていた。

一方で、プログラマ界隈では「これは新しいプログラミングの形だ、プロンプトエンジニアリングだ」と話題になった。プログラムで課題を解決するのではなく、特殊な穴埋め問題を作ることで、LLMに埋め込まれた常識を利用して課題を解決することができる。

従来のプログラムが取扱いにくかった「不定形な自然文の入出力」が行えるようになった。

### 11.4.5 ChatGPTは「次の対話を予測するAI」に進化

Instruction Tuning（対話履歴を学習）によって、自然な応答が可能になった。発話者という特殊なトークンを導入することで、対話履歴を学習することが可能になり、対話や会話履歴を通じた問題解決が可能になった。

- GPTは「次の単語を予測して生成するAI」であった
- ChatGPTはそこから発展して「次の対話を予測し、生成するAI」になった

結果として、一般人が触れるようになり爆発的に普及した。ChatGPTはGPTになかった新しい用途を切り開いた。AIに作業を依頼したり、質問に対して回答してくれたりするツールである。対話をしながら思考を深められるツールでもある。

次の単語を予測するという能力が極まった結果、AGI（汎用人工知能、どんな問題でも解けるAI）と言っても過言ではない能力を獲得してしまった。

### 11.4.6 GPT-4の能力

2023年3月、OpenAIはChatGPTにGPT-4を搭載し、ChatGPT Plusで有料提供を開始した。GPT-4により、性能が飛躍的に向上した。

- GPT-4は司法試験に合格できるくらいのスコアを記録した
- 日本の医師国家試験も突破した
- 最近ではo3で東大入試の理IIIも突破した

様々な用途への利用が現実的なラインを超えてきた。東大に入れるAIが月20ドルで使い放題という状況なので、これを使いこなすスキルが必要である。さらにはAPI経由で「東大に入れるAI」が誰でもプログラムに組み込める状態になった。

### 11.4.7 LLMによるプログラミングの拡張

LLMをプログラムに組み込む例として、ColorGPTがある。これはChatGPTのAPIを利用した最初期のアプリで、Webブラウザからカメラを起動し、現実世界の色のRGBの値から、色の名前に変換するソフトウェアである。

```javascript
const GPT35TurboMessage = (question: string) => [
  {
    role: "system",
    content: `You're a designer give the closest base color name from hex code. Answer with short and simple name.`,
  },
  {
    role: "user",
    content: "Color name of #af6e4d ?",
  },
  {
    role: "assistant",
    content: "brown",
  },
  { role: "user", content: `Color name of ${question} ?` },
];
```

カラーコードと色の対応表を作るのは骨が折れるが、それをChatGPTを使って一発で処理している。たったこれだけの指示で、色変換を実現している。

### 11.4.8 Few-shot Learning

LLMにほんの少しの正解となる応答パターンを見せる（受け答えの履歴を捏造する）ことで、LLMに空気を読ませて、目的となる出力を行わせる手法である。特定のフォーマットへの準拠などはこれでうまくいくことが多い。

例えば、英語とフランス語の対訳表を作り、チーズの後を欠落にすることで、そこを予測させて、フランス語でのチーズを答えさせる。

```
Translate English to French:
sea otter => loutre de mer
peppermint => menthe poivrée
plush girafe => girafe peluche
cheese =>
```

### 11.4.9 LLMは何が得意なのか

LLMは「論理的思考力」と「常識」を提供する。カラーコードと色の名前はある種の「常識」である。東大の理IIIの入試に合格できる程度の論理的思考力は持ち合わせている。

一方でLLMには「常識」はあるが「知識」はない。ハルシネーションが発生するのは「知識」を問うからである。高校までの知識程度の「常識」や、文章処理能力であれば、極めてうまく動作する。「知識」が欲しいなら、RAGを構築する必要がある。

ここでいう「常識」と「知識」の違いを明確にしておこう。

- **常識**：多くの文章に繰り返し登場するため、学習データに十分な量が含まれている情報。「太陽は東から昇る」「水は100度で沸騰する」「パリはフランスの首都」など。
- **知識**：特定のドメインや最新の情報など、学習データに十分な量が含まれていない可能性がある情報。「○○社の今期の売上」「△△市の条例の詳細」など。

LLMは学習データに十分な量が含まれていない情報を聞かれたとき、「わからない」と答える代わりに、もっともらしい（しかし事実ではない）回答を生成してしまうことがある。これがハルシネーション（幻覚）である。

コードで書くのは大変だが、常識と論理的思考力で処理できる問題であれば、極めてうまく機能する。広聴AIでは、文章分割や要約といったタスクでうまく機能している。これらは「知識」を必要とせず、文章の構造や意味を理解して処理する「常識」的なタスクだからである。

### 11.4.10 Structured Output

2024年8月、OpenAIはAPIにStructured Output機能を搭載した。ユーザが指定したJSONフォーマットで出力させることが可能になった。

Structured Outputによって、LLMは「自然言語を入力すると、JSONが出力されるモジュール」として、システムの一部に組み込み可能になった。それまでのLLMは自然言語がそのまま出力されるため、システムに組み込みづらかった。

JSONのフォーマットを例示することで、90%くらいの確率で準拠させることは可能であったが、それでも10%はエラーが起こる。10%の確率でエラーが起こるモジュールをシステムに組み込むのは困難である。

```python
client = openai.Client()

class Character(BaseModel):
    name: str = Field(description="キャラクターの名前")
    age: int = Field(description="キャラクターの年齢")
    occupation: str = Field(description="キャラクターの職業")
    description: str = Field(description="キャラクターの説明")
    strength: int = Field(description="キャラクターの強さ")
    agility: int = Field(description="キャラクターの敏捷性")
    intelligence: int = Field(description="キャラクターの知性")

result = client.beta.chat.completions.parse(
    model="gpt-4o",
    messages=[{"role": "user", "content": "架空のキャラクターを作成して"}],
    n=1,
    response_format=Character
).choices[0].message.parsed

print(result.model_dump_json(indent=2))
# => {
#   "name": "Haru Yamada",
#   "age": 28,
#   "occupation": "Urban Explorer / Photographer",
#   ...
# }
```

JSON Schemaで型を渡すと、その型を満たすJSONを返してくれる。これにより、こちらが狙ったフォーマットでの出力を行わせることができ、プログラムに組み込みやすくなった。

Structured Outputの登場は、LLMのシステム組み込みにおける大きな転換点となった。従来は、LLMの出力をパースするための複雑なコードが必要で、エラーハンドリングも煩雑だった。Structured Outputにより、LLMは「文字列を入力すると構造化データを返す関数」として扱えるようになり、従来のプログラミングの枠組みにシームレスに統合できるようになった。

広聴AIでも、意見の抽出やラベリングの際にStructured Outputを活用している。これにより、LLMの出力を確実にJSONとして受け取り、後続の処理に渡すことができる。

## 11.5 クラスタリングによる似たものの集約

### 11.5.1 クラスタリングとは

クラスタリングとは、似た特徴をもつデータ同士を自動でグループ分けすることである。「クラスタ（cluster）」は英語で「房」や「集団」を意味し、ブドウの房のように似たものが集まっている状態をイメージするとわかりやすい。

機械学習の分類では、クラスタリングは「教師なし学習」に分類される。「教師あり学習」では正解データを用意して「この画像は猫、これは犬」と教える必要があるが、「教師なし学習」では正解を与えずに、データの特徴だけからグループを自動的に発見する。

同じデータに対して、様々なクラスタリングアルゴリズムを適用すると、それぞれ異なる結果が得られる。

![クラスタリングアルゴリズムの比較](images/chapter11/slide_51.png)

代表的なクラスタリングアルゴリズムには以下のようなものがある。

- **k-means**：データをk個のグループに分割する。各グループの中心点（セントロイド）を繰り返し更新することで、データを分類する。シンプルで高速だが、kを事前に指定する必要がある。
- **階層的クラスタリング（Ward法など）**：最も近いデータ同士を順次統合していき、木構造（デンドログラム）を作成する。kを事前に決めなくてよいが、計算コストが高い。
- **DBSCAN**：密度の高い領域をクラスタとして認識する。ノイズの検出が得意だが、パラメータ調整が難しい。
- **SpectralClustering**：データ間の類似度行列を使い、グラフ理論に基づいて分割する。複雑な形状のクラスタを検出できるが、計算コストが高い。

### 11.5.2 埋め込みベクトルはクラスタリング可能

埋め込みベクトルは数値の集合である。同じような意味の言葉や文章は、近い位置にある。コサイン類似度やユークリッド距離で、ベクトル間の距離が計測可能である。

適切にクラスタリングを行うことで、同じような文脈ベクトルを集約できる。つまり、同じような意見を集約できる。これがTTTCや広聴AIの発想の根源である。

### 11.5.3 階層化クラスタリング

クラスタの統合を繰り返し、最終的に1つのクラスタに統合していく手法である。閾値を下げていくと、クラスタを少しずつ分割していける。

広聴AIではWard法という階層化クラスタリングアルゴリズムが採用されている。Ward法は、クラスタを統合する際に、統合後のクラスタ内分散が最小になるようにペアを選ぶ手法である。

「分散が最小になる」とは、統合後のクラスタ内のデータ点ができるだけ近くに集まっている状態を選ぶということである。言い換えれば、「似たもの同士を優先的にまとめる」アルゴリズムである。

![階層化クラスタリングのデンドログラム](images/chapter11/slide_53.png)

デンドログラム（樹形図）は、階層的クラスタリングの結果を視覚化したものである。下から上に向かって読むと、最初は個々のデータ点がそれぞれ独立したクラスタとして存在し、徐々に統合されていく様子がわかる。縦軸は統合された時点での距離（または分散の増加量）を表す。

閾値を設定して横に線を引くと、その時点でのクラスタ数が決まる。閾値を低く設定すれば細かいクラスタに、高く設定すれば大きなクラスタに分かれる。この柔軟性が階層的クラスタリングの利点の一つである。

## 11.6 次元圧縮による高次元データの可視化

### 11.6.1 次元圧縮とは

次元圧縮とは、データの本質だけを残して余分な情報を捨て、「軸」を減らすことで、より少ない数の特徴で表現し直す技術である。広聴AIでは1536次元から2次元といった圧縮が行われている。

なぜ次元圧縮が必要なのだろうか。大きく2つの理由がある。

1. **可視化のため**：人間は3次元（奥行き、高さ、幅）までしか直感的に理解できない。1536次元のベクトル空間をそのまま見ることは不可能である。2次元に圧縮すれば、スクリーン上に散布図として表示できる。
2. **計算効率のため**：次元が高いほど計算量が増える。特にクラスタリングでは、次元の呪い（curse of dimensionality）と呼ばれる問題が発生し、高次元空間では距離の概念が意味を持たなくなる。次元を下げることで、クラスタリングの精度と効率が向上する。

### 11.6.2 主成分分析（PCA）

データの相関を利用した一番基本的なアルゴリズムである。相関のある軸を中心にグラフ全体を回転させて、有効な軸を残す手法である。次元数が少なく、軸間の相関が強い場合にうまく機能する。

### 11.6.3 UMAP

UMAP（Uniform Manifold Approximation and Projection）は、局所的な距離構造を維持したまま、データをより低次元に写しかえる手法である。曲がったゴムシートを空間に押し付けて、低次元に写しかえるイメージである。TTTCや広聴AIはこのアルゴリズムを利用している。

UMAPの「局所的な距離構造を維持する」という特性は、ブロードリスニングにとって重要である。グローバルな構造（全体的な配置）よりもローカルな構造（近くにあるもの同士の関係）を優先して保存するため、「似ている意見は近くに配置される」という直感的な可視化が可能になる。

UMAPは2018年にLeland McInnesらによって発表された。t-SNEという先行手法と比較して、計算が高速で、グローバルな構造もある程度保存できるという利点がある。

### 11.6.4 UMAPのデモンストレーション

MNIST（手書きの数字）のデータをPCAとUMAPで2次元に次元圧縮して可視化すると、その違いがよくわかる。PCAでは混在した状態になるが、UMAPではうまく分離できている。

手書きだと間違いやすい{0,6}、{2,3,5,8}、{4,9,7}が隣接しながら分離されている。これは、UMAPが局所的な構造を保存しながら次元削減を行うためである。

![PCAとUMAPの比較](images/chapter11/slide_55.png)

## 11.7 本章のまとめ

本章では、ブロードリスニングを支える基盤技術について解説した。

1. **文字と数値の関係**：コンピュータにとって「猫」と「ねこ」は異なる数値であり、同じものとして扱うには工夫が必要である
2. **Word2Vec**：同じような文脈で使われる単語は同じような意味を持つという考えに基づき、単語をベクトル化する
3. **BERT**：文脈を考慮した単語・文章のベクトル化を実現した
4. **LLMとChatGPT**：次の単語・対話を予測することで、知性と呼べる能力を獲得した
5. **Structured Output**：LLMをシステムに組み込み可能にする技術である
6. **クラスタリング**：似た意見を自動でグループ分けする
7. **次元圧縮**：高次元データを人間が理解できる2次元に変換する

これらの技術を組み合わせることで、大量の意見を分析、集約、要約するブロードリスニングが実現されている。次章では、これらの技術が広聴AIでどのように組み合わされているかを詳しく見ていく。
