# 第11章 広聴AI要素技術解説

本章では、広聴AIを支える基盤技術について解説します。広聴AIは複数の先端技術を組み合わせることで実現されています。データサイエンスの素養がないプログラマーでも理解できるよう、各技術が「何をインプットして、どのようなアウトプットを行い、何に使えるのか」という視点で説明していきます。

## 11.1 広聴AIを構成する技術群

広聴AIは、ここ10年ほどの間に登場した複数の技術を組み合わせることで実現されています。どれか一つが欠けても成り立ちません。主要な構成技術とその登場年は以下の通りです。

- **UMAP**（2018年）：高次元データを2次元に圧縮し、人間が見られる形に可視化する
- **BERT**（2018年）：文脈を考慮して、文章の「意味」をベクトル（数値の配列）に変換する
- **LLM**（2020年〜）：大規模言語モデル。GPT-3は2020年、ChatGPTは2022年11月に登場

これらの技術が出揃ったのは、ほんの数年前のことです。つまり広聴AIは、技術的に「やっと可能になった」ばかりの新しい手法なのです。

TODO: 図を挿入「広聴AIに至る技術ツリー」
- 左側に自然言語処理の系譜（シノニム辞書→Word2Vec→BERT→LLM）
- 右側に統計・可視化の系譜（クラスタリング→次元圧縮→UMAP）
- 両者が「コサイン類似度」で合流し、広聴AIに至る流れを示す

この図を見ると、一つの問いが長年にわたって追求されてきたことがわかります。その問いとは、「コンピュータに『同じ』『似ている』をどう理解させるか」です。

左側の流れは「言葉の意味をコンピュータに理解させる」という挑戦の歴史です。シノニム辞書（同義語辞書）から始まり、Word2Vec、BERTへと発展し、最終的にLLMによる意味理解・文章生成へとつながっています。

一方、右側にはクラスタリングや次元圧縮といった統計的手法があります。これらは「似ているものを集める」「高次元のデータを人間が見られる形にする」という技術です。

この二つの流れが、コサイン類似度という「ベクトル間の距離を測る」概念を介して合流します。言葉を数値（ベクトル）に変換し、その距離を測り、近いものを集めて可視化する——これが広聴AIの基本的な仕組みです。

以降の節では、これらの技術を一つずつ解説していきます。

## 11.2 コンピュータにおける「同じ」とは何か

広聴AIの技術を理解するために、まずコンピュータが「言葉」をどのように扱っているかを知る必要があります。人間にとって当たり前の「同じ意味」という概念が、コンピュータにとってはまったく自明ではないのです。

### 11.2.1 文字は数値である

コンピュータの中では、文字はすべて数値として扱われています。私たちが画面上で「あ」や「A」という文字を見ているとき、コンピュータの内部ではそれらは特定の数値として処理されています。

試しにPythonで「デジタル民主主義」という文字列を数値に変換してみましょう。

```python
>>> [c for c in "デジタル民主主義".encode("utf-8")]
[227, 131, 135, 227, 130, 184, 227, 130, 191, 227, 131, 171, 230, 176,
 145, 228, 184, 187, 228, 184, 187, 231, 190, 169]
```

「デジタル民主主義」という8文字の文字列が、24個の数値の配列に変換されました。UTF-8というエンコーディング方式では、日本語の1文字は通常3バイト（3つの数値）で表現されます。「デ」は[227, 131, 135]、「ジ」は[227, 130, 184]という具合です。

私たちが文字として認識しているものは、コンピュータにとっては単なる数値の並びに過ぎません。この事実が、次に述べる問題の根源となります。

### 11.2.2 「猫」と「ねこ」問題

人間にとって「猫」と「ねこ」は同じものを指しています。漢字で書こうがひらがなで書こうが、四本足でにゃーと鳴くあの動物のことです。しかし、コンピュータにとってこれらはまったく別のものです。

```python
"猫".encode("utf-8").hex()    # => 'e78cab'
"ねこ".encode("utf-8").hex()  # => 'e381ade38193'
```

「猫」は`0xE78CAB`、「ねこ」は`0xE381ADE38193`という数値になります。この2つの数値は似ているでしょうか？　まったく似ていません。コンピュータから見れば、「猫」と「ねこ」は「猫」と「机」くらい違うものなのです。

では、どうすればコンピュータに「猫」と「ねこ」が同じものだと理解させることができるのでしょうか。

この問題は日本語に限った話ではありません。英語でも「color」と「colour」（アメリカ英語とイギリス英語の違い）、「organize」と「organise」のようなスペルの揺れがあります。さらに厄介なのは、「car」と「automobile」のように、綴りがまったく異なるのに同じ意味を持つ単語の存在です。

人間なら文脈から「ああ、同じことを言っているな」と理解できます。しかしコンピュータにとっては、文字列が違えば別物です。この「意味の同一性」をコンピュータにどう理解させるか——これが自然言語処理という分野が長年取り組んできた根本的な課題であり、広聴AIを実現するための出発点でもあります。

### 11.2.3 シノニム辞書による解決とその限界

この問題に対する従来のアプローチは、同義語の対応表（シノニム辞書）を人力で整備することでした。「パソコン」と「PC」、「自動車」と「車」と「クルマ」と「カー」、「携帯電話」と「スマートフォン」と「スマホ」といった対応関係を、一つひとつ辞書に登録していく地道な作業です。

日本語の場合、漢字の開き（「猫」と「ねこ」）やカタカナ表記（「ネコ」）程度であれば機械的な変換も可能ですが、意味に基づく同義語の対応は人間が判断するしかありませんでした。2010年頃までは、検索エンジンや自然言語処理システムの開発において、このシノニム辞書の整備が重要な作業として位置づけられていました。検索精度を上げるために、専門のチームが膨大な時間をかけて同義語を収集・登録していたのです。

しかし、人力での辞書整備には本質的な限界がありました。

**網羅性の問題**

新しい言葉は日々生まれています。「インスタ映え」「バズる」「推し活」といった新語、業界特有の専門用語、さらには若者言葉やネットスラングまで含めると、すべてを網羅することは現実的に不可能です。辞書を整備している間にも、新しい言葉が次々と生まれてきます。

**文脈を無視してしまう問題**

「ネコ」は通常は動物を指しますが、建設現場では一輪車のことを「ネコ」と呼びます。「Apple」は果物かもしれませんし、IT企業かもしれません。「クラウド」は雲なのか、クラウドコンピューティングなのか。単純な対応表では、こうした文脈による意味の違いを捉えることができません。同義語辞書に「Apple＝りんご」と登録してしまうと、「AppleがiPhoneを発表した」という文章を正しく処理できなくなってしまいます。

**スケーラビリティの問題**

シノニム辞書は言語ごとに整備する必要があり、多言語対応には膨大なコストがかかります。英語、日本語、中国語、フランス語……と対応言語を増やすたびに、ゼロから辞書を構築しなければなりません。しかも言語間で概念が一対一に対応するとは限りません。日本語の「木漏れ日」に相当する英単語は存在しませんし、ドイツ語の「Schadenfreude（他人の不幸を喜ぶ感情）」を日本語の一語で表すことはできません。

こうした限界から、「同義語の判定を機械に任せたい」という要求が自然と生まれました。人間が一つひとつ対応関係を教えるのではなく、大量のテキストデータから機械が自動的に「似ている言葉」を学習できないか——この課題を解決するために登場したのが、次節で紹介するWord2Vecです。

## 11.3 テキストのベクトル化：Word2VecからBERTへ

### 11.3.1 Word2Vecの登場（2013年）

Word2Vecは、Googleが2013年に発表したアルゴリズムで、自然言語処理の分野に革命をもたらしました。その基本的な考え方は、「同じような前後文脈で使われる単語は、同じような意味を持つだろう」というものです。

この考え方は言語学では「分布仮説（distributional hypothesis）」と呼ばれ、1950年代から提唱されていました。Word2Vecはこの仮説を大規模データと機械学習で実現したものです。標準的な実装では、対象の単語の前後5単語を参照して意味を推定します。

例えば、大量のテキストを分析すると、「○○を飼っている」「○○が膝の上で丸くなった」「○○がにゃーと鳴いた」といった文脈では、○○に「猫」「ねこ」「ネコ」のいずれも入りうることがわかります。同じような文脈で使われているなら、これらは同じ意味なのだろう——Word2Vecはこの推論を数学的に行います。

さらに、「○○を散歩に連れて行く」「○○に餌をあげる」「○○を動物病院に連れて行く」といった文脈を見ると、ここには「猫」だけでなく「犬」「うさぎ」なども入りうることがわかります。こうして「猫」「犬」「うさぎ」はペットという近い概念だと学習されます。一方、「○○を運転する」「○○に乗り込む」といった文脈には「車」「バス」「タクシー」が入ります。文脈パターンが異なるので、「猫」と「車」は遠い概念として配置されます。

Word2Vecの画期的な点は、人間が明示的に「『猫』と『ねこ』は同じ」と教えなくても、大量のテキストデータから自動的にその関係性を学習できることでした。

### 11.3.2 単語をベクトルで表現する

単語の「同じ」や「近しい」を数学的に表現するにはどうしたらいいでしょうか。Word2Vecのアプローチは、単語を多次元空間上の点（ベクトル）として表現し、その点の間の距離を考えることです。同じような使われ方をしている単語は、近しい位置に配置されるように自動調整されます。

Word2Vecのあるモデルを使うと、「猫」という単語が300次元のベクトルに変換されます。

```python
model["猫"]
# => array([-0.0938, -0.5717, -0.0722,  0.1387,  0.0142, -0.1156,  0.4258,
#           -0.4551, -0.2258,  0.5309,  0.1736, -0.1482,  0.0961,  0.1825,
#           ... (300次元分続く)
```

300次元というのは、300個の数値の組み合わせで単語の意味を表現するということです。人間には300次元の空間を直感的に理解することは難しいですが、コンピュータにとっては単なる数値の配列であり、容易に計算できます。

ベクトル化することで、文字列の完全一致でなくとも、コンピュータは「近さ」を計算できるようになりました。「猫」に近い単語を計算すると、以下のような結果が得られます。

```python
model.most_similar("猫")
# => [('ネコ', 0.7426), ('ねこ', 0.6689), ('仔猫', 0.6409),
#     ('ウサギ', 0.6322), ('子猫', 0.6317), ('犬', 0.6310), ...]
```

「猫」「ねこ」「ネコ」が近い位置にあり、さらにペットの仲間である「ウサギ」「犬」も近くに配置されていることがわかります。

TODO: 図を挿入「Word2Vecによる単語の空間配置」
- 2次元または3次元に圧縮した空間上に単語が点として配置されている図
- 「猫」「ねこ」「ネコ」「犬」「ウサギ」などが近くに集まり、「車」「バス」などは離れた位置にあることを示す

### 11.3.3 コサイン類似度：ベクトル間の「近さ」を測る

多次元空間において「近い」とは何でしょうか。一般にはコサイン類似度を利用します。コサイン類似度は、2つのベクトルがなす角度のコサイン値で、ベクトルの内積から計算できます。

```
A・B = |A| |B| cosθ  （高校数学Bの範囲）
cosθ = A・B / |A| |B|
```

cosθは、2つのベクトルが同じ方向を向いていれば1に近く、反対方向を向いていれば-1に近くなります（θ=0°のとき1、θ=180°のとき-1）。

```python
cat1 = model["猫"]
cat2 = model["ネコ"]

# word2vecの類似度計算
similarity1 = model.similarity("猫", "ネコ")

# 手動での類似度計算（コサイン類似度の定義通りに計算）
similarity2 = 0
for v1, v2 in zip(cat1, cat2):
    similarity2 += v1 * v2
similarity2 /= (sum(v1 ** 2 for v1 in cat1) ** 0.5)
similarity2 /= (sum(v2 ** 2 for v2 in cat2) ** 0.5)

print(similarity1, similarity2)
# => 0.74265 0.7426499817057948
```

単語が適切なベクトルに変換できれば、意味の近さは内積の計算で求まります。つまり、言語学の問題が空間幾何の問題に変換されたのです。幾何の問題に帰着すれば、ベクトル計算（四則演算）に帰着します。四則演算に帰着すれば、コンピュータで計算できます。

### 11.3.4 ベクトルの足し引きで意味の演算ができる

Word2Vecには、もう一つ有名な特性があります。単語のベクトルを足し引きすることで、意味の演算ができるのです。

```python
# 「王様」から「男」を引いて「女」を足すと「女王」に近くなる
model.most_similar(positive=['王様', '女'], negative=['男'])
# => [('女王', 0.72), ...]

# 「東京」から「日本」を引いて「フランス」を足すと「パリ」に近くなる
model.most_similar(positive=['東京', 'フランス'], negative=['日本'])
# => [('パリ', 0.68), ...]
```

この現象は、Word2Vecが単なる単語の類似性だけでなく、単語間の「関係性」もベクトル空間に埋め込んでいることを示しています。「王様と女王の関係」と「男と女の関係」が、ベクトル空間上で同じ方向の差分として表現されているのです。

TODO: 図を挿入「Word2Vecによる意味の演算」
- 「王様」「女王」「男」「女」のベクトル関係を示す図
- 「王様」-「男」+「女」=「女王」という演算が視覚的にわかる図

### 11.3.5 日本語のトークナイズ

ここまでWord2Vecの仕組みを見てきましたが、実際に日本語で使うには一つ準備が必要です。Word2Vecは「単語」を単位として学習しますが、日本語には英語のように単語間のスペースがありません。「私は猫が好きです」という文章を、どこで区切ればよいのでしょうか。

ここで必要になるのが形態素解析です。形態素解析とは、文章を意味を持つ最小単位（形態素）に分割する処理のことです。

```python
from janome.tokenizer import Tokenizer
tok = Tokenizer()
text = "私は猫が好きです"
tokens = tok.tokenize(text)
print(",".join([token.surface for token in tokens]))
# => 私,は,猫,が,好き,です
```

「私は猫が好きです」が「私/は/猫/が/好き/です」と6つの形態素に分割されました。Word2Vecは、この分割された単位を基準にして前後の文脈を学習します。日本語でWord2Vecを使うには、まず形態素解析で文章を単語に分割し、その結果を入力として渡す必要があります。

### 11.3.6 Word2Vecの限界とBERTの登場

Word2Vecは画期的な技術でしたが、一つ大きな限界がありました。同じ単語は常に同じベクトルになるため、文脈によって意味が変わる多義語を適切に扱えないのです。

例えば、「bank」という英単語は文脈によって「川岸・土手」や「銀行」を意味します。しかしWord2Vecでは単語単位でしか学習できていないので、以下の2つの文章の"bank"は同じベクトルになってしまいます。

- 文1: "He sat by the bank of the river."（川岸）
- 文2: "She deposited money in the bank."（銀行）

日本語の「ネコ」も実は多義的です。「土木作業員は『ネコを取ってきてくれ』と言った」という文章であれば、「ネコ」は一輪車のことを指している可能性が高いでしょう。

この課題を解決したのがBERT（2018年）です。BERTは「Bidirectional Encoder Representations from Transformers」の略で、Googleが2018年に発表しました。「Bidirectional（双方向）」という名前が示すように、ある単語の意味を理解するために、その単語の前後両方の文脈を同時に参照します。

BERTでは、ベクトル表現された単語の値を、前後のベクトルと共に計算を行うことで、その文脈にあった意味を持つ値に変換します。この「前後の文脈を考慮する」仕組みこそが、BERTの革新的な点です。

上記の例では、"river"という単語が近くにあることで"bank"は「川岸」のベクトルに、"money"や"deposited"という単語が近くにあることで"bank"は「銀行」のベクトルになります。BERTの登場によって、Googleでは自然文による検索の精度が大きく改善したとされています（2019年頃の話）。

### 11.3.7 文章全体のベクトル化

BERTのもう一つの重要な機能は、文章全体の意味をベクトル化できることです。文章のベクトル化自体は2014年頃から行われていましたが（doc2vecなど）、性能が向上してうまく機能するようになったのはBERT以降です。

BERTの処理では、文章の開始地点に必ず[CLS]（Classification）という特殊なトークンが挿入されます。BERTの計算が終わると、この[CLS]トークンの位置のベクトルは、文章全体の意味を表現するベクトルに変換されています。

TODO: 図を挿入「BERTの処理の流れ」
- 入力文章が[CLS]トークンと共にトークン化される様子
- 各トークンがベクトル化され、Transformerで処理される様子
- [CLS]トークンの位置に文章全体の意味ベクトルが出力される様子

### 11.3.8 文脈ベクトルとベクトル検索

文章をベクトル化できるようになったことで、文章間の「近さ」も計算できるようになりました。以下の9つの文章をベクトル化して、コサイン類似度を計算してみましょう。

**料理関連**
- トマトソースのパスタを作るのが好きです
- 私はイタリアンの料理が得意です
- スパゲッティカルボナーラは簡単においしく作れます

**天気関連**
- 今日は晴れて気持ちがいい天気です
- 明日の天気予報では雨が降るようです
- 週末は天気が良くなりそうで外出するのに最適です

**技術関連**
- 新しいスマートフォンは処理速度が速くなりました
- 最新のノートパソコンはバッテリー持ちが良いです
- ワイヤレスイヤホンの音質が向上しています

これらの文章をSentenceTransformerなどのモデルでベクトル化し、相互にコサイン類似度を算出すると、同一ジャンルの文章間では類似度が高く、異なるジャンルの文章間では類似度が低くなります。また、同一ジャンル内でも「晴れ」と「雨」のように意味が異なる文章ではコサイン類似度が下がる傾向があります。

TODO: 図を挿入「文章間のコサイン類似度ヒートマップ」
- 9つの文章の相互類似度を示す9×9のヒートマップ
- 料理関連、天気関連、技術関連がそれぞれブロックとして高い類似度を示している様子

この「文脈ベクトル」と「コサイン類似度」の組み合わせが、「ベクトル検索」のコアとなる技術です。

従来のキーワード検索では、ユーザが入力した単語と完全一致または部分一致するドキュメントしか見つけられませんでした。「犬の飼い方」で検索しても、「ペットの育て方」という文書は見つかりません。しかしベクトル検索では、文章の「意味」を比較するため、表現が異なっていても意味的に近い文書を発見できます。

この技術は、RAG（Retrieval-Augmented Generation）の基盤としても使われています。LLMに回答させる際に、質問文に意味的に近いドキュメントをベクトル検索で取得し、それを参考情報としてLLMに渡すことで、より正確な回答を生成させることができます。

### 11.3.9 エンベディングAPI

エンベディング（embedding）とは、言葉や文章をベクトルの世界に変換することです。「埋め込み」と呼ばれることもあります。機械学習によって、似た意味の言葉は高次元空間において近い位置に配置されます。

現在、エンベディングは様々なAPIやライブラリとして提供されており、簡単に利用できます。

**OpenAI Embeddings API**

```python
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
    input="Your text string goes here",
    model="text-embedding-3-small"
)

print(response.data[0].embedding)
# => 1536次元のベクトルが返ってくる
```

**SentenceTransformers（オープンソース）**

```python
from sentence_transformers import SentenceTransformer
model_name = "sentence-transformers/paraphrase-multilingual-mpnet"
emb_model = SentenceTransformer(model_name)
data = ["hello world"]
emb = emb_model.encode(data)
print(emb)
# => 768次元のベクトルが返ってくる
```

広聴AIでは、OpenAIの`text-embedding-3-small`モデルを使用して意見をベクトル化しています。このモデルは1536次元のベクトルを出力します。

## 11.4 大規模言語モデル（LLM）

### 11.4.1 BERTからGPTへ：理解から生成へ

BERTは文章を理解する方向性で開発された技術なので、文章生成には適していませんでした。BERTは「穴埋め問題」を解くように設計されており、文章の途中にある欠けた単語を予測するのは得意ですが、文章を続けて書くことは苦手でした。

一方、GPTは単語の分散表現の列から次の単語を予測する方向に進化し、文章生成が可能になりました。GPTは「Generative Pre-trained Transformer」の略で、OpenAIが開発しました。名前に「Generative（生成的）」とあるように、文章を生成することを目的として設計されています。

### 11.4.2 GPTの仕組み：次の単語を予測するAI

GPTの処理では、入力されたテキストに基づいて、次に来る単語の確率を計算します。例えば、"He sat by the bank of the ???"という文章に対して、次に来る単語の確率が以下のように計算されます。

```
river   35%
stream  25%
creek   15%
pond     6%
lake     6%
canal    4%
...
```

過去の言葉から次の言葉を予測することは、次の言葉を紡ぐことと同義です。GPTの文章生成の流れは以下の通りです。

1. 入力された文章をトークン化
2. トークンをベクトル化
3. Transformerで処理して、次のトークンの確率分布を計算
4. 確率に基づいて次のトークンを選択
5. 選択されたトークンを追加して、2〜4を繰り返す
6. 生成されたトークン列を文章に戻す

「繋がりそうな単語を繋いでいく」という、ただそれだけで、「知性」と呼んでも差し支えない能力が得られてしまいました。

なぜ「次の単語を予測する」だけで知性が生まれるのでしょうか。それは、正しく次の単語を予測するためには、文章の内容を「理解」している必要があるからです。

例えば、「日本の首都は」という文章の次に来る単語を正しく予測するには、「日本」という国の「首都」が「東京」であるという知識が必要になります。「ピタゴラスの定理によれば、直角三角形の斜辺の二乗は」という文章の続きを予測するには、数学的な知識が必要です。

大量のテキストで「次の単語の予測」を学習する過程で、LLMは言語のパターンだけでなく、世界についての膨大な知識を獲得しました。これが「創発」と呼ばれる現象であり、単純なタスクの学習が、予期せぬ高度な能力を生み出したのです。

### 11.4.3 トークナイズ：自然言語を数値に変換する

自然言語はそのままの形ではAIに入力できないので、内部的に数値（トークン）に変換します。この変換がトークナイズです。

出現頻度に応じてトークンの割り当てが行われており、英語では頻出単語は1トークンになり、複雑な単語は途中で分割されます。ChatGPTのトークナイザでは、日本語の文書は1文字が約1.3トークンになります（平易な文章では1文字1.1トークン程度）。

「日本」「自由」のような出現頻度が高い単語は1トークンにまとめられています。一方、出現頻度の低い「挙」「孫」「諸」「恵」「禍」「権」「憲」などは1文字が複数のトークンに分割されます。

日本語が英語に比べて推論性能がやや低いのは、英語では「次の単語」を予測するのに対し、日本語では「次の文字（の一部）」を予測しているからではないかという議論もあります。また、トークン数はAPI利用料金の計算基準にもなっています。同じ内容を伝える場合、日本語は英語よりも多くのトークンを消費するため、API利用コストが高くなる傾向があります。

### 11.4.4 ChatGPTの登場とその衝撃

GPT-3のAPIは2020年9月から提供されていましたが、そこまで話題にはなりませんでした。GPT-3は「次の単語を予測する」AIでしたが、それを使いこなすには「次の言葉を予測させるための独り言」を入力する必要があり、これができる人が限られていたのです。

一方で、プログラマ界隈では「これは新しいプログラミングの形だ、プロンプトエンジニアリングだ」と話題になりました。プログラムで課題を解決するのではなく、特殊な穴埋め問題を作ることで、LLMに埋め込まれた常識を利用して課題を解決することができます。従来のプログラムが扱いにくかった「不定形な自然文の入出力」が行えるようになりました。

2022年11月、OpenAIはChatGPTを公開しました。ChatGPTでは、Instruction Tuning（対話履歴を学習させる手法）によって、自然な対話が可能になりました。発話者という特殊なトークンを導入することで、対話履歴を学習することが可能になったのです。

- GPTは「次の単語を予測して生成するAI」でした
- ChatGPTはそこから発展して「次の対話を予測し、生成するAI」になりました

この違いは決定的でした。ChatGPTでは、普通の言葉で質問すれば、普通の言葉で回答が返ってきます。結果として、一般人が触れるようになり爆発的に普及しました。

2023年3月、OpenAIはChatGPTにGPT-4を搭載し、ChatGPT Plusで有料提供を開始しました。GPT-4により、性能が飛躍的に向上しました。GPT-4は司法試験に合格できるくらいのスコアを記録し、日本の医師国家試験も突破しました。最近ではo3というモデルで東大入試の理IIIも突破しています。

様々な用途への利用が現実的なラインを超えてきました。東大に入れるAIが月20ドルで使い放題という状況なので、これを使いこなすスキルが求められています。さらにはAPI経由で「東大に入れるAI」を誰でもプログラムに組み込める状態になりました。

### 11.4.5 LLMをプログラムに組み込む

LLMをプログラムに組み込む例として、ColorGPTを紹介します。これはChatGPTのAPIを利用した最初期のアプリで、Webブラウザからカメラを起動し、現実世界の色のRGBの値から色の名前に変換するソフトウェアです。

```javascript
const GPT35TurboMessage = (question: string) => [
  {
    role: "system",
    content: `You're a designer give the closest base color name from hex code. Answer with short and simple name.`,
  },
  {
    role: "user",
    content: "Color name of #af6e4d ?",
  },
  {
    role: "assistant",
    content: "brown",
  },
  { role: "user", content: `Color name of ${question} ?` },
];
```

カラーコードと色名の対応表を人力で作るのは大変な作業ですが、ChatGPTを使えば一発で処理できます。たったこれだけの指示で、色変換を実現しています。

この例では「Few-shot Learning」という手法を使っています。LLMに少数の正解となる応答パターンを見せる（受け答えの履歴を捏造する）ことで、LLMに「空気を読ませて」目的の出力を行わせる手法です。特定のフォーマットへの準拠などは、この手法でうまくいくことが多いです。

### 11.4.6 LLMの得意なことと限界

LLMは「論理的思考力」と「常識」を提供します。カラーコードと色の名前の対応はある種の「常識」です。東大の理IIIの入試に合格できる程度の論理的思考力も持ち合わせています。

一方でLLMには「常識」はありますが「知識」はありません。ここでいう「常識」と「知識」の違いを明確にしておきましょう。

- **常識**：多くの文章に繰り返し登場するため、学習データに十分な量が含まれている情報。「太陽は東から昇る」「水は100度で沸騰する」「パリはフランスの首都」など。
- **知識**：特定のドメインや最新の情報など、学習データに十分な量が含まれていない可能性がある情報。「○○社の今期の売上」「△△市の条例の詳細」など。

LLMは学習データに十分な量が含まれていない情報を聞かれたとき、「わからない」と答える代わりに、もっともらしい（しかし事実ではない）回答を生成してしまうことがあります。これがハルシネーション（幻覚）です。

LLMの「知識」の限界を補う手法として、RAG（Retrieval-Augmented Generation：検索拡張生成）があります。RAGは、ユーザーの質問に関連する情報をデータベースやウェブから検索し、その情報をLLMに渡して回答を生成させる手法です。これにより、LLMの学習データに含まれていない最新情報や専門的な知識も扱えるようになります。

ただし、RAGもハルシネーションの問題を完全に解決するわけではありません。検索で取得したウェブページや文書の内容が誤っていたり、質問に対して不適切な情報だった場合、LLMはその誤った情報をもとに、もっともらしい（しかし事実ではない）回答を生成してしまいます。RAGは「LLMに正しい情報を渡せば正しく答えられる」という前提に立っているため、情報源の品質管理が重要になります。

Microsoft Copilotなどの企業向けAIアシスタントでは、SharePointやOneDriveといった社内データソースと連携し、組織内の文書やデータを検索して回答を生成できるようになっています。これにより、「○○社の今期の売上」や「△△プロジェクトの進捗」といった、従来のLLMでは答えられなかった社内固有の情報にも対応可能になりました。ただし、この場合も社内文書の品質や正確性がそのまま回答の品質に影響します。

コードで書くのは大変だが、常識と論理的思考力で処理できる問題であれば、LLMは極めてうまく機能します。広聴AIでは、文章分割や要約といったタスクでLLMを活用しています。これらは「知識」を必要とせず、文章の構造や意味を理解して処理する「常識」的なタスクだからです。

### 余談： なぜLLMはハルシネーションを起こすのか？
OpenAIの調査報告によれば、ハルシネーションが起きる根本的な理由は、標準的な学習と評価の手順にあります。多くの評価では「正解した質問の割合」（正確性）のみでモデルを評価するため、モデルは「わかりません」と答える代わりに推測することを選ぶようになります。これは多肢選択テストで「当てずっぽうでも正解になる可能性があるが、空欄なら確実にゼロ点」という状況に似ています。

この問題に対し、GPT-5系統では評価と学習の設計思想が変更されました。

- **従来のモデル（GPT-4oなど）**：回答に対して「正解」「不正解」で報酬を与えて学習。不確かでも推測した方がスコアが上がるため、誤った答えでも自信満々に回答する傾向がある
- **GPT-5系統**：回答に対して「正解」「不正解」「棄権」の3カテゴリで評価。自信を持った間違いには大きなペナルティを課し、不確実性を適切に表現した場合（「わかりません」と答える場合）には点数の一部を付与

OpenAIのSimpleQA評価では、o4-miniのエラー率が75%であるのに対し、gpt-5-thinking-miniは26%まで低下しています（棄権率は52%）。ハルシネーションが完全になくなったわけではありませんが、「知らないことは知らないと言う」ことができるようになり、信頼性は大きく向上しています。

https://openai.com/ja-JP/index/why-language-models-hallucinate/

### 11.4.7 Structured Output：LLMをシステムに組み込む

2024年8月、OpenAIはAPIにStructured Output機能を搭載しました。ユーザが指定したJSONフォーマットで出力させることが可能になったのです。

Structured Outputによって、LLMは「自然言語を入力すると、JSONが出力されるモジュール」として、システムの一部に組み込み可能になりました。それまでのLLMは自然言語がそのまま出力されるため、システムに組み込みづらかったのです。

JSONのフォーマットを例示することで、90%くらいの確率で準拠させることは可能でしたが、それでも10%はエラーが起こります。10%の確率でエラーが起こるモジュールをシステムに組み込むのは困難です。

```python
from pydantic import BaseModel, Field
import openai

client = openai.Client()

class Character(BaseModel):
    name: str = Field(description="キャラクターの名前")
    age: int = Field(description="キャラクターの年齢")
    occupation: str = Field(description="キャラクターの職業")
    description: str = Field(description="キャラクターの説明")
    strength: int = Field(description="キャラクターの強さ")
    agility: int = Field(description="キャラクターの敏捷性")
    intelligence: int = Field(description="キャラクターの知性")

result = client.beta.chat.completions.parse(
    model="gpt-4o",
    messages=[{"role": "user", "content": "架空のキャラクターを作成して"}],
    n=1,
    response_format=Character
).choices[0].message.parsed

print(result.model_dump_json(indent=2))
# => {
#   "name": "Haru Yamada",
#   "age": 28,
#   "occupation": "Urban Explorer / Photographer",
#   ...
# }
```

JSON Schemaで型を渡すと、その型を満たすJSONを返してくれます。これにより、こちらが狙ったフォーマットでの出力を行わせることができ、プログラムに組み込みやすくなりました。

Structured Outputの登場は、LLMのシステム組み込みにおける大きな転換点となりました。従来は、LLMの出力をパースするための複雑なコードが必要で、エラーハンドリングも煩雑でした。Structured Outputにより、LLMは「文字列を入力すると構造化データを返す関数」として扱えるようになり、従来のプログラミングの枠組みにシームレスに統合できるようになりました。

2025年現在、Structured OutputはOpenAI（GPTシリーズ）だけでなく、Anthropic（Claude）、Google（Gemini）など主要なLLMプロバイダーがすべてサポートしています。PythonのPydanticやTypeScriptのZodといったスキーマ定義ライブラリとの連携も標準化されており、どのLLMを使っても同様の開発体験が得られるようになりました。

広聴AIでも、意見の抽出やラベリングの際にStructured Outputを活用しています。これにより、LLMの出力を確実にJSONとして受け取り、後続の処理に渡すことができます。

## 11.5 クラスタリングと次元圧縮

### 11.5.1 クラスタリングとは

クラスタリングとは、似た特徴をもつデータ同士を自動でグループ分けすることです。「クラスタ（cluster）」は英語で「房」や「集団」を意味し、ブドウの房のように似たものが集まっている状態をイメージするとわかりやすいでしょう。

機械学習の分類では、クラスタリングは「教師なし学習」に分類されます。「教師あり学習」では正解データを用意して「この画像は猫、これは犬」と教える必要がありますが、「教師なし学習」では正解を与えずに、データの特徴だけからグループを自動的に発見します。

### 11.5.2 代表的なクラスタリングアルゴリズム

同じデータに対して、様々なクラスタリングアルゴリズムを適用すると、それぞれ異なる結果が得られます。

TODO: 図を挿入「クラスタリングアルゴリズムの比較」
- 同じ2次元データセットに対して、k-means、階層的クラスタリング、DBSCAN、SpectralClusteringを適用した結果を並べて表示
- それぞれのアルゴリズムがどのようにデータを分割するかの違いがわかる図

代表的なクラスタリングアルゴリズムには以下のようなものがあります。

- **k-means**：データをk個のグループに分割します。各グループの中心点（セントロイド）を繰り返し更新することで、データを分類します。シンプルで高速ですが、クラスタ数kを事前に指定する必要があります。

- **階層的クラスタリング（Ward法など）**：最も近いデータ同士を順次統合していき、木構造（デンドログラム）を作成します。クラスタ数を事前に決めなくてよいですが、計算コストが高いです。

- **DBSCAN**：密度の高い領域をクラスタとして認識します。ノイズの検出が得意ですが、パラメータ調整が難しいです。

- **SpectralClustering**：データ間の類似度行列を使い、グラフ理論に基づいて分割します。複雑な形状のクラスタを検出できますが、計算コストが高いです。

### 11.5.3 クラスタリングと広聴AI

埋め込みベクトルは数値の集合です。同じような意味の言葉や文章は、ベクトル空間上で近い位置にあります。コサイン類似度やユークリッド距離で、ベクトル間の距離が計測可能です。

適切にクラスタリングを行うことで、同じような文脈ベクトルを集約できます。つまり、同じような意見を集約できるのです。これがTTTCや広聴AIの発想の根源です。

広聴AIではWard法という階層的クラスタリングアルゴリズムが採用されています。Ward法は、クラスタを統合する際に、統合後のクラスタ内分散が最小になるようにペアを選ぶ手法です。「分散が最小になる」とは、統合後のクラスタ内のデータ点ができるだけ近くに集まっている状態を選ぶということです。言い換えれば、「似たもの同士を優先的にまとめる」アルゴリズムです。

TODO: 図を挿入「階層的クラスタリングのデンドログラム」
- 階層的クラスタリングの結果を示す樹形図（デンドログラム）
- 下から上に向かって個々のデータ点が統合されていく様子
- 閾値を設定してクラスタ数を決める様子

デンドログラム（樹形図）は、階層的クラスタリングの結果を視覚化したものです。下から上に向かって読むと、最初は個々のデータ点がそれぞれ独立したクラスタとして存在し、徐々に統合されていく様子がわかります。縦軸は統合された時点での距離（または分散の増加量）を表します。

閾値を設定して横に線を引くと、その時点でのクラスタ数が決まります。閾値を低く設定すれば細かいクラスタに、高く設定すれば大きなクラスタに分かれます。この柔軟性が階層的クラスタリングの利点の一つです。

### 11.5.4 次元圧縮とは

次元圧縮とは、データの本質だけを残して余分な情報を捨て、「軸」を減らすことで、より少ない数の特徴で表現し直す技術です。広聴AIでは1536次元から2次元といった大幅な圧縮が行われています。

なぜ次元圧縮が必要なのでしょうか。大きく2つの理由があります。

**可視化のため**

人間は3次元（奥行き、高さ、幅）までしか直感的に理解できません。1536次元のベクトル空間をそのまま見ることは不可能です。2次元に圧縮すれば、スクリーン上に散布図として表示できます。

**計算効率のため**

次元が高いほど計算量が増えます。特にクラスタリングでは、「次元の呪い（curse of dimensionality）」と呼ばれる問題が発生し、高次元空間では距離の概念が意味を持たなくなります。次元を下げることで、クラスタリングの精度と効率が向上します。

### 11.5.5 PCAとUMAP

データの次元圧縮にはいくつかの手法があります。

**主成分分析（PCA）**

データの相関を利用した最も基本的なアルゴリズムです。相関のある軸を中心にデータ全体を回転させて、分散が大きい軸を残す手法です。線形の関係を捉えるのは得意ですが、複雑な構造を持つデータには適さない場合があります。

**UMAP（Uniform Manifold Approximation and Projection）**

2018年にLeland McInnesらによって発表された手法で、局所的な距離構造を維持したまま、データをより低次元に写しかえます。曲がったゴムシートを空間に押し付けて、低次元に写しかえるイメージです。TTTCや広聴AIはこのアルゴリズムを利用しています。

UMAPの「局所的な距離構造を維持する」という特性は、広聴AIにとって重要です。グローバルな構造（全体的な配置）よりもローカルな構造（近くにあるもの同士の関係）を優先して保存するため、「似ている意見は近くに配置される」という直感的な可視化が可能になります。

UMAPはt-SNEという先行手法と比較して、計算が高速で、グローバルな構造もある程度保存できるという利点があります。

TODO: 図を挿入「PCAとUMAPの比較」
- MNISTデータセット（手書き数字）をPCAとUMAPでそれぞれ2次元に圧縮した結果
- PCAでは混在した状態、UMAPでは数字ごとにきれいに分離された状態
- 手書きで間違いやすい数字（0と6、2と3と5と8、4と9と7など）が隣接しながら分離されている様子

MNISTデータセット（手書きの数字0〜9）をPCAとUMAPで2次元に次元圧縮して可視化すると、その違いがよくわかります。PCAでは数字が混在した状態になりますが、UMAPではうまく分離できています。さらに、手書きで間違いやすい{0,6}、{2,3,5,8}、{4,9,7}といった数字が隣接しながら分離されています。これは、UMAPが局所的な構造を保存しながら次元削減を行うためです。

## 11.6 広聴AIにおける技術の統合

ここまで解説してきた技術が、広聴AIでどのように組み合わされているかを整理しましょう。

1. **意見の収集**：パブリックコメントやアンケートなどで自由記述の意見を収集する

2. **意見の分割・整形（LLM）**：一つの投稿に複数の意見が含まれている場合、LLMを使って個別の意見に分割する。感情的な表現を除去し、建設的な意見のみを抽出する

3. **ベクトル化（エンベディング）**：分割された意見をエンベディングAPIでベクトルに変換する。似た意見は近いベクトルになる

4. **次元圧縮（UMAP）**：1536次元のベクトルを2次元に圧縮する。これにより、散布図として可視化可能になる

5. **クラスタリング（Ward法）**：似た意見を自動でグループ分けする。階層的クラスタリングにより、大きなグループから細かいグループまで階層構造を持たせる

6. **ラベリング・要約（LLM）**：各クラスタにLLMでラベル（タイトル）と説明文を付与する。全体の要約も生成する

7. **可視化**：2次元座標上に意見を点として配置し、クラスタごとに色分けして表示する

これらの技術が一つのパイプラインとして統合されることで、数千・数万件の意見を短時間で分析・可視化することが可能になります。

次章では、これらの技術が広聴AIの実装でどのように組み合わされているかを、コードレベルで詳しく見ていきます。

## 11.7 本章のまとめ

本章では、広聴AIを支える基盤技術について解説しました。

1. **文字と数値の関係**：コンピュータにとって「猫」と「ねこ」は異なる数値であり、同じものとして扱うには工夫が必要です

2. **Word2Vec**：同じような文脈で使われる単語は同じような意味を持つという考えに基づき、単語をベクトル化します。2013年にGoogleが発表しました

3. **コサイン類似度**：ベクトル間の「近さ」を測る指標です。言語学の問題を空間幾何の問題に変換します

4. **BERT**：文脈を考慮した単語・文章のベクトル化を実現しました。2018年にGoogleが発表しました

5. **LLMとChatGPT**：次の単語・対話を予測することで、知性と呼べる能力を獲得しました。文章分割や要約といったタスクで活用されています

6. **Structured Output**：LLMをシステムに組み込み可能にする技術です。JSONフォーマットでの出力を保証します

7. **クラスタリング**：似た意見を自動でグループ分けします。広聴AIではWard法が採用されています

8. **次元圧縮（UMAP）**：高次元データを人間が理解できる2次元に変換します。似たものが近くに配置される特性があります

これらの技術はそれぞれ独立して発展してきましたが、広聴AIではこれらを巧みに組み合わせることで、大量の意見を分析・可視化することを実現しています。
