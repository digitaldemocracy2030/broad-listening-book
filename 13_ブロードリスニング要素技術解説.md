# 第13章 ブロードリスニングの要素技術

## 13.1 本章の学習目標

本章では、広聴AIを支える基盤技術について、データサイエンスの専門知識がなくても理解できるように解説します。

本章を読み終えると、以下のことが理解できるようになります。

- コンピュータが「言葉の意味」をどのように扱っているか
- テキストを数値（ベクトル）に変換する技術の発展の歴史
- 大規模言語モデル（LLM）がなぜ「知性」のように振る舞えるのか
- 大量のデータを整理・可視化するための技術
- これらの技術が広聴AIでどのように組み合わされているか

各技術について「何をインプットして、どのようなアウトプットを行い、何に使えるのか」という視点で説明していきます。

---

## 13.2 技術の全体像

### 13.2.1 広聴AIを可能にした3つの技術革新

広聴AIは、ここ10年ほどの間に登場した複数の技術を組み合わせることで実現されています。どれか一つが欠けても成り立ちません。

| 技術 | 登場年 | 役割 |
|------|--------|------|
| **Sentence-BERT** | 2019年 | 文章の「意味」をベクトル（数値の配列）に変換する |
| **UMAP** | 2018年 | 高次元データを2次元に圧縮し、人間が見られる形に可視化する |
| **LLM** | 2020年〜 | テキストの理解、分割、要約、ラベル付けを行う |

これらの技術が出揃ったのは、ほんの数年前のことです。つまり広聴AIは、技術的に「やっと可能になった」ばかりの新しい手法なのです。

なお、Sentence-BERTは2018年に登場したBERTを改良したものです。BERTからSentence-BERTへの発展については、13.3節で詳しく解説します。

### 13.2.2 2つの技術系譜の合流

広聴AIの技術は、2つの異なる系譜から発展してきました。

一つ目は、言葉の意味をコンピュータに理解させる自然言語処理の系譜です。

```
シノニム辞書 → Word2Vec → BERT → Sentence-BERT
（人力で同義語を登録）（文脈から学習）（双方向の文脈理解）（文章のベクトル化）
```

そしてBERTの発展形として、テキスト生成に特化したLLMの系譜もあります。

```
BERT → GPT → ChatGPT
（理解）（生成）（対話）
```

二つ目は、データを整理して見やすくする統計・可視化の系譜です。

```
クラスタリング → 次元圧縮 → UMAP
（似たものを集める）（軸を減らす）（局所構造を保存して圧縮）
```

これらの流れが合流することで広聴AIが実現しました。Sentence-BERTで言葉をベクトルに変換し、コサイン類似度でその距離を測り、クラスタリングで近いものを集め、UMAPで可視化し、LLMでラベル付けする。これが広聴AIの基本的な仕組みです。

### 13.2.3 「同じ」「似ている」を理解させる挑戦

技術の歴史を振り返ると、一つの問いが長年にわたって追求されてきたことがわかります。

> **「コンピュータに『同じ』『似ている』をどう理解させるか」**

人間にとって「猫」と「ねこ」は同じものを指しています。しかしコンピュータにとっては、これらはまったく別のデータです。この「意味の同一性」をコンピュータにどう理解させるか、これが自然言語処理という分野が長年取り組んできた根本的な課題であり、広聴AIを実現するための出発点でもあります。

---

## 13.3 言葉の意味をベクトル化する：Word2VecからSentence-BERTへ

### 13.3.1 文字は数値である

コンピュータの中では、文字はすべて数値として扱われています。私たちが画面上で「あ」や「A」という文字を見ているとき、コンピュータの内部ではそれらは特定の数値として処理されています。

試しにPythonで「デジタル民主主義」という文字列を数値に変換してみましょう。

```python
>>> [c for c in "デジタル民主主義".encode("utf-8")]
[227, 131, 135, 227, 130, 184, 227, 130, 191, 227, 131, 171,
 230, 176, 145, 228, 184, 187, 228, 184, 187, 231, 190, 169]
```

「デジタル民主主義」という8文字の文字列が、24個の数値の配列に変換されました。UTF-8というエンコーディング方式では、日本語の1文字は通常3バイト（3つの数値）で表現されます。

私たちが文字として認識しているものは、コンピュータにとっては単なる数値の並びに過ぎません。

### 13.3.2 「猫」と「ねこ」問題

人間にとって「猫」と「ねこ」は同じものを指しています。漢字で書こうがひらがなで書こうが、四本足でにゃーと鳴くあの動物のことです。しかし、コンピュータにとってこれらはまったく別のものです。

```python
"猫".encode("utf-8").hex()    # => 'e78cab'
"ねこ".encode("utf-8").hex()  # => 'e381ade38193'
```

「猫」は`0xE78CAB`、「ねこ」は`0xE381ADE38193`という数値になります。この2つの数値は似ているでしょうか？まったく似ていません。コンピュータから見れば、「猫」と「ねこ」は「猫」と「机」くらい違うものなのです。

そして、コンピュータは四則演算（足し算、引き算、割り算、掛け算）と比較演算（等しい、より大きい、より小さい）しかできません。したがって、コンピュータは「猫」と「ねこ」が同じものだと理解できないのです。

では、どうすればコンピュータに「猫」と「ねこ」が同じものだと理解させることができるのでしょうか。

### 13.3.3 シノニム辞書による解決とその限界

この問題に対する従来のアプローチは、同義語の対応表（シノニム辞書）を人力で整備することでした。

- 「パソコン」=「PC」
- 「自動車」=「車」=「クルマ」=「カー」
- 「携帯電話」=「スマートフォン」=「スマホ」

このような対応関係を、一つひとつ辞書に登録していく地道な作業です。2010年頃までは、検索エンジンや自然言語処理システムの開発において、このシノニム辞書の整備が重要な作業として位置づけられていました。

しかし、人力での辞書整備には本質的な限界がありました。

| 限界 | 説明 |
|------|------|
| 網羅性の問題 | 新語（「インスタ映え」「推し活」など）が日々生まれ、すべてを網羅することは不可能 |
| 文脈の問題 | 「ネコ」は動物だが、建設現場では一輪車のこと。「Apple」は果物かIT企業か |
| スケーラビリティの問題 | 言語ごとに辞書整備が必要。多言語対応には膨大なコスト |

こうした限界から、「同義語の判定を機械に任せたい」という要求が自然と生まれました。

### 13.3.4 Word2Vecの革新（2013年）

Word2Vecは、Googleが2013年に発表したアルゴリズムで、自然言語処理の分野に革命をもたらしました。

Word2Vecの基本的な考え方は、「同じような前後文脈で使われる単語は、同じような意味を持つだろう」というものです。この考え方は言語学では「分布仮説」と呼ばれ、1950年代から提唱されていました。Word2Vecはこの仮説を大規模データと機械学習で実現したものです。

では、具体的にどのように学習するのでしょうか。大量のテキストを分析すると、以下のようなパターンが見えてきます。

- 「○○を飼っている」「○○が膝の上で丸くなった」→ ○○には「猫」「ねこ」「ネコ」が入る
- 「○○を散歩に連れて行く」「○○に餌をあげる」→ ○○には「猫」「犬」「うさぎ」が入る
- 「○○を運転する」「○○に乗り込む」→ ○○には「車」「バス」「タクシー」が入る

Word2Vecは、このような文脈パターンから「猫」「ねこ」「ネコ」が同じ意味だと学習し、「猫」と「犬」は近い概念（ペット）、「猫」と「車」は遠い概念だと自動的に学習します。

この手法の画期的な点は、人間が明示的に「『猫』と『ねこ』は同じ」と教えなくても、大量のテキストデータから自動的にその関係性を学習できるようになったことです。

### 13.3.5 単語をベクトルで表現する

Word2Vecは、単語を多次元空間上の点（ベクトル）として表現します。同じような使われ方をしている単語は、近しい位置に配置されます。

```python
model["猫"]
# => array([-0.0938, -0.5717, -0.0722, 0.1387, 0.0142, ...])
# 300個の数値で「猫」の意味を表現
```

300次元というのは、300個の数値の組み合わせで単語の意味を表現するということです。人間には300次元の空間を直感的に理解することは難しいですが、コンピュータにとっては単なる数値の配列であり、容易に計算できます。

「猫」に近い単語を計算すると、以下のような結果が得られます。

```python
model.most_similar("猫")
# => [('ネコ', 0.74), ('ねこ', 0.67), ('仔猫', 0.64),
#     ('ウサギ', 0.63), ('子猫', 0.63), ('犬', 0.63), ...]
```

「猫」「ねこ」「ネコ」が近い位置にあり、さらにペットの仲間である「ウサギ」「犬」も近くに配置されていることがわかります。

### 13.3.6 コサイン類似度：ベクトル間の「近さ」を測る

多次元空間において「近い」とは何でしょうか。一般にはコサイン類似度を利用します。

コサイン類似度とは、2つのベクトルがなす角度のコサイン値で、-1から1の範囲の値を取ります。

| 値 | 意味 |
|----|------|
| 1に近い | 同じ方向を向いている（意味が似ている） |
| 0に近い | 直交している（関連がない） |
| -1に近い | 反対方向を向いている（意味が反対） |

```python
# word2vecの類似度計算
model.similarity("猫", "ネコ")  # => 0.74265
```

ここで重要なのは、言語学の問題が空間幾何の問題に変換されたということです。幾何の問題に帰着すれば、ベクトル計算（四則演算）に帰着します。四則演算に帰着すれば、コンピュータで計算できます。

---

**【コラム】ベクトル演算の不思議「王様 − 男 + 女 = 女王」**

Word2Vecには、驚くべき特性があります。単語のベクトルを足し引きすることで、意味の演算ができるのです。

```python
# 「王様」から「男」を引いて「女」を足すと「女王」に近くなる
model.most_similar(positive=['王様', '女'], negative=['男'])
# => [('女王', 0.72), ...]

# 「東京」から「日本」を引いて「フランス」を足すと「パリ」に近くなる
model.most_similar(positive=['東京', 'フランス'], negative=['日本'])
# => [('パリ', 0.68), ...]
```

この現象は、Word2Vecが単なる単語の類似性だけでなく、単語間の「関係性」もベクトル空間に埋め込んでいることを示しています。

「王様と女王の関係」と「男と女の関係」が、ベクトル空間上で同じ方向の差分として表現されているのです。これは人間が明示的に教えたわけではなく、大量のテキストから自動的に学習された結果です。

---

### 13.3.7 Word2Vecの限界とBERTの登場（2018年）

Word2Vecは画期的な技術でしたが、一つ大きな限界がありました。同じ単語は常に同じベクトルになるため、文脈によって意味が変わる多義語を適切に扱えないのです。

例えば英語の「bank」という単語は、文脈によって意味が変わります。

- 文1: "He sat by the bank of the river."（川岸）
- 文2: "She deposited money in the bank."（銀行）

日本語にも多義語は多くあります。「甘い」は、「このケーキは甘い」では味覚を、「審査が甘い」では基準の緩さを、「見通しが甘い」では判断の楽観性を意味します。「頭」も、「頭が痛い」では身体の部位を、「組織の頭」ではリーダーを、「頭がいい」では知性を指します。「金」も、「金を稼ぐ」ではお金を、「金メダル」では金属を、「金曜日」では曜日を意味します。

しかしWord2Vecでは、これらの異なる意味の「bank」も「甘い」も「頭」も「金」も、同じ言葉であれば、すべて同じベクトルになってしまいます。このような簡単な単語でさえ文脈を無視してしまうのでは、実用的な自然言語処理には限界があります。

この問題を解決したのがBERTです。BERT（Bidirectional Encoder Representations from Transformers）は、2018年にGoogleが発表しました。「Bidirectional（双方向）」という名前が示すように、ある単語の意味を理解するために、その単語の前後両方の文脈を同時に参照します。

- "river"という単語が近くにあれば → "bank"は「川岸」を意味するベクトルに
- "money"や"deposited"が近くにあれば → "bank"は「銀行」を意味するのベクトルに

BERTの登場によって、Googleでは自然文による検索の精度が大きく改善したとされています（2019年頃）。

### 13.3.8 単語から文章へ：Sentence-BERTの登場（2019年）

BERTは文脈を考慮した単語ベクトルを生成できますが、文章全体を1つのベクトルとして効率的に表現することには向いていませんでした。2つの文章の類似度を計算するには、両方の文章をBERTに同時に入力する必要があり、大量の文章を比較するには計算コストが膨大になります。

この問題を解決したのがSentence-BERT（S-BERT）です。2019年に発表されたこの手法は、BERTを改良して文章全体を1つのベクトルとして効率的に表現できるようにしました。これにより、事前に各文章のベクトルを計算しておけば、類似度の計算はベクトル同士の比較だけで済むようになりました。

以下の9つの文章をベクトル化して、コサイン類似度を計算してみましょう。

| カテゴリ | 文章 |
|---------|------|
| 料理 | トマトソースのパスタを作るのが好きです |
| 料理 | 私はイタリアンの料理が得意です |
| 料理 | スパゲッティカルボナーラは簡単においしく作れます |
| 天気 | 今日は晴れて気持ちがいい天気です |
| 天気 | 明日の天気予報では雨が降るようです |
| 天気 | 週末は天気が良くなりそうで外出するのに最適です |
| 技術 | 新しいスマートフォンは処理速度が速くなりました |
| 技術 | 最新のノートパソコンはバッテリー持ちが良いです |
| 技術 | ワイヤレスイヤホンの音質が向上しています |

これらをベクトル化してコサイン類似度を算出すると、同一カテゴリ内では類似度が高く、異なるカテゴリ間では類似度が低くなります。

この「文脈ベクトル」と「コサイン類似度」の組み合わせが、ベクトル検索のコアとなる技術です。従来のキーワード検索では見つからなかった「意味的に近い文書」を発見できるようになりました。

### 13.3.9 エンベディングAPIの実践

エンベディング（embedding）とは、言葉や文章をベクトルの世界に変換することです。現在は様々なAPIやライブラリとして簡単に利用できます。

OpenAI Embeddings APIを使った例を見てみましょう。

```python
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
    input="Your text string goes here",
    model="text-embedding-3-small"
)

print(response.data[0].embedding)
# => 1536次元のベクトルが返ってくる
```

オープンソースのSentenceTransformersライブラリでも同様のことができます。

```python
from sentence_transformers import SentenceTransformer
model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-mpnet")
emb = model.encode(["hello world"])
# => 768次元のベクトルが返ってくる
```

広聴AIでは、OpenAIのEmbeddings APIと、SentenceTransformersを切り替えて使えるようにしています。

---

## 13.4 テキストを理解・生成する：大規模言語モデル（LLM）

### 13.4.1 BERTからGPTへ

BERTは文章を「理解する」ことに優れていますが、文章を「生成する」ことは苦手でした。一方、GPT（Generative Pre-trained Transformer）は「生成」を目的として設計されています。OpenAIが開発し、名前に「Generative（生成的）」とあるように、文章を生成することができます。

GPTの処理は、驚くほどシンプルな原理に基づいています。

> **入力されたテキストに基づいて、「次に来る単語」の確率を計算する**

例えば、"He sat by the bank of the ???"という文章に対して：

| 単語 | 確率 |
|------|------|
| river | 35% |
| stream | 25% |
| creek | 15% |
| pond | 6% |
| lake | 6% |
| ... | ... |

この確率に基づいて次の単語を選択し、選択した単語を追加して、また次の単語を予測する。これを繰り返すことで文章が生成されます。

「次の単語を予測する」だけで、なぜ「知性」と呼べるような能力が生まれるのでしょうか。それは、正しく次の単語を予測するためには、文章の内容を「理解」している必要があるからです。「日本の首都は」→「東京」を予測するには地理の知識が、「ピタゴラスの定理によれば」→ 数学の知識が必要です。大量のテキストで「次の単語の予測」を学習する過程で、LLMは言語のパターンだけでなく、世界についての膨大な知識を獲得しました。

「次の単語を予測する」という能力が極まった結果、知性と呼んでも差し支えがないような振る舞いが現れたのです。現在のLLMの知性は、このシンプルな仕組みによって支えられています。

### 13.4.2 Few-shot学習：穴埋め問題で様々なタスクを解く

GPTは「次の単語を予測する」というシンプルな仕組みで動いていますが、この仕組みを使って翻訳や分類などの様々なタスクを解くことができます。2020年に発表されたGPT-3の論文では、Few-shot学習という手法が紹介されました。

Few-shotの「Few」は「少数の」という意味です。具体例を数個見せるだけで、LLMは新しいタスクを学習できます。以下はGPT-3の論文に掲載された英語からフランス語への翻訳の例です。

```
Translate English to French:

sea otter => loutre de mer
peppermint => menthe poivrée
plush giraffe => girafe peluche
cheese =>
```

GPTにとって、これは翻訳問題ではありません。「cheese =>」の次に来る単語を予測する穴埋め問題です。3つの例から「英語 => フランス語」というパターンを学習し、「cheese」の次には「fromage」が来ると予測します。

この発見は画期的でした。専用の翻訳モデルを作らなくても、例を見せるだけで翻訳ができる。分類問題も、要約も、質問応答も、すべて「適切な例を見せて穴埋め問題に変換する」ことで解けるのです。

言語モデルはインターネット上のありとあらゆる文章から学習しているため、幅広い知識を活用でき、ある種の「常識を持っている」といっても過言ではありません。そのため、「常識」で解ける範囲の課題であれば、プロンプトエンジニアリングによって解決できる可能性があります。今回のチーズの翻訳もその一例です。英語とフランス語の文献はインターネット上に大量に存在するため、それらを学習したGPT-3にとっては「常識」の範囲内です。

ただし、ここで言う「常識」とは、インターネット上に大量に存在する情報から学習されたパターンのことです。一方、学習データに含まれていない情報、たとえば「○○社の今期売上」「△△市の条例詳細」「昨日のニュース」などは「知識」と呼ぶべきもので、LLMはこれらを持っていません。LLMは「常識」はあるが「知識」が足りないのです。この区別は、LLMを活用する上で非常に重要になります（詳しくは13.4.5節で解説します）。

例を1つだけ示す場合はOne-shot、例を示さずに指示だけで依頼する場合はZero-shotと呼びます。GPT-3論文のタイトルは「Language Models are Few-Shot Learners（言語モデルはFew-Shot学習者である）」でした。

他の例も見てみましょう。

```
以下の意見を分類してください:

「公園を増やしてほしい」=> 環境
「保育園の待機児童を減らして」=> 子育て
「道路の渋滞がひどい」=> 交通
「高齢者向けの施設を充実させて」=>
```

例として示したカテゴリが「環境」「子育て」「交通」のようにシンプルな一言なので、GPTもそれに従って「福祉」のような一言で出力します。もし例のカテゴリを「環境・緑化政策」「子育て支援制度」のように詳しく書けば、出力も同様に詳しくなります。Few-shot学習では、例の形式が出力の形式を決めるのです。

### 13.4.3 プロンプトエンジニアリングの誕生

Few-shot学習の発見は、ソフトウェア開発のあり方を根本から変えました。従来、新しい課題を解くには専用のプログラムを開発する必要がありました。翻訳には翻訳エンジン、感情分析には感情分析モデル、分類には分類器と、それぞれ専門のエンジニアが何ヶ月もかけて開発していたのです。

しかしFew-shot学習により、プロンプト（指示文）を書くだけで課題が解けるようになりました。翻訳も、感情分析も、分類も、適切な例を数個見せるだけで実現できます。専用のプログラムを開発する必要がなくなったのです。

この「プロンプトを工夫することで課題を解く」という新しいアプローチは、プロンプトエンジニアリングと呼ばれるようになりました。どのような例を見せるか、どのような順序で並べるか、どのような言葉で指示を書くか。これらの工夫によって、同じモデルでも出力の品質が劇的に変わります。

広聴AIでも、クラスタのラベリングにおいてFew-shot学習を活用しています。「良いラベルの例」を示すことで、抽象的すぎず具体的すぎない、適切な粒度のラベルを生成させています。

### 13.4.4 GPTからChatGPTへ：対話型AIの誕生

GPT-3のAPIは2020年から提供されていましたが、使いこなすにはプログラミングの知識とプロンプトエンジニアリングの技術が必要でした。Few-shot学習で適切な例を設計し、APIを呼び出すコードを書く必要があったのです。

先のフランス語の翻訳の例を思い返してみてください。こんなヘンテコな指示文を書ける人は、この手のパズルが大好きな技術者だけだったのです。そのため、GPT-3は一部の技術者の間で話題になりましたが、一般にはあまり知られていませんでした。

そして、2022年11月、OpenAIはChatGPTを公開しました。普通の言葉で質問すれば、普通の言葉で回答が返ってくる。この違いは決定的でした。

ChatGPTを実現した技術的なブレイクスルーは、Instruction Tuning（対話チューニング）です。GPT-3は「次の単語を予測する」ように訓練されていましたが、ChatGPTは「人間との対話応答を生成する」ように追加で訓練されています。

これにより、文脈を踏まえた自然な会話が可能になりました。Few-shotで例を見せなくても、「〇〇してください」と言うだけでタスクを実行できるようになったのです。

2023年3月にはGPT-4が登場し、司法試験に合格できるレベルのスコアを記録しました。日本の医師国家試験も突破し、最近ではo3というモデルで東大理系入試で理Ⅲ合格ラインも突破しています。その後も、GPT-5が登場する等、LLMの進化は止まらず、2026年初頭の現在も苛烈な競争が続いています。

### 13.4.5 LLMの得意なことと限界

LLMの能力は万能ではありません。得意なことと苦手なことが明確に分かれています。

得意なのは「常識」を使うタスクです。文章の要約・言い換え、翻訳、分類、論理的な推論、コード生成などがこれに当たります。一方、苦手なのは「知識」が必要なタスクです。最新のニュース、特定企業の内部情報、専門的なドメイン知識など、学習データに含まれていない情報は扱えません。

| 常識 | 知識 |
|------|------|
| 学習データに十分含まれている情報 | 学習データに含まれていない情報 |
| 「太陽は東から昇る」「水は100度で沸騰する」 | 「○○社の今期売上」「△△市の条例詳細」 |

この「常識」と「知識」の区別は、LLMと検索エンジンの違いを理解する上でも役立ちます。両者は表裏の関係にあり、得意・不得意が正反対なのです。

|  | LLM | 検索エンジン |
|------|------|------|
| **得意** | 「常識」を使うタスク（要約、翻訳、分類、推論） | 「知識」を見つけるタスク（最新ニュース、専門情報、固有名詞） |
| **苦手** | 「知識」が必要なタスク（最新情報、ローカル情報、専門知識） | 「常識」が必要なタスク（文脈理解、要約、意図の解釈） |
| **更新頻度** | 数ヶ月に一度（モデルの再学習が必要） | 数時間〜数日（クローラーが自動で巡回） |

検索エンジンの時代、私たちは「誰かがウェブに公開した情報」にたどり着くことができました。検索エンジン自体は「知識」を持っていませんが、知識が書かれた場所への道案内をしてくれます。そのため、あたかも検索エンジンが何でも知っているかのように感じられました。

しかしLLMは道案内ではなく、自ら回答を生成します。問題は、「知識」を持っていないにもかかわらず、質問されれば何かしらの回答を返してしまうことです。検索エンジンと同じ感覚で「○○について教えて」と質問すると、もっともらしいが事実ではない回答が返ってくることがあります。これがハルシネーション（幻覚）と呼ばれる現象です。

LLMは「わからない」と答える代わりに、それらしい回答を生成してしまう。この特性を理解せずにLLMを使うと、誤った情報を信じてしまう危険があります。

余談ですが、最新のモデルではハルシネーションの問題が改善されつつあります。GPT-5系のモデルでは、ハルシネーションを減らすためのアライメント（調整）が行われました。技術的には、従来の「正解」「不正解」の2択で学習していた仕組みに「棄権（回答しない）」という選択肢を加え、確信が持てない質問に対しては回答を控えるよう訓練されています。

その結果、「わからない」「この情報だけでは回答できない」と答える傾向が強くなりました。以前のモデルでは「とにかく何か答える」という振る舞いが目立ちましたが、推論能力を強化したモデルほど、自分の限界を認識して不確実性を表明するようになってきました。

https://openai.com/ja-JP/index/why-language-models-hallucinate/

とはいえ、ハルシネーションが完全になくなったわけではありません。LLMを使う際には、事実確認が必要な情報については一次情報源を参照する習慣が引き続き重要です。

### 13.4.6 RAG：外部知識でLLMを補強する

ハルシネーション問題への対策として、RAG（Retrieval-Augmented Generation、検索拡張生成）があります。

RAGの仕組みはシンプルです。ユーザーの質問に関連する情報をデータベースやウェブから検索し、その情報をLLMに渡して回答を生成させます。LLM自身が知らない情報でも、検索で取得した情報をもとに回答できるようになります。

現在の主要なLLMサービス（ChatGPT、Claude、Geminiなど）はすべて検索エンジンと連携しており、最新情報を含む回答を生成できます。Microsoft CopilotがビジネスユーザーのLLM市場で大きなシェアを獲得しているのは、SharePointに蓄積された社内文書を検索できるからです。「自社の過去の提案書を参照して回答する」「社内規定を踏まえてアドバイスする」といった、企業固有の知識を活用した回答が可能になります。

ただし、検索で取得した情報が誤っていれば、LLMもその誤りを反映した回答を生成します。LLMの回答を鵜呑みにせず、情報源を確認する姿勢が重要です。

### 13.4.7 Structured Output：LLMをシステムに組み込む

2024年8月、OpenAIはAPIにStructured Output機能を搭載しました。ユーザが指定したJSONフォーマットで出力させることが可能になったのです。

従来は、LLMの出力は自然言語（文字列）であり、JSONで出力させても90%程度しかフォーマットに準拠しませんでした。10%の確率でエラーが起きるモジュールはシステムに組み込めません。

Structured Outputはこの問題を解決しました。以下のようなコードで、確実に構造化データを取得できます。

```python
from pydantic import BaseModel, Field
import openai

class Character(BaseModel):
    name: str = Field(description="キャラクターの名前")
    age: int = Field(description="キャラクターの年齢")
    occupation: str = Field(description="キャラクターの職業")

client = openai.Client()
result = client.beta.chat.completions.parse(
    model="gpt-4o",
    messages=[{"role": "user", "content": "架空のキャラクターを作成して"}],
    response_format=Character
).choices[0].message.parsed

print(result.model_dump_json(indent=2))
# => {
#   "name": "Haru Yamada",
#   "age": 28,
#   "occupation": "Urban Explorer"
# }
```

これにより、LLMは「自然文を入力すると、決められたフォーマットの構造化データ（JSON）を返す関数」として扱えるようになりました。

この変化をプログラマの視点で考えてみましょう。従来、自然言語から構造化データを取り出すには、正規表現、形態素解析、構文解析、ルールベースの抽出器など、複雑な処理を組み合わせる必要がありました。それでも「公園を増やしてほしい」と「もっと公園がほしいです」が同じ意味だと認識させるのは困難でした。

Structured Outputは、この問題を根本から解決します。「プロンプトと型を定義するだけ」で、自然言語から構造化データを取り出せるようになったのです。

```python
class Opinion(BaseModel):
    category: str = Field(description="意見のカテゴリ（環境/福祉/経済/教育/その他）")
    sentiment: str = Field(description="賛成/反対/中立")
    summary: str = Field(description="意見の要約（20文字以内）")

# これだけで、自然言語の意見を構造化できる
def parse_opinion(text: str) -> Opinion:
    return client.beta.chat.completions.parse(
        model="gpt-4o",
        messages=[{
            "role": "user",
            "content": f"以下の市民意見を分析してください：\n\n{text}"
        }],
        response_format=Opinion
    ).choices[0].message.parsed
```

プログラミングの歴史において、これは大きな転換点です。従来は「データ構造を定義し、それを埋めるロジックを書く」必要がありました。Structured Outputでは「プロンプトとデータ構造を定義するだけ」で、LLMが自動的にデータを埋めてくれます。処理ロジックを書く必要がなくなったのです。

2025年現在、Structured OutputはOpenAI、Anthropic（Claude）、Google（Gemini）など主要なLLMプロバイダーがすべてサポートしています。

広聴AIでも、意見の抽出やラベリングの際にStructured Outputを活用しています。数千件の自由記述を、カテゴリ・感情・要約といった構造化データに変換する処理は、Structured Outputなしには実現できませんでした。

---

## 13.5 データを整理・可視化する：クラスタリングと次元圧縮

### 13.5.1 クラスタリングとは

クラスタリングとは、似たものを自動でグループ分けする技術です。まず、シンプルな例から見てみましょう。

図13-1は、200個の点を平面上にプロットしたものです。

![データの分布](images/13_clustering_two_gaussians.png)
*図13-1：2つの集団に分かれたデータ*

この図を見て、あなたはおそらく「左上のかたまり」と「右下のかたまり」という2つのグループを認識したのではないでしょうか。人間の目には、データが自然と2つに分かれているように見えます。

では、どこで分けるのが「自然」でしょうか？ 図13-2のように、それぞれのかたまりを円で囲むのが妥当に思えます。

![自然なグループ分け](images/13_clustering_two_gaussians_clustered.png)
*図13-2：自然なグループ分け*

クラスタリングとは、この「人間が直感的に認識するグループ分け」をコンピュータに自動で行わせる技術です。「クラスタ（cluster）」は英語で「房」や「集団」を意味し、ブドウの房のように似たものが集まっている状態をイメージするとわかりやすいでしょう。

### 13.5.2 データの形状とアルゴリズムの選択

先ほどの例は、2つの丸い塊が離れている単純なケースでした。しかし、現実のデータはもっと複雑な形をしていることがあります。

図13-3は、4種類のデータ構造に対して、3種類のクラスタリングアルゴリズム（K-means、Ward法、DBSCAN）を適用した結果です。

![アルゴリズム比較](images/13_clustering_algorithms_comparison.png)
*図13-3：データ構造とアルゴリズムによるクラスタリング結果の違い（scikit-learn公式ドキュメントを参考に作成）*

この図から、いくつかの重要な点がわかります。

| データ構造 | K-means | Ward法 | DBSCAN |
|-----------|---------|--------|--------|
| 1つの塊 | 無理やり2つに分割 | 無理やり2つに分割 | ほぼ1つのクラスタとして認識（周縁部に外れ値が出る場合あり） |
| 円形 | うまく分けられない | うまく分けられない | きれいに分けられる |
| 三日月形 | うまく分けられない | うまく分けられない | きれいに分けられる |
| 3つの塊 | 2つが1つにまとめられる | 2つが1つにまとめられる（きれいに3つに分けることもできる） | きれいに3つに分けられる |

注目すべきは「1つの塊」と「3つの塊」の行です。K-meansとWard法は「2つに分けろ」と指示されれば、たとえ1つの塊であっても無理やり分割し、3つの塊があっても2つにまとめてしまいます。一方、DBSCANは「密度の高い領域をクラスタとする」ため、データの自然な構造を認識できます。ただし、DBSCANはパラメータ（密度の閾値）の設定次第で、周縁部の点が外れ値として分類されたり、飛び地的な小さなクラスタが生まれたりすることがあります。

つまり、**万能なアルゴリズムは存在しません**。データの形状に合ったアルゴリズムを選ぶ必要があります。広聴AIで扱う意見データは、ベクトル空間上で「塊状」に分布する傾向があるため、K-meansやWard法が適しています。

### 13.5.3 K-meansのアルゴリズム

K-meansは、最も広く使われているクラスタリング手法です。「K」はグループの数を表し、事前に「何グループに分けたいか」を指定します。

アルゴリズムの動作を図13-4で見てみましょう。

![K-meansのステップ](images/13_kmeans_steps.png)
*図13-4：K-meansアルゴリズムの動作*

1. **Step 0**: 初期データがあります
2. **Step 1**: 2つの「中心点」（★）をランダムに配置します
3. **Step 2**: 各点を、最も近い中心点のグループに割り当てます
4. **Step 3**: 各グループの重心に中心点を移動します
5. **Step 4-7**: 「割り当て→中心移動」を繰り返します。中心点が動かなくなったら終了です

K-meansの特徴は、シンプルで高速なことです。ただし、最初に「何グループに分けるか」を決める必要があります。もう1つの重要な特徴は、**1つの塊を無理やり複数に分割できる**ことです。図13-3の「1つの塊」データをK-meansで「K=2」に分けた場合、塊が強制的に2つに切断されていました。これは一見デメリットに見えますが、後述する広聴AIのアルゴリズムでは、この特徴を活用してデータを細かく分割してから階層的に統合する手法をとっています。

### 13.5.4 階層的クラスタリング（Ward法）

Ward法は、階層的クラスタリングの一種です。K-meansとは異なり、グループ数を事前に決める必要がありません。

アルゴリズムの動作を図13-5で見てみましょう。

![Ward法のステップ](images/13_ward_steps.png)
*図13-5：Ward法（階層的クラスタリング）の動作*

1. **Step 0**: 最初は全員がバラバラです
2. **Step 1**: 最も近い2点（AとB）を結合します
3. **Step 2-3**: 次に近いペアを順次結合していきます
4. **Step 4-5**: グループ同士も結合していき、最終的に全員が1つになります

この過程を「デンドログラム（樹形図）」として表現できます（図13-6）。

![デンドログラム](images/13_hierarchical_dendrogram.png)
*図13-6：階層的クラスタリングとデンドログラム*

デンドログラムの重要な特徴は、**どこで切るかによってグループ数が変わる**ことです。

- 赤い線（距離=3）で切ると → 2グループ
- オレンジの線（距離=1.5）で切ると → 4グループ

つまり、一度クラスタリングを実行すれば、後から「もっと細かく分けたい」「もっと大きくまとめたい」という調整ができます。これが階層的クラスタリングの大きな利点です。

### 13.5.5 次元圧縮とは

次元圧縮とは、データの本質だけを残して「軸」を減らす技術です。広聴AIでは、1536次元のベクトルを2次元に圧縮しています。

なぜ次元圧縮が必要なのでしょうか。最大の理由は、私たち人間が3次元までしか直感的に理解できないことです。1536次元のデータをそのまま見ることは不可能です。次元を2次元まで減らすことで、初めて散布図として可視化できるようになります。

次元圧縮の基本的な考え方を、シンプルな例で見てみましょう。図13-7は、2次元のデータを1次元に圧縮する過程を示しています。

![PCAによる次元削減](images/13_pca_2d_to_1d.png)
*図13-7：PCAによる次元削減（2次元→1次元）*

左の図は、斜め方向に伸びた2次元のデータです。このデータには「斜め方向の広がり」と「それに直交する方向の広がり」がありますが、斜め方向の広がりの方がはるかに大きいことがわかります。

中央の図では、データが最も広がっている方向（赤い線＝第1主成分軸）を見つけ、すべての点をこの軸に投影しています。灰色の線は、各点から軸への投影を示しています。

右の図は、投影後の1次元データです。元々2次元だったデータが、1本の数直線上の点になりました。

この手法はPCA（主成分分析、Principal Component Analysis）と呼ばれます。「データが最も広がっている方向」を見つけて、その方向に投影することで次元を減らします。2次元を1次元に減らすと、直交方向の情報は失われますが、主要な構造は保たれています。

PCAは1901年に統計学者のカール・ピアソンによって考案された古典的な手法で、100年以上の歴史があります。統計学、心理学、経済学、生物学、画像処理など、非常に幅広い分野で使われてきました。たとえば、心理学では多数の質問項目から「外向性」「神経症傾向」といった性格因子を抽出するのに使われますし、経済学では複数の経済指標から「景気動向指数」のような総合指標を作成するのに活用されています。

PCAがこれほど広く使われている理由は、現実世界のデータでは異なる変数間に強い相関があることが多いからです。たとえば、身長と体重は強く相関しています。身長が高い人は体重も重い傾向があり、その逆もまた然りです。同様に、学校の成績でも算数が得意な生徒は国語も得意であることが多く、両者には正の相関があります。このような相関関係があるデータでは、見かけ上は多くの変数があっても、実質的な情報の「広がり」は限られた方向に集中しています。PCAはこの性質を利用して、少数の主成分で元のデータの大部分の情報を表現できるのです。

情報量を減らすことで、意思決定も容易になります。100の変数を見て判断するより、2〜3の主成分で傾向を掴む方が、人間にとってはるかに理解しやすくなります。

ただし、次元圧縮には情報損失が伴います。「大部分の情報」を保存するということは、「一部の情報」は失われるということです。全体の傾向から外れた例外的なケースは、次元圧縮によって見えにくくなります。たとえば、身長は低いが体重が重い人（力士など）は、身長と体重の相関という「主成分」では捉えきれません。このような例外を切り落としてしまう点は、次元圧縮の限界として意識しておく必要があります。

### 13.5.6 UMAPによる次元圧縮

PCAは直線的な相関関係を捉えるのに適していますが、複雑な構造を持つデータには限界があります。これを確認するため、MNISTという手書き数字のデータセットを使ってみましょう。

MNISTは、0から9までの手書き数字を集めたデータセットで、機械学習の研究で広く使われています。図13-8にサンプルを示します。

![MNISTサンプル](images/13_mnist_samples.png)
*図13-8：MNIST手書き数字データセット*

各画像は28×28ピクセルのグレースケール画像です。つまり、1枚の画像は784個の数値（各ピクセルの明るさ）で表現されます。これが「784次元のデータ」という意味です。同じ数字でも、書く人によって形が異なることがわかります。

図13-9は、この784次元のデータを2次元に圧縮した結果を、PCAとUMAPで比較したものです。

![PCA vs UMAP](images/13_pca_vs_umap.png)
*図13-9：PCAとUMAPの比較（MNIST手書き数字データ）*

左のPCAによる圧縮では、0から9までの数字が混ざり合っており、どの数字がどこにあるのか判別が困難です。PCAは「最も分散が大きい方向」に投影するだけなので、「同じ数字は近くに配置する」という意味的な構造を捉えることができません。

右のUMAPによる圧縮では、同じ数字同士がまとまって配置され、異なる数字は離れた位置に配置されています。たとえば「1」と「0」は明確に分離されています。

さらに興味深いのは、**人間が手書きで間違えやすい数字同士が隣接している**点です。

- **0と6**: ともに丸い形状を持ち、書き方によっては似てしまう
- **2、3、5、8**: 曲線を主体とした数字で、崩れると互いに似る
- **4、9、7**: 縦線と斜め線で構成され、手書きでは混同しやすい

これは偶然ではありません。UMAPは784次元の画像データ（28×28ピクセル）の類似性を捉えて配置しているため、「画像として似ている数字」が自然と近くに配置されるのです。人間の直感的な類似性判断と、ピクセルデータに基づく数学的な類似性には、一定の対応関係があることをこの可視化は示唆しています。

図13-10は、4と9が混在する境界付近を拡大したものです。各点に実際の手書き画像を表示しています。

![境界付近の拡大](images/13_mnist_boundary_zoom.png)
*図13-10：4と9が混在するUMAP境界領域の拡大*

下部には4（赤枠）が多く、上部には9（青枠）が多いことがわかります。その中間領域では、両者が混在しています。画像を見ると、上部が閉じた4は9に似ており、下部が開いた9は4に似ていることがわかります。UMAPは、このような「どちらにも見える」サンプルを、適切に境界付近に配置しているのです。

重要なのは、**このような曖昧なデータに対しては、そもそも正解が曖昧である**という点です。広聴AIでも同様に、複数の話題にまたがる意見や、解釈が分かれる意見は、クラスタの境界付近に配置されます。

UMAP（Uniform Manifold Approximation and Projection）は2018年に発表された手法で、「近くにあるもの同士の関係」を優先して保存します。高次元空間で近かった点は、2次元に圧縮しても近くに配置されます。これにより、「似ている意見は近くに配置される」という直感的な可視化が可能になります。広聴AIの散布図で、同じ話題の意見が固まって表示されるのは、UMAPのおかげです。

ただし、UMAPの結果を解釈する際には注意が必要です。UMAPは「近いもの同士を近くに配置する」ことを優先するため、**近くにある点同士は「似ている」と解釈できますが、離れた点同士の距離には意味がありません**。たとえば、図13-9で「1」のクラスタと「0」のクラスタが離れていますが、その距離が「1と0は4と9より違う」ことを意味するわけではありません。また、PCAの軸には「第1主成分＝データの最大分散方向」という解釈がありましたが、**UMAPの軸1・軸2には特定の意味がありません**。軸の向きや回転は任意であり、「右にあるほど○○」といった解釈はできません。UMAPが提供するのは、あくまで「似ているものが近くに集まる」という相対的な配置情報です。

#### UMAPの具体例：都道府県データ

UMAPの動作をより具体的に理解するため、47都道府県の統計データを使った例を見てみましょう。

図13-11は、政府統計（e-Stat）の都道府県別データを参考に作成したサンプルデータを使って、47都道府県をUMAPで2次元に配置した結果です。人口密度、産業構造、高齢化率など約50の統計指標を入力しています。

![都道府県のUMAP](images/13_prefecture_umap.png)
*図13-11：統計指標に基づく都道府県の類似性*

ここで重要なのは、**本来比較できないはずのデータを比較可能にしている**点です。人口密度（人/km²）、高齢化率（%）、製造品出荷額（億円）といった指標は、単位も桁数もバラバラです。しかも50もの指標を同時に見て「どの県とどの県が似ているか」を人間が判断することは困難です。UMAPは、これらの異なる指標を総合的に考慮して、**地理的に離れていても、統計的に似ている都道府県を近くに配置**します。

- **右上のエリア（大都市圏）**: 東京・大阪・神奈川などの大都市圏と、沖縄が近くに配置されています。第三次産業比率が高い（80%以上）、人口密度が高い、自動車保有率が低い、大卒比率が高いといった特徴を共有しています
- **中央のエリア（ベッドタウン）**: 埼玉・千葉・兵庫・奈良などが集まっています。人口密度は大都市より低いが地方より高い、第三次産業比率が高い（77%程度）、1人当たり所得は大都市より低い（300万円程度）といった、大都市と地方の中間的な特徴を持ちます
- **左上のエリア（工業県）**: 静岡・群馬・栃木・愛知など、製造業が盛んな県が集まっています。第二次産業比率が高い（35〜40%）、製造品出荷額が高い、有効求人倍率が高いといった特徴があります
- **左下のエリア（農業県・高齢化県）**: 北海道・鹿児島などの農業県と、秋田・高知・島根などの高齢化が進んだ県が近くに配置されています。第一次産業比率が高い、人口密度が低い（150人/km²以下）、高齢化率が高い（32〜36%）、農業産出額が高いといった特徴を共有しています

このように、UMAPは「多くの指標で似ている」ものを近くに配置します。意見データでも同様に、「多くの観点で似ている意見」が近くに集まることで、直感的な可視化が実現されています。

### 13.5.7 広聴AIでの組み合わせ

広聴AIでは、これらの技術を以下のように組み合わせています。

```
エンベディングベクトル（1536次元）
    ↓
UMAP（2次元に圧縮）
    ↓
K-means（細かくクラスタリング）
    ↓
Ward法（階層的に統合）
    ↓
LLM（クラスタに名前付け）
```

なぜK-meansで細かく分割してからWard法で統合するのでしょうか。これは計算量の問題です。Ward法は全ての点の組み合わせを比較するため、データ数が増えると計算量が急激に増大します。1万件の意見に直接Ward法を適用すると、膨大な計算時間がかかってしまいます。

そこで広聴AIでは、まずK-meansで意見を数百個の細かいクラスタに分割します。13.5.3節で説明したように、K-meansは「1つの塊を無理やり分割できる」特徴があるため、データを均等に細かく切り分けることができます。次に、各クラスタの中心点（セントロイド）をWard法で階層的に統合します。こうすることで、Ward法が処理するデータ数は「意見の数」ではなく「K-meansのクラスタ数」になり、計算が現実的な時間で完了します。

この組み合わせにより、大量の意見を「似ているものを近くに配置」しながら「階層的にグループ分け」することが可能になります。

---

## 13.6 技術の統合：広聴AIのパイプライン

ここまで解説してきた技術が、広聴AIでどのように組み合わされているかを整理しましょう。

### 13.6.1 処理の全体像

```
┌─────────────────────────────────────────────────────────────┐
│  ① 意見の収集                                                │
│     パブリックコメント、アンケートなど                          │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  ② 意見の分割・整形（LLM）                                    │
│     一つの投稿に複数の意見 → 個別の意見に分割                    │
│     感情的な表現を除去、建設的な意見のみを抽出                   │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  ③ ベクトル化（エンベディング）                                │
│     各意見を1536次元のベクトルに変換                            │
│     似た意見は近いベクトルになる                                │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  ④ 次元圧縮（UMAP）                                          │
│     1536次元 → 2次元に圧縮                                    │
│     散布図として可視化可能に                                   │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  ⑤ クラスタリング（k-means + Ward法）                         │
│     似た意見を自動でグループ分け                                │
│     階層構造を持たせる                                        │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  ⑥ ラベリング・要約（LLM）                                    │
│     各クラスタにタイトルと説明文を付与                          │
│     全体の要約も生成                                          │
└─────────────────────────────────────────────────────────────┘
                              ↓
┌─────────────────────────────────────────────────────────────┐
│  ⑦ 可視化                                                   │
│     2次元座標上に意見を点として配置                             │
│     クラスタごとに色分けして表示                                │
└─────────────────────────────────────────────────────────────┘
```

### 13.6.2 各技術の役割まとめ

| ステップ | 使用技術 | 入力 | 出力 |
|---------|---------|------|------|
| 分割・整形 | LLM | 生の意見テキスト | 整形された個別意見 |
| ベクトル化 | エンベディング | 意見テキスト | 1536次元ベクトル |
| 次元圧縮 | UMAP | 1536次元ベクトル | 2次元座標 |
| グループ化 | k-means + Ward法 | 2次元座標 | クラスタ割り当て |
| ラベリング | LLM | クラスタ内の意見群 | タイトル・説明文 |

これらの技術が一つのパイプラインとして統合されることで、数千・数万件の意見を短時間で分析・可視化することが可能になります。

次章では、このパイプラインの実装について、コードレベルで詳しく見ていきます。
