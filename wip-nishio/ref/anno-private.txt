広聴AI開発internal / 旧TTTCプロジェクト
https://docs.google.com/document/d/1LmhigvWEGpKiZFSSub8xgmvITGqvYERiZ9Epc393S6o/edit?tab=t.0


2025年5月27日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  

共有事項
角野
やっていたこと
広聴AIの機能開発（意見グループの文言編集機能等）
広聴AIのPRレビュー
やろうと思っていること
7月PJでも必要になりそうな機能の実装（レポートコピー等）とレビューは継続して実施する想定。その他、以下をやろうと思っている。
広聴AIの活用推進
一旦は角野が手を動かしてレポートを作成しようと思っているが、レポート作成が型化できてきた段階で徐々に他チームに移譲していこうと考えている
データ取得周りがボトルネックになりそう（エンジニアが必要になりそう）なので、データ取得（+広聴AIに入れる前の事前解析）ツールを作成した。ノンエンジニアでもなるべくデータ -> 分析を回していけるようにしたい。
https://github.com/nasuka/flexible-text-analyzer
youtube動画のURLを入れるとコメントが取得できる
その他、sensemaking的なタグ付け等（e.g. チームみらいに対する懸念・期待等をLLMで付与する）ができる


X APIについては未実装
youtubeと異なり、取得枠を超えたときの追加コストが高すぎるので、自由にデータ取得できるようにしたときのリスクが大きい
現在はローカル環境でしか動作しないが、GCP or Azureあたりで環境を立ててチームみらい内部でいじれるようにしたい
コストがかかるが問題ないか？
ベーシック認証はかける想定
問いを立てる前提の示唆分析
広聴AIは問いがない状態で意見を分析している
-> 明確に問いがある状態での分析には適していない
e.g. 「チームみらいに対してどのような批判が寄せられているのか」といった問いがあっても、これに対応するアウトプットが得られるとは限らない
アウトプットを見ていて、面白い示唆が得られるケースもあればそうでないケースもある
分析者が面白い示唆を得られる打率を高くするために、問いがある前提でデータを要約・分析するようなツール（or 機能）があっても良いのでは、と思っている
例
https://chatgpt.com/share/68342a77-ad4c-800f-b4ce-c1cd09fff368
明確な問いに対応した要約を生成・原文を引用する形式の要約

にしお
パブコメ分析、記事をリリースして一旦done
大量PRを捌くことの支援に軸足を移して活動中
Devinで「プロトタイプを作って『こんな感じでいい？』と聞くとこ」までは爆速
安定運用に入ろうとした時のトラブルは全然解決できてないので後で僕が見る(high priority)
“ラベルのついていないPRにどのラベルをつけるのがおすすめか理由付きで教えてくれるAI”(nishio)
https://annyotakagmailcom.slack.com/archives/C08LTV14C2C/p1748011298006159
肌感良さそうなので上記トラブル解決したら対象拡大する予定です
“変更されたセクションごとにまとめる”(nishio)
https://github.com/team-mirai/policy-pr-data/tree/main/section_reports
一応作ってみました(5/24時点の全データのはず)
“セクション”だと荒すぎるかな... という気もしています、行単位がいい？
ボールがこぼれがちな「明確なプロジェクトの境界の中にないタスク」も拾いたい
Stripe APIからデータをとって個人情報を抜いてGithubに置く
https://annyotakagmailcom.slack.com/archives/C08LPF8TJ0N/p1747884982048849
https://github.com/team-mirai/get_stripe_info
AIあんのからFAISS部分だけ取り出してポランティアがライトな実験をできるようにしよう
https://annyotakagmailcom.slack.com/archives/C08LPF8TJ0N/p1748015316403519
ボランティアSlackでもDevinを使えるようにしよう
https://annyotakagmailcom.slack.com/archives/C08LPF8TJ0N/p1748215273492079?thread_ts=1748154397.479179&cid=C08LPF8TJ0N
広聴AIは工夫なしのプロンプトで5/22の時点のPRデータでつくってみた(with nasuka-san)
政策チームには共有しているけど、多分こっちを見る余裕がないみたいで特に反響はない
「こういうことをやった」というのは事例として公開されるとバリューありそう
元データがopenなので改善の試行錯誤をうまいことボランティアに切り出せるといいなぁ
この場合の問題点は政策チームのニーズを汲み取るところのバスが細いこと(中にいる僕ですら細いんだから難しいね / 政策チームにボランティアと話してもらうのはキャパがないと思う / LINE OpenChat問題と同様)
ざっくり上から順の優先順位
安野
広聴AIは下記２点が課題っぽい気がする
１）いかにチームみらいの中の運用にのせていくか
２）それを有効な発信につなげてゆくか
有賀（すみません、今日欠席になります）
自治体トライアルは、岩手県が、早ければ今週中に事例発信されます
ほぼ宇多津町の資料の内容更新版
地元メディアにも載るかも（先方で打診中）


いどばたPRの分析レポート
https://client.salmonpebble-febdd0ee.japaneast.azurecontainerapps.io/policy-pr-0522-test/


メモ
とりあえず現状のレポートをver0.1として公開してよいのでは？
PRのリンクまで紐づけて、レポートで見れるようにしたい
extractionの部分で、PRの#を最後に切り出して付与する
追加スクリプトを実装すれば本体修正しなくても良いかも
チームみらいボランティアと各プロジェクトの詳細を知っている人間のミートアップを開く
安野・nishi/sumino/muraiで日程調整する
実務に寄せてタスクに着手していくと良さそう
githubのボランティア用orgを作る


アクション アイテム





2025年5月20日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  


共有事項
角野
チームみらいの広聴AI環境（外部共有用）を立てました
https://docs.google.com/document/d/1qaP8fBE_euaQ1gSCHdNfLxY2erVBaBosFJBUlQfxKWk/edit?tab=t.0
広聴AIは、機能としては最低限利用できる水準に到達している認識
本格的に運用する場合、以下の機能はあった方が良いので今週〜来週目処に実装する予定
クラスタタイトルの手動編集機能
レポート設定の複製・再利用機能
アクション アイテム





2025年5月13日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  

共有事項
角野
ウタコさんにメンテナになってもらいました
OpenRouter対応はPRまで出ていて現在レビュー中
有賀
渋谷区
分析済み。副区長以下は全体的に好評
6月にはプレスリリースだせそう（宇多津町と同じような感じ）
デジタル民主主義2030の名前は出せるが安野の名前は出せないライン
それでいい(anno)
岩手県
分析は提出済み
プレスリリース準備中（7月PJの関係で若干調整に時間かかっている）
舞鶴市
分析開始に向けて調整中
西尾
特になし。
Pluralityの議論を見てて用語集的なページがあった方がいいのかもな〜、Plurality用語集ってよりはデジタル民主主義用語集の方がいいかもな、非エンジニアに貢献機会を創出できるし、と思った(まだ思ってるだけ)
いま未踏社団側で議論したんだけど未踏社団の理事は降りることになりました、来週手続きをfinishする感じ(広聴AIの話ではないが)
TODO: FAQとして軸には意味がない話を書く
>embeddingをUMAPしてから軸を見出そうとするのがいけないので、まず適当に軸を作って「その軸でどれくらいか」の値をLLMで生成し、大きめの係数でembeddingのベクトルに足し算してやれば、その後のUMAPでもきっとその軸が再現すると思う(nishio)
https://dd2030.slack.com/archives/C08PX74S5T4/p1747028156031969
これは面白いものが出るかもしれないから、新しいアルゴリズムの実験は進めてもいいと思う(anno)



メモ
チームみらい広聴AIをホスティングする
https://client.greendune-ba8f55d5.japaneast.azurecontainerapps.io/
現状はデフォルトで公開する設定になっており、Azureでホスティングするには限定公開に対応する必要がある
https://github.com/digitaldemocracy2030/kouchou-ai/issues/365
Azure使う場合、ドメインもサブドメイン切ってAzure環境に割り当てる必要がある
ノンエンジニアの人が運用担当になって、適宜フィードバックもらえると開発にも役立ちそう
理想は理解した(nishio)
サイクルを爆速にしたい(anno)
unlisted
urlうちこめば見れるもの
認証が必要なもの
エネルギーパブコメ
やっぱり部分的みたい(nishio)
>残りは「来年の2月28日までに開示決定します」ときているのでかなり先になりそう...(nemoto) https://dd2030.slack.com/archives/C08PX74S5T4/p1747062256384269?thread_ts=1746765945.770059&cid=C08PX74S5T4
(nishio)分析レポート作成はチャレンジする予定
次は環境省が来る
150人のエンジニアボランティアをどう活用するか
エンジニアパブリックチャットが生まれる予定(パーミッションレス)
選対Slackと切り離す

アクション アイテム
限定公開機能を実装できそうか確認する（sumino）
今使っているAzure環境のクレカが問題ないか確認する
5~6月で、広聴AIで何を達成するのかを整理する
(nishio)エネルギーパブコメ分析レポート作成



2025年5月6日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  

共有事項
角野
GYP関連
全体的に良い感じだと思っています
tokorotenさんが機械学習関連の実装をガッツリ進めてくださっている。↓でnishio sanも記載されていますがメンテナを打診したいと思います。
デザインについては、ウタコさんが環境整備を進めてくださっている
他にも新たなコミッターの方も出てきている。既存のメンテナーはどちらかというとレビューに徹している状況。
優先タスク
参院選本格化にあたってOpenRouter対応は必須だと思っているがまだ対応できていない状況（PRまで出ていて実装が進行中のステータス）
西尾
本日電車移動と被るため耳だけ参加できるかも的な感じ
tokorotenをメンテナにしていいと思います
今までの慣習では安野さんから打診してるけど、出馬しますと言った後はダメな気がするから適当なタイミングで角野さんにバトンタッチするといいのかな
パブコメフィルタについて(これは公開でやったほうがいいかもだけど)
(1)ローカルモデルでembedして凝集クラスタリングで類似コメントの発見は効率化できる
しかし実データ(文化庁AIパブコメ)で試した結果、なんでこれをくっつけるのかな〜というものも出てくることがわかった、embeddingの限界だと思う
納得感のないまとめを出すより圧縮率が下がってでも納得感を高めたい
(2)そこで文字列としてのdiffをとってそれを使うことを試した
これは処理時間が増えすぎるのでNG
(3予定) (1)の仕組みで結合候補を見つけてから文字列としてのdiffが大きすぎるものはリジェクトするスタイルでいけると思う
文化庁AIパブコメで実験しながら10万件データ(良い名前を考えとこう)を待つ
E2Eテストはできてません
キャパ溢れで脳から追い出された
広聴AI側のコードをあまり見れていない
個人的今週の優先順位
5/11 Audrey Tang自社イベント > 5/10 Pluralityイベント登壇(Takaとblu3moも) > パブコメ > 広聴AI
パブコメと広聴AIはデッドラインのあるものではないから...
古川（別打ち合わせとバッティングのため不参加）
7月PJの方に時間をとられて広聴AI関係作業できていません。。
とりあえずパブコメ関係は10万件待ちの認識です
有賀
今週は特にアップデートありません。
岩手県：広報協議中
渋谷区：分析中
舞鶴市：手続き中



メモ



アクション アイテム
ところてんさん打診 nishio.hirokazu@gmail.com
OKもらいました。留保事項:
原状は広聴aiしか見てないので、メンテナの範囲はそこだけにしておいてください。
水曜20時は客先にいる可能性が高い




2025年4月29日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  

共有事項
角野
7月Pj用の広聴AI Azure環境を構築
アカウント情報などはこちら 7月_広聴AIアカウント情報
出馬検討に対するX上のリアクションを分析にかけた
レポートはこちら
データはこちら: データ
望ましいレポートを出力するためにはプロンプトを試行錯誤する必要があることを改めて実感した
GYP関連
デザイン経験者の方に明日の定例に参加していただける予定
アルゴリズム関連の議論は活発だがそれ以外は少し停滞気味
明日の定例で幾つかタスクをピックアップして、着手してくれる方を募集してみる？
テンションを上げる施策
使われた事例を広報する
ミートアップ
(僕がガッとアルゴリズムの話を書いたせいで参加しにくくなった感はあるかも。別チャンネルに切り分けたので今後本体の議論をしていくと復活しそう)(nishio)
西尾
パブコメ大量投稿問題の解決について
100万件でも解決できる目処がたった
1: 1万件サンプリングして凝集クラスタリングし、それを分析者がみて閾値を決定
2: ステップ(1)で作られたクラスタに対して残りのデータが入るかどうかチェック
ステップ(1)が10分前後で可能なことを確認した、もう少しいじればもう少し縮められる
ユースケースを考えた時に、閾値が事前に決めてあったとしても「AIがなんか適当な閾値でまとめたからどうなってるかわかりません」をユーザは許容できないので「みなくてよくする」のではなく「見るのが簡単にする」路線
コミュニティでは発想の幅を広げたり貢献の機会を生み出したりのためにフワッとしてるけど、西尾の肌感としてはもう実際に作り始められるくらい具体化できてる状況。ただし現実の大規模データが手元に来てから作り始めたい
手元でプロトタイプをガッと作ってから/kouchou-ai/tools/あたりに入れるイメージ
(sumino):+1:
テスト戦略について
AIエージェント時代のテストドリブン 
熱い話ではあるが、とても実験的で、具体的にどうするかは見えていない
これは「夢みがち路線」
一方で7月にかけてコードオーナーたちが軒並み多忙になってPRをレビューすることが難しくなる問題を解決するために、テストケースを用意して「テスト通ってるからマージしていいんじゃない？壊れたらその時考えよう」をやりやすくしていく必要はありそう
(パブコメフィルタのことを考えてて3日レビュー依頼に気づかなかった、すみません/今後7月PJで色々走り出すともっと見落としが増えると思う)
これは「泥臭い路線」
「夢みがち路線」を念頭に置きつつ「泥臭い路線」をやるのがいいかな〜という気持ち
extractionの前段階の支援が必要という話もパブコメフィルタと同様にスクリプトで実験してから独立のツールとして/kouchou-ai/tools/に入れるのが良さそうだが、現状は要件が不明瞭なので何をしたらいいんだろうなという感じで止まっている感
なお脳内イメージとしてはextractionのプロンプトは人間に調整させるのではなくLLMが生成したらいいと思う
nasukaさんの例における「ラベリングまで進んでから全部に共通キーワードが入ってたので削るプロンプトにした」パターンはなんかいい感じに解決する方法がありそうね
extractionの再利用がまだ実現できてないか、僕の手元はDevinが作ってるけど、話がでかいのでレビューが捗ってない、テストと鶏と卵の関係か...


ちょうどPRも全部吸収したことだし、えいやと今のバージョンにver2.0とかタグを振ってリリースしてしまうのはどうか
非開発者向けのガイドは「リリースページからzipをダウンロード」に変える
(sumino)やりましょう
貢献可視化する
有賀
自治体対応状況
宇多津町
4/30朝に自治体HPでプレスリリース発表予定
その後、メディアの反応をみて取材対応
安野チャンネルの掲載可なら動画化もあるかも
岩手県、渋谷区、舞鶴市
対応中。
広報関連は、若干7月pjの影響あるかも。
要望
個別の要望は引き続き収集中
宇多津町より下記の要望あり。検討に値するかも
手元のローカル環境に試しに導入する際には、OllamaのローカルLLMを使用した。
ちょっとコード書き換えたらいけましたよ、と。
予想外に強い人がいたケースwww(nishio)
理由は、OpenAIのAPIはクレジットカードがないので不可（クレカがあったところで、サーバーが外国なので利用は厳しい）で、Azure Open AIはまだ契約していないので直近では使用できないため。
無償であることより“サーバーが外国なので利用は厳しい”の方が強いらしい(nishio)
自治体セキュリティ基盤、外部サーバへの接続がdefaultでブロックされてるらしい
こういう自治体は多そうなので、ローカルLLMでお試しできるような仕組みがあると自治体的には手元で動かせるのでよさそうとのこと。
この事例自体がすごく面白い話なので、全体定例で紹介して「無償で使える」をissueにするとtokoroten氏もしくはmtane氏あたりがやってくれそう(nishio)
パブコメフィルタでのtokoroten氏の無償でできるのをみて、いいなと思った
“宇多津町”とは言わずに「いろいろな自治体からの要望で〜〜」という形でコミュニティに共有する路線
anno
6末までに移譲していく感じ
自治体の窓口的には鈴木さんの部下が入ってくれるかも
コミュニティにはもっと顔を出して欲しいけど
開示請求したパブコメデータは守秘義務を負ってるわけではないので広聴AIチャンネルでやっていい

アクション アイテム
リリース作ります(nishio)
国会議員の国政調査権でパブコメの生データを獲得できるかは新井さんに確認(nishio)



2025年4月22日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  

共有事項
角野
メンテナーについて
ohkiさんに就任いただいた
なのくろさんが直近お忙しそうなので、レビュアーにアサインしない前提で動く方が良さそうに思ったんですが、いかがでしょう？ > なのくろさん
(なのくろ) なのくろさんレビューはマストにしなくても良さそう。スピード優先で、壊れてもOKの精神でとりあえず進めていく。
+1(nishio)
stable releaseを作ってもいいかも？or mainとdevelopとか
（時間あれば7月PJの広聴AIの運用についてすり合わせておきたいです）
目的
マニフェストの改善？
集めた声を広聴AIで分析し、改善に有用な示唆を抽出する
（前提、まだかっちり決まっているものはない）
上記以外にもユースケースは色々ありそう
参院選を分析するメディアとして活用する
個別の対談イベントの分析をする
etc
企画は今後考えていく
情報発信・政策立案チームと連携
パブコメ問題を解決できた、というプレスをできると7月PJでもインパクトが大きい
本物のパブコメデータの獲得に時間がかかっているのでGoogle Formなどで自前で集めるのも手なのかも。報告の義務を負わないでいいようないいかたで集めてオープンデータにするとか(nishio)
分析対象のデータ
X？
Youtube？
データ収集の実行者
エンジニア
分析の実行者
エンジニア or 非エンジニア？
プロンプトのチューニングが主な作業になるので、非エンジニアでも可能ではある
広聴AIのプロダクト改善にも繋げられそうなので、（エンジニアがサポートする前提で）できれば非エンジニアの方にやってもらえると嬉しいと思っている。誰がやるにしてもどこかのタイミングで分析実行の担当者は決めたい。
分析環境
Azure
レポートの置き場
Azure
現在のAzureのインフラ構成だとミニマム月1万円程度かかるので、もし選挙期間以降もレポートを公開したいなら、static exportしたものをgithub pagesなどで公開するのもアリかも（ただし手間はかかる）

最初のレポートを作成/公開するのはいつ頃か？
5/8時点でレポートが公開されている必要がある？
ファーストレポート出力までのTODO
(done) Azure環境の構築（昨年使ったAzureアカウントで構築）
レポートに埋め込むOGP画像・アイコン画像・メタデータの作成
データ収集周りのコード整備・APIアカウントの整備
データ取得の実施
レポート出力
西尾
mtane(たねのぶ)さんは近いうちにメンテナにしてもいいと思う
(sumino) 👍
いいと思います！（anno)
有賀
自治体対応の状況
トライアル
宇多津町
トライアル結果を広報にむけて準備中
早ければ来週中にも出せるか。
岩手県
データが出てきて分析中
小中高校生のアンケートのフリーコメントの可視化と比較
渋谷区、舞鶴市
手続き中
進めるなかでの気づき・要望
（※以下は、どれもあまり緊急度は高くないです）
少数のコメントに引っ張られている印象があった（クラスタ内の母数が少ないからか？）
（確認）ラベリングの際に参照する意見はどれくらいの件数を抜粋しているか？
(sumino)階層クラスタマージ時に少数クラスタのタイトルが影響しているかも。件数情報は入れてもいいかも。
件数の増減を調整できるようにすることも一案か？
（参考までに共有）コメントが単語レベルや数文字程度と極端に短い場合に、勝手にAIが内容を補って実質ハルシネーションっぽくなりやすい
これは投入前のユーザー側でデータを確認・クレンジングすべきという整理でも良いが、文字数が5文字以下は意見として取り入れない、などの設定を選べるようにするのも一案かもしれない。
（可能性として確認したい）extractionの際に、複数の意見の間で内容が干渉することはロジック上あり得ないと考えてよいか？
(sumino)内容が干渉する、のニュアンスを確認したいです！例えばどんな事象が起こるイメージでしょうか？
→現時点ではコメント1つ1つをLLMに投げてそれぞれ抽出しているので干渉しないはず
高速化とコスト削減のために将来的には複数個をまとめて入れることを検討している、懸念を把握した
古川
パブコメ元データ整形が進んだ
クラスタリングはこれからやりますが、普通にブチ込むだけでいいのかな？
Q: 人力extractionして人力clusteringの結果物、extractionが二度手間だがこれを入れていいのか？
A: extractionを飛ばして処理するのは理論的には可能だがその実装はないのでそれで良いと思う(nishio)
（有賀さんに質問）自治体からパブコメ処理への要望とかってきてます？
パブコメできるよとは伝えているが、現在の所分析しているデータは住民アンケートなど


根本
情報公開請求
エネ庁...4/30に開示請求決定（部分）予定
環境省...確認の電話が来ました。開示はできそう、ただし時間かかりますとのこと(個人情報の黒塗りに時間がかかる)
他...まだ手付かずです、今週末に対応します
安野
総務省でAIを使っていきたい的な発言があったらしく、いくつか取材依頼が来ている
万単位のパブリックコメントが届くからって…「AIで集約」アリなのか　数の重み「ガン無視」を心配する声（安定の東京新聞）
→AIでやってもいいと主張する理論武装が重要になる
開発的には下記の３つにフォーカスしてゆきたいなと思います
有賀さんのリアルなインプットが最優先
アルゴリズムを良くする
デザイナーを入れた画面の整理
組織的にはGYPのリクルーティングと戦力化が必要
特にデザインとアルゴリズムに関しては戦力化が進んでいない？
個別でリクルーティングしても良さそうだと思う
polimoneyのように声がけしても良いかも

メモ



アクション アイテム





2025年4月15日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  
共有事項
角野
共有事項は特になし
meetupの広聴AI関連のログはざっと見ました。盛り上がったようで何よりです！
meetupでGYPの方々をメンテナーにするという話があったと思うのですが、その辺りがどうなったかは気になっています
(nishio)すっかり忘れてた()
大木さん(truego)さん
しろうちさん
安野さんから打診してOKもらって明日に間に合えば明日安野さんから全体に報告の流れ

nishio
(4/12 Meetupでの観察) Windows環境で使おうとしてトラブルになっている人はしばしば「使いたい」のであって「開発したい」のではなかった、なので
Gitを使う必要はないからzipでダウンロード
(最初の一歩としてはAzureにデプロイしたりはせずローカルで動けば十分なので)自分で.envを開いて編集するのではなく、セットアップスクリプトが.envを生成すればいい
という方針のセットアップガイドを作った
この「開発者でないユーザ」の次の欲求は「出来上がったレポートを他の人に見せたい」だと思う
WebUIからの静的HTML出力は既にissueに積んである
しかしそれでは不十分、受け取った人も同様に開発者ではないならローカル環境でhttpサーバを立てるなんてことはできないと想定するべき
この後どうするべきか議論したい
案A: result.jsonを受け取ってホストするだけのサービスをdd2030ドメインで提供
ユーザは「静的HTML出力」するのではなく、「共有」を押して「パスワードを設定」する
(レポート共有部分だけのSaaSみたいなもの。コストがかからないから無償提供でいいのでは)
案B: frontでfetchしないでembedしたHTMLを出力する？
メンテナビリティ的に片方に寄せた方がいい？
「でかいパブコメが処理できますよ」は強みなので捨てたくないな
有賀
自治体対応の状況
無償のトライアル（受託分析）
香川県宇多津町
最初のレポートを提供済み
第一印象は「思ったよりもわかりやすくて驚いている」
今週フィードバックの打ち合わせを予定
広報・報道の活用への話も進めていく予定
岩手県
最初のレポートを提供済み
今週or来週でフィードバックの打ち合わせを予定
渋谷区
実施に向けて手続き中
舞鶴市
実施方向で先方内部で調整中。今後手続き。
要件・要望
4/12meet up
慣れない利用者目線でいろいろでてきた。
無償トライアル（受託分析）より
自治体的には、クラスタを細かく分ける方向の議論が強い。
プロットグラフのUIの対応
ラベルは重なって見にくくならないようにできる？
ドラッグアンドドロップで動かせるとかも？
ラベル全部は表示しない設定を選べるようにする？
東京都の公開レポートがそうなってる
モチベーション
散布図で全体感を把握したいが、現在の20が上限だとトピックが荒すぎる
年齢層・居住エリアなどで比較をみるニーズは結構ある。（現状だと集合が異なるcsvを別々にかける形で、クラスタ分けやビジュアル上の配置も異なるので比較しにくい）
全部まとめてextractionしてクラスタリング・ラベリングまでした後で、特定のフィルタをかけてプロットグラフを表示、件数分布・要約を更新・比較できると理想
レポート上で追加で分析をまわす感じか？
理想は、集団（フィルタリング）ごとの比較を要約できると理想だが、一旦それは急ぎではない
その他、UIの改善
ラベリングは手作業で記載更新できるようにするのも一案だと思う。
(修正履歴を公開するかどうかは悩ましいところ、公開したくない人も多そう。公開しないと「過程の透明性」という主張が弱まる)(nishio)
「Userはこういっている、どうする？」とオープンで議論するのは健全(anno)
なの
広聴AI全然見れてなかった
polimoneyに寄ってました
（おかげさまでUIだいたい出来ました！）
しかし広聴AIはみんなのおかげでフロントエンドも自走できていた感
なのくろ不要説
メモ



アクション アイテム




2025年4月12日 
0412meetup_公聴AI使ってみよう

2025年4月8日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  

前提: 
パブリックな開発定例で話せる内容はそちらで話す。こちらでは、個別の自治体に関する案件などをメインに会話する。
共有事項
角野
衆院選データでの実験
（データが過去案件のもので、公開定例で共有してよいか微妙に思えたのでこちらに記載しています）
衆院選投票終了直後のデータを対象に、広聴AIで可視化
extractionプロンプトは、日テレのものと同一のものを利用し、モデルもgpt-4oを利用
それ以外の処理はデフォルトプロンプトを利用
結果
広聴AIによるレポートはこちら
入力ファイル・設定ファイル・中間ファイルはこちらにアップしてます
第一階層のクラスタ数は10
フロント側で上限が10に設定されているため（この制限はなくした方が良さそう）
TTTCによるレポートはこちら
第一階層のクラスタ数は15
所感
（クラスタ数の違いはあるが）広聴AIの方が第一階層のクラスタ名が抽象的
TTTCでは具体の政党名や事象に対して言及したクラスタ名が多いが、広聴AIだとクラスタ名がふわっとしている
e.g. 「日本の選挙参加と政治意識の多様性」
ラベリング部分のプロンプトの問題なのか、それ以前のクラスタリングの問題なのかは、切り分けができていない。
ネクストアクション
衆院選データの結果の分析とラベリング改善施策の検討
（感覚的にはラベリング周りのプロンプトを改善するだけでももう少しマシになりそうな気がしており、その辺りは早めに試したい。広島案件の問題解決につながる可能性もあるので）
クラスタリング結果の自動評価方法の検討
衆院選データについてextraction後のデータは公開しても良いかも？
クラスタリングの改善にあたって、ベンチマーク的なデータを他の開発者の方に共有できると開発が捗ると思うので
（権利的には問題ない気はしつつ、あんまり大々的にやるとテレビ局側との関係性が微妙になりそうであればやめた方が良さそう）
古川
Devinいいですね。
この会で話すことかわかりませんが、発言メンバー、contributeしてくれるメンバーが少数で固定されてしまった感。
目標の一つであるGYP探し、コミュニティの自走にむけて、エンジニアは軌道に乗りつつあるが、それ以外の人が取り残されてる感がありそうで、もったいない。g0vとかどうなってるんだろう。
12日のイベントでテコ入れできるとよい
(いどばたの対面イベントの雰囲気わかってないので全然杞憂かもしれませんが)
ノンエンジニア巻き込み案の議論
結果のbefore/afterの良し悪し？行政のまとめた結論との比較？
パブコメ原文が出てるものが多くない(これを探してって話はSlackに投げたけど収穫0)
文化庁のパブコメは個人部分をまとめることを放棄してる感(nishio)
原文がなくても、クラスタリングのテストとしては「意見」に抽出した後のデータが得られればいいのでは(nemoto) +1 (nishio)
有賀
自治体対応の状況
無償のトライアル（受託分析）の状況
宇多津町（香川県）
データ受領済み、分析開始。来週中くらいまでには分析のカタをつけたい
対象データは総合計画（町の長期計画）策定の際の住民アンケートのフリーコメント
岩手県、渋谷区、舞鶴市
手続き中・協議中。今月中にできるだけ分析を終わらせたい
早めに分析結果をもとに自治体によい印象を醸成したい
早めに広報・報道でも活用していきたい。
受託分析をやっていてのコメント（※全体の定例に振るものは明日議論で。）
クラスタリングの上位階層は最大40個とかでよいのでは
2050東京戦略も30個でレポートに載せている
いったんコードをいじれば済む話であれば、なおすところ教えてください㎜
extractionのプロンプトはデフォルトでなおした方がよさそう。修正案
静的ファイルのアウトプットはうまく動いていない？
トライアルの自治体に手触り感をもってもらううえでは結構大事なので、できれば今週中にもほしいくらいで…（私がみている範囲では一番優先度高いです。）
レポートを全画面表示したときに、入ってはいけないテキストが表示されてくる？
（表示の話だけですが）「パブコメモード」ではなく「分析過程をcsv出力」などでよいのでは
windowsローカル環境でうまく動かない問題（文字コード問題？結構エラーでとまる。出力の内容もちょっとおかしい？extractionがうまく動いていない？）
優先順位でいえば、Azure環境の方が優先という考え方でよいと思う
一方で、ためしにwindowsローカルで触ってみるユーザーは多そう
機能は遅れていたとしても、エラーなく動くようにだけできるとやさしそう
安野
基本機能は揃ってきたので、次は下記の２つかなと思います
アルゴリズムを良くする
特に有賀さんのリアルなインプットを受けてゴリゴリ良くしていきたい気持ちがある
とにかくパブコメデータを入れながら、人間の整理より良くなっているよねということを示す
デザイナーを入れた画面の整理
組織運営的には12日の1 day hackathon用のissueをたくさん見繕っておきたい気持ちがある
Devinをreferalで売り続けることでDevin永久機関をだな
安野さんの大事だと思うものを一歩前進させられるのはとてもいいと思う、着手の仕方がわからないとかで踏み出しにくい問題が、動くかどうか試すところから人間の仕事になることで、ぜんぶがgood first issueになる(nishio)
西尾
いどばたに参加してて思ったんだけど政党に売り込む上で「ChatGPTの政治利用NG」が障害になる可能性がある？Geminiへの切り替え機能が必要？
→やった方がいい、後でnishioがissueに記録しておく
(情報整理)OpenAIが使えないケースのケア · Issue #255 · digitaldemocracy2030/kouchou-ai
https://github.com/digitaldemocracy2030/kouchou-ai/issues/255
今週日曜までぱつり気味
根本
パブコメ、エネ庁も開示請求しました（いつかえってくるかはまだわからない）
→出すのはどんどんやって、コストが高くなりすぎるなら「ふざけんな」と社会運動(anno)

https://www.nikkei.com/article/DGXZQOUA128RU0S5A310C2000000/
並行してリクエストして省庁ごとの反応速度の差を可視化しちゃおう()(nishio)
なの
PRをレビューする係になっている
みんな素晴らしい・・・！！！
なのくろさんの負荷を分散する +1 (nasuka)
タイミングをどうする
GYPの人をハッカソン4/12までにメンテナーに昇格でいいんじゃない？(nishio)
4/11の全体会でなにかやる？


メモ



アクション アイテム





2025年4月1日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  

共有事項
角野
やったこと
python側のCI整備（テスト）
レビューにそれなりに工数がかかるのでテスト/CIを整備
今後は機能開発系のPRを送る際はなるべくテストコードもセットで実装してもらう運用にしたい
コントリビューションガイドの改善
streamlitのものが良さそうだったのでそちらを参考にガイドを修正
e.g. Issueに:+1:リアクションつけてね、実装に着手する前にメンテナーや他の開発者のリアクションを待ってね、等
レビュー実施
着手中
Azureストレージ連携
今週中にはPRを出せると思います
やっていくと良さそうなこと
Azure上で本番運用をできるようにする（=ストレージ連携）
パブコメデータでの検証
リアルなデータがまだ入手できないので、ダミーデータを作って実験する？
アルゴリズム改善
色々アプローチはあるが、広島のデータがもらえるならそれで階層クラスタリングの改善案を実験できると良さそう？
ねもと
広島県Update
テストデータで「広聴AI」をいじってみている（庁内の業務改善アンケートが対象）
(sumino) ↑アクセス権限がなかったのですが、我々はアクセスできないやつですかね？
広島県からの質問で答えられなかったもの
質問
「初期ラベリング」→「統合ラベリング」の処理の違い（「初期ラベリング」は意見1つに対するラベリングをしている？それとも2つ（ミニマム）が対象？）
(sumino)
 広聴AIでは二階層のクラスタリングを行っており、タイトル生成は以下のように行われています
初期ラベリングでは二階層目の細かいクラスタに対してクラスタ情報（タイトルと説明）を生成
統合ラベリングでは、二階層目のクラスタ情報を踏まえて1階層目のクラスタ情報を生成
「クラスタ名が綺麗に出ていない」をどのあたりに感じているかをもう少し詳しく知りたいです
クラスタ内部のデータ点とタイトルが整合しない、という話なのか？それともクラスタ間で似通ったタイトルがついてしまうのか？（or その他）
場合によってはアルゴリズムレベルでの改善を検討する必要があるかもしれません
現状はラベルを生成する時に外のクラスタの情報を持ってないからプロンプトレベルでは無理
1階層目 or 2階層目のどちらか？（どちらも？）
2階層目なら初期ラベリング、1階層目（だけ）なら主に統合ラベリングのプロンプトが影響します
濃いクラスタは二階層目のクラスタをフィルタして表示しているため、濃いクラスタの場合は初期ラベリングが影響します
2階層目のラベルのデータがしれると良さそう？(nishio)
2階層目にAが複数あって1階層目でそれが他のクラスタに割当たってるなら1階層目でもAが複数になりうる、現状はこっちかな？
2階層目で細かく分かれているなら逆に1階層目が「AとBとC」みたいなタイトルになる、これも「よくない」と言われうる
階層をどこで切るかで変えられる
今の内部データはこんな感じになっている


 背景
クラスタ名が綺麗に出ておらず、どこをいじるかを模索している。クラスタ数をいじるのはやるとして、それ以外にプロンプトをいじるとすると「初期ラベリング」「統合ラベリング」それぞれを操作した場合に何がどう変わりうるかを知りたい
ふるかわ
やったこと
パブコメモード搭載！！
具体的には、管理者のみ以下が可能に
AI抽出された議論・ラベルと元のコメントデータを紐付け（CSV出力用データのみ）
CSVデータのダウンロード
これにより、東京都の懸念（公平性担保のため、「目を通さない」というのは自治体としてNG（小林））にも一定対応
今後の方向性
パブコメモード関係
CSVフォーマットの改善
現在は、comment-id, original-comment(これ名前変えた方がいいかも), arg-id, argument, category_id, categoryを表示。要約が変なことになっていないかをチェックできる。
広島、東京都などからフィードバックいただけると嬉しい
回答素案作成機能
最終的にRAGを活用して回答案まで作れるとよさそう
とりあえずChatGPTに回答作成してもらう一般論回答機能から始める？
元コメントの表示を散布図にも広げる？
案１：管理者限定にする？
案２：コメントごとに表示可能かどうかのフラグをつける？
表示するかどうかで出力するデータを変えないといけない(データにあって表示だけしないのはNGなので)(現状は出力されてない)(nishio)ですよねえ（ふるかわ）
その他
バックエンドの処理速度を改善したい
リリースノート的なものがあると嬉しいかも
あるが
共有事項
無償のトライアルは最大下記3つの自治体で動きそう
岩手県：知事からゴーサイン済み。実務作業にはいる。
香川県宇多津町：町長の最終確認のみ。データ数396件、過去の総合計画策定の際の住民アンケート
渋谷区（来週打ち合わせ予定）
できるだけ4月中に分析を進めて、早めに広報を進めたい。
トヨタとも打ち合わせ済み。進め方は先方で検討中。データの取得自体が6月頃予定なので、少し時間かかるかも。
静的ファイル生成: makeで叩く形のがpushされたみたい、実装済みだがまだ試してない(nishio)
なのくろ
低浮上ぎみ
PR&Merge は滞り無く
めっちゃUpdateされてます・・・！！！
にしお
社内運用が始まって色々使ってみるアイデアやデータが出てきている、今後やっていき
デプロイを最新版に更新することはできた、レポートが消えた(想定通り)、レポートの復元をするのが次の優先タスク
まだ本格的な開発はできてない
あんの
活躍している人は？→shingo_ohkiさん、しろうちさん、ふるかわさん
メンテナふやす？→まだ早いイメージ
4月末ぐらいにメンテナどうですかと聞きにいくイメージ
インターナルの上手い使い方→自治体の話、HRの話
進捗はオープンの方に持って行った方がいい

メモ



アクション アイテム
（角野）ゼロ選挙のデータを入れて出力してみる



2025/03/28 GTTヒアリング

セットアップ時、Azure環境下でやったこと
ScatterとTurboが二種類ある？
Turboが上手くいかなかった
OSSとしてTurboの立ち上げのしやすさというのは課題がある理解
Scatterの方はそんなに課題はないかなと思っています
Turboはアプリがそもそも立ち上がらなかったよ
zscalerというクラウドプロキシーを挟んでやっている
そこまで時間はかからなかった。小林さんの稼働1日いらないぐらい。
新しいバージョンはMakefileで3つのサーバ(管理画面、分析API、結果閲覧画面)が起動するようになった、こういう方向性はそちらに取ってやりやすくなるか？
最新版へのコメント
Kocho-ai
必要だと思い、GUI画面を作っていた。これDBはある？（小林さん）
Consoleどうなってる？（小林さん）
TTTCに入力されたconfigファイルを保存している。DBは今後対応したいと思っている。Consoleは未完・完了の二つにしている（角野）
DBにしてしまうと他の人が見えてしまうためcsvにした（小林さん）
認証問題ありますよね（斉藤さん）
件数多くなると出てくる問題
今3万件より多い件数を入れることがあるが、速度が課題。並列化したりしているが解決できていない（小林）
3万件のデータで1時間かかる、ちょっと短くなると嬉しいは嬉しい。これ以下にはならなかった
Extractionを並列化するのはありかもしれない。そうするとRequest数を減らして早くできるかもしれない（角野）
データセットが変わった場合は未着手、データセットが同じで見せ方を変える時は途中から処理を開始することを考えている。Extractionから変わると難しそう（角野）
フロント側が重かった、最終的にはクラスターごとのマップを除く形にした（1万件を超えると表示できなくなっていた）
フロント。件数が増えると点が増えすぎてカーソルが合わない問題があった。「レポートで10件」と書いてあるが、点が重なってしまっていた。都がやる場合、「コメントが全部でないと公平制に欠ける」と言われてしまう。
同じアプローチをとった。階層図アプローチは（なのくろ）
View側の機能が増えたときに、どのくらいの意見（量）まで分析できるのか気になります（亀山さん）
今は13万件のデータまでやっている。（小林）
13万件だと処理に2.5時間（小林）
Step2の埋め込み時にRAMのスペックが良くないとembeddingの途中で落ちてしまう（小林）
メモリで持たない方法（恐らくembeddingを逐次的に保存するようにしていた？）も考えたが、結局クラスタリングでも同じことが起こるため結局メモリを上げた。コンソールみている感じ20GB使っているため、32GBメモリがあると良さそう（小林）


周知されるほど意見数が増えてくる。捨てることは公的機関としてはできない。アテンションが集まると処理すべきデータが増える（亀山）
30万件程度のデータで試してみると良いかも（安野）
クラスタリング結果に納得感は感じられたか？
初期フェーズ、プロンプトを決めていった（小林）
一番頑張ったのが抽出部分。幅広に取ったので除外要件（リハックのコメントなど）設定に苦労した。最初にフィルターをしてコメントを取る選択肢もあったが、都としてはそこをやるべきでないという判断だったため、除外要件としてのプロンプトに注力した（小林）
ここのプロンプト試行錯誤は結果的に好評だった。内部だけではなくメディアからも「あ、ちゃんとやってるんですね」と言われた（小林）
全体版を20-30パターン程度用意して、一番良さそうなものを作った。オフレコです（小林）
同じプロンプトで何回も出した
その後、出てきた結果を戦略の柱（29個）に分類するアルゴリズムを用意して分類した。こちらもアウトプットを数種類作り、部局の人に選んでもらった。（小林）
２９個を分ける
29の柱の概要を書いてLLMで分類した。「その他」も含む30のカテゴリを作った（小林）
ハルシネーションはなかった。「あれ？」と思ったものをみにいったら実際にあった（小林）
政策部局とやり取りした内容
各局からは特段強いフィードバックなかった（小林）
「出したくない単語を変えたい」というのが政策部局からの要望。特にクラスタの概要説明。ピックアップした5つのコメントも吟味した。点をみると出てしまうが、ピックアップするものに気をつけた（小林）
プロンプトにはかけないので、JSONパラメーターをいじることになった。概要⇆ピックアップするコメントの行き来をしていた（小林）
レポートも、代表性のあるコメントを出すのが良いかと思い、新verでは落としている（安野）
代表的なコメントはニーズである。5-10程度みたい的なコメントもあった（小林）
その他
「濃いクラスター」の活用方法に苦労した。結果的に全部のクラスタを一覧にして「みといてね」になった。（小林）
グルーピングを2にしたら、大量の意見が出てきた（小林）
公平性担保のため、「目を通さない」というのは自治体としてNG（小林）
関心度的なものをクラスタにつけてフィルタリングするのはどうか？（角野）
試したが、関心度にあまり納得感がなく結局全てのクラスタを確認した（小林）
戦略時に使う粒度感とは違ったのかもしれない（安野）
ある事業の開始前・開始中・終了後に声の変遷をみていくことをしたいと思っている。時系列的なデータを取る。BI的な機能があると嬉しい（亀山）
Power BI Exportなどは良いのでは（安野）
Docker
あんまり行政機関だとニーズなさそうですけど、、GTTだと基本的にTerraformでプロビジョニングするようにしているので、HCLのソースコード上に埋め込めるリポジトリのURLがあると展開しやすいと思ってお聞きしておりました。（斉藤）
フロントエンドの構成が難しい（斉藤）
クラスタ数の妥当性はあるのか？（小林）
人間がみて良い感じでやっている笑（安野）
階層性を持たせ、異なる粒度で見られるようにするというので対応してみている（角野）
トピック推定→コメント割り当てるというようなやり方もありえる（角野）
東京都内の市区町村から話が来ている（小林）
TTTC結果と既存政策とを対応づける作業どうやったか
「こんなクラスター作れます？」的なリクエストにどう応えたか
上記の計画を踏まえてアップデートして欲しい内容







2025年3月25日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  


共有事項
角野
やったこと
PR merge/環境整備等
UIや環境構築関連のイシューを中心に解消されている
大きめのトピックとしては、コマンド1発でAzure環境をさくっと立てられるようになった
現状認識
精力的なコミッターの方が何名か出てきている（truegoさん、shgtkshruchさん等）
一方で、解決されているイシューはフロント方面に偏っている
バックエンド側はそもそもイシュー起票が少ないという話はあるので、意識的にイシューを起票していく（宣言）
データサイエンス関連はそもそもイシューを起票していない（タスクの抽象度が高くイシューにできていない）が、そもそもslack内の自己紹介等を見ていても人材が少ない印象
特にデータサイエンスは、LLMを使える人はいそうだがクラスタリングやデータの可視化に詳しい人はあまりいなさそう
(nishio) 僕が「(情報整理)Azureについて #80」でやったみたいに課題意識だけの抽象的issueを立てて徐々に分割すると言う手もあるし、僕とnasukaさんとでこの後ブレストして作ると言う手もある
アプリとしての品質は徐々に上がってきているが、レポートの品質（クラスタリング等）についてはきちんと検証されていない状況
やっていきたいこと（人材）
バックエンド人材の巻き込み
バックエンドのイシュー起票を増やす & slack内での声がけ
もくもく会の実施
NLP/データサイエンス周りのコミッター候補を増やしたい
一旦角野の所属コミュニティ（言語処理学会、kaggler界隈）でも告知してみようと思います
やっていきたいこと（開発項目）
レポート品質の改善
グラウンディング
クラスタ説明文に根拠となるargument（もしくはcomment）を埋め込み、説明文の説得力を補強する
ファクトチェック
個々のargumentのファクトチェックを行いハルシネーションを防ぐ
openai apiがweb検索と連携したので、個別の意見に関するファクトチェックがしやすくなっているはず（どこまでワークするかは要検証）
クラスタ品質の自動評価
クラスタリングアルゴリズムの品質評価を自動化・効率化したい
タイトル・説明・所属データの一貫性はLLMである程度は自動評価できるのではないか
-> （APIコストはかかるが）LLMによってクラスタリングの質を自動評価する枠組みを実装したい
LLMベースドな分類の実装
柔軟かつ小規模なデータでも動作する分類ロジックを構築する
データから自動で抽出したトピックによる分類
ユーザーが定義したトピックによる分類(東京都でニーズのあったやつ)
（idobata likeな分類を広聴AI側に実装するイメージ。既存の分類とどう整合させるかについては考えて切れていない）
（全体的に、いきなり実装に着手するというよりはスクリプト等で実装してアウトプットを検証してから本体に手を入れるイメージ）
+1(nishio)
レポート品質以外だと以下は優先度が高そう？（明日夜の開発定例でも触れられると良さそう）
元コメントの表示機能
レポートの説得力を担保する上で必須
単一ページの出力機能
静的サイトとしてホスティングする上で必須
+1👍(nishio)ゆう猫さんからもどうホスティングしたらいいんだろうと聞かれた。ほづみゆうきさんも自分が作ったレポートをスクショで公開しているけど、静的HTML出力が可能になれば実物自体を公開してくれるんじゃないか。「みんなが使ってレポートを公開している状態」は認知を集める上でも改善点を見つける上でも好ましい 
相談したいこと
コードオーナーを増やしたい
なのくろさん/角野の2人だけだと、互いのコードを互いに見なければならない（が、角野はtypescriptに詳しくなく、なのくろさんもpythonに詳しくないのでほぼ形骸化している）
また、お互いのPRを見れる人間は一人だけなので、レビュワー側が忙しいとちょっとしたドキュメント改善等のPRでもなかなかmergeされない状態になってしまう
(nishio)なのくろさん/角野さんの更新はセルフマージでいいのでは？それで壊すのが不安なら「人間が見ること」で担保するのではなくCIテストで担保した方がいいのでは(そしてテストケースを作ること自体がいいコントリビューション機会になるかも)
コードオーナーになるのはやぶさかでないがあんまりレビューできないとおもう(nishio)
なのくろさんのTSと角野さんのPythonはセルフマージでもいいのでは(anno)
truegoさんは1~2week後にはレビュワー権限をつけていいのでは(anno)
truegoさんの過去の貢献は何か、を可視化できるといいな(nishio)
なのくろ
今週は少し減速していた
OSS化を果たせたので、いい意味でバトンタッチできたとも言える
GYPっぽい人達も見えつつある
PRのレビュー＆マージだけは即時対応している（はず
デザインについては甘めに見てる（機能追加優先）
レビュー＆マージのルール
角野さんが課題提起していただいているのと同意見
現状は main ブランチへの merge にPRレビュー必須にしてある
フロントのPRはなのくろ、サーバーのPRは角野さんがそれぞれ見ている
角野さんのPRは形骸化させちゃっている(レビューせずにApproveしている)
ルール緩和するかコードオーナー増やすと良い説is大賛成です
次週以降の動きとか
きっと広聴AIは大丈夫だな！！！（と楽観視してます）
３月下旬は本業荷重、４月は見える化にフルコミット予定
GYP！GYP！GYP！
にしお
AzureのAPI側は動くようになった、そしてそれをやってる間にtruegoさんがMakefileを作ってくれた、1回試した時はまだ動かなかったけどその後のやりとりを見ると動くようになったらしいので今後試す
gcloudのkouchou-ai(id:shotokutaishi)でGoogle Sheets API, Google Drive APIを有効にし、サービスアカウントを追加しました
slacklogexporter@shotokutaishi.iam.gserviceaccount.com
SlackログをGoogle Driveに書き込むため
ここでやるか新しいプロジェクトを作るかはちょっと迷ったんだけど、Google Spreadsheetから読めるようにしたいというIssueがあったなと思ったのでここに生やしました
サービスアカウント経由なら、対象のSpreadsheetが公開でなくても、閲覧権限者として上記メアドを招待すれば読めると思う(要実験)
やっていきたいこと
まずは事例が増えることが大事だと思っている、Azureで動くようになったので社内の人に使わせたい
データサイエンス系Issueを増やしていくことをしてもいいけど、LPの改善とかコミュニティ内の情報流通の活発化とかをしてスキルの高くない人の活動の余地を広げるの方が優先度は高いという認識
あるが
共有事項
問い合わせがあった自治体8つ（既に動いている広島の次にくる自治体群）と先週・先々週で一通り面談。関心が高い自治体もあるが、エンジニアの確保などがハードルになってるところもおおい
有賀（または有賀の会社）にてデータを受領して広聴AIで分析して返す無償のトライアルサービスを実施予定。昨日8自治体に案内済み。（提案書）
既に岩手県、滋賀県からは反応あり。もう1-2は反応くるのでは
条件は、4-5月までの短期で分析まで終わらせて、広報に協力いただくこと。
システム実装的には、このサービスを回すうえで必要な機能と、分析品質に関する裏側については、可能な範囲で準備いただけるとたすかります。
特に、HTMLで分析結果を返したいので静的ファイルの出力はおねがいしたいです。
広島以外の自治体はそれくらいの温度感なので、自治体ユーザーのコミュニティへの巻き込みは、早くやっていきたいが、いますぐという感じではなさそう。
(nishio)ところで広島はCode for Japanが伴走してるんです？
(nemoto)東さんに入ってもらっています
ねもと
共有＆質問
3/28（金）09:00-09:55　GovTechTokyoヒアリング at Teams
特に聞きたい項目あれば教えてください（現時点での想定は以下です）
セットアップ時、Azure環境下でやったこと
新しいバージョンはMakefileで3つのサーバ(管理画面、分析API、結果閲覧画面)が起動するようになった、こういう方向性はそちらに取ってやりやすくなるか？
政策企画局とやり取りした内容
TTTC結果と既存政策とを対応づける作業どうやったか
「こんなクラスター作れます？」的なリクエストにどう応えたか
上記の計画を踏まえてアップデートして欲しい内容
最新版へのコメント
クラスタリング結果に納得感は感じられたか？
LT企画したら登壇していただけますか？ハッカソン（4/12に声かける？）に参加していただけますか？


直近の稼働
4月3wまで本業がパツパツです、すみません（それ以降動きます）
ふるかわ
パブコメ出力、作業中。CSVダウンロード＆元コメントの表示
ちょうどよいデータが欲しい。。
(nemoto)来週、エネ庁40000件が手に入るかどうかわかります...
パブコメ荒らし対応？
データが来てから、いろいろ実験していけたら。
パブコメの元データが公開されている事例がないかSlackで聞いてみる！

メモ
静的HTML出力はどういう状態？
next exportはかつて可能だった
サーバに問い合わせるかどうかの分岐のダブルメンテが負担
ここまでは速度重視のために一旦削除されている状態
今は落ち着いたので改めて作ってもいいのでは
LLMやデータ分析ができる人が少ないのでは問題に関しては、パブコメ開示請求データが来たら解析してみたい人を勧誘しやすそう(nishio)
kaggler界隈など興味を持つ人はいるのではないか？

アクション アイテム
静的HTML出力のissueを作ってなのくろさんがいなくても実装できるかトライを促す？(nishio)
https://github.com/digitaldemocracy2030/kouchou-ai/issues/53





2025年3月18日 | 広聴AI開発週次定例
参加者:   Hiroshi Nemoto Nasuka Sumino  

共有事項
角野
公開に向けた準備を実施
デモ環境構築
ドキュメンテーション
その他細々とした実装修正
今後の運用について
開発をどう回していくか？
新しく入った方にどこまで任せるか悩ましいと感じている
特に、チーム安野内でなる早でやりたいことについて、無償のコミッターにどこまでお任せするかが悩ましい
（いどばたの方ですでに議論されたものがあれば把握しておきたい）
(青山さんが強いのでみんな忘れてるかもだけど社会人経験のない大学生なのでむしろ支援が必要な気がする。mainに直接ガンガンcommitしてるのか〜と思った)(nishio)
（安野）当面は、ミッションクリティカルな部分は内部（角野・なのくろ）のコミッターがやる
優先Issueとそうでないものを振り分けた上で、優先Issueにはこの場（もしくはコミッター含めたパブリックな場）でアサイン・期限を決める？
会議体としては、この場は残す
パブリックな会だと離しにくい会を話す場として残す(人事とか、非公開の自治体や政党との関係の話)
開発タスクの優先度・振り分けなどの議論はパブリックな場を設けてやっていく
定例の時間を何時にするか（平日夜が良さそう。別途調整）
レビュー・マージ体制
一旦はなのくろ・角野のどちらかがmergeする
コードオーナーをその2人に設定する
将来的には、GYPを見つけてコードオーナーも追加したい
最終的な権限は安野が持つ
パブコメ周りについて
3月末にデータを貰える予定とのことなので、
機能面での課題感
リモート環境の構築が大変
クラウド等でのインフラ構築に手間がかかる。知識がないとハードルが高い。
renderやherokuくらい気軽にアプリを立ち上げられるようにしたい
以下の2ステップが必要と考えている
1.外部ストレージ(S3等)連携の実装
現在は解析結果のファイルや個別のレポートの状態を管理するファイルが、APIサーバーのマシンにのみ存在する
このため、fargateやapp runner等のフルマネージドなコンテナデプロイサービスでバックエンドデプロイができない
インフラをコード化する上では、なるべくコンテナ化してデプロイできるようにしておきたい
2.インフラのコード化
terraform等でインフラをコード化しておき、AWS等のクラウドのアカウントがあればコマンドを打つだけでデプロイできるようにしたい
テンプレート
レポートのexport機能がない
TTTCにはあった、レポートをhtmlで出力する機能がないのでここはサポートしたい
ローカルでhtml出力 -> htmlをwebサーバーでホスティングして、組織内部/外部のユーザーに共有するというユースケースはニーズがあるはず
西尾
Azureを学びながらぼちぼちやってます(まだ問題を切りわけられるところまでいってないのでしばらくぼちぼちやります)
なの
祝！１ｓｔリリース！
公開準備
細々としたやついろいろ
OSS化に向けたIssue作成とか
名前
広聴AIに決定→諸々修正
組織移植
安野事務所→デジ民2030に大移動した
OSS化後対応
リポジトリ各種設定
GitHub Flow にした
コミュニティマネジメント
なのくろ手が回ってない感、得意な人だれかｗ
Issueベースでの開発に移行済
滞り無くPRをレビューするのが主な作業になりそう
課題を把握して課題をメンテする人が必要そう
根本
GovTechヒアリング　3/28（金）09:00-09:55で決まりそうです
安野
いかにしてデジ民SlackからGYP（ガチでやる気パーソン）になりうる人を特定し、コミットをいただくことができるかを考える
参考：https://scrapbox.io/nishio/%E3%82%AC%E3%83%81%E3%81%A7%E3%82%84%E3%82%8B%E6%B0%97%E3%83%91%E3%83%BC%E3%82%BD%E3%83%B3


メモ
自治体から、Azure OpenAIを使いたいという話が出ている
Azureのニーズは2つある
Azure OpenAIを使いたい
基本はこちら
Azureで環境構築したい
Azureの話は2つに分解してそれぞれissueにしてもいいかも、やっときます(nishio)
前者は実装済だけどその後追加されたバリデーションで弾かれる認識
shingo_ohki = 電通デジタル = truego = Code for JapanのSlackでTTTCサービスをやってた人 ということか(理解)(nishio)
プロダクトマネージャーロールを誰かに任せたい
当面は明確にPdMを設けるわけではなく、定例の場で優先順位をつけるという立て付けにする
PdMがいないので、探しているよという話はパブリックに共有していき、GYPを探していきたい
パブリック定例でイシューを共有 -> やってくれる人を探して棚卸ししていく
自治体からこういうイシューが上がっているという話を根本さん・有賀さんから共有してもらう（）
(こういうものを非公開の状態で議論するためにやはりclosed internalの会議は必要そう)(nishio)
パブリック定例の場で、やりたいタスクを取ってもらった上で、取った人は自分をasigneeにする
インストラクションを作って共有する（角野）




アクション アイテム





2025年3月11日 | TTTC開発 weekly
参加者:   Hiroshi Nemoto Nasuka Sumino

共有事項
角野
バックエンドのインフラ構築done
フロント・バックエンドが疎通し、一通り利用できるようになった
その他細かい実装修正を実施
コメントidのuuidv4化対応
フロントに元コメントのデータを送らないようにする対応（X APIの利用規約回避のため）
このため、shotokutaishiではargumentの元ポストが現状確認できない状態となっている
README追記
最低限、dockerを知っているエンジニアであれば起動できる情報（環境変数をセットしてdocker compose upしてね）を記載
TTTCを参考に、免責事項も追記
大規模言語モデル（LLM）にはバイアスがあり、信頼性の低い結果を生成することが知られています。私たちはこれらの問題を軽減する方法に積極的に取り組んでいますが、現段階ではいかなる保証も提供することはできません。特に重要な決定を下す際は、本アプリの出力結果のみに依存せず、必ず内容を検証してください。
TTTCに対するクレジットも記載
このプロジェクトは AI Objectives Institute が開発した TttCを参考に開発されており、一部コードを流用しています。ここに原作者の貢献に感謝の意を表します。
tttc -> 正式名称にする
オープンソースソフトウェアのxxxxx
ライセンスに基づいてソースコードを一部活用し、機能追加や改善を実施した
なの
フロント改修
CSVデータ
「comment」というカラムだけが必須になった（他は無視）
サーバーへは「id(uuidv4), body(comment)」の２つを送信
現状はオプショナルなカラムは捨てている(これが良いかは問題)
サーバ側はそれを絞り込みや追加情報として使う可能性がある
JSONに変換するところまでフロントでやって送る想定
“id”だと衝突する？→”予約語です”という？ぶつかったらリネームする？(id→original_idとか)
コメントデータ
件数だけを受け取るように修正済み
UI改修
前回のフィードバックはだいたい反映済み
一時的に単一ページの export 機能は廃止（追って直す予定）
濃いクラスタしきい値
一旦は「上位20%かつ５件以上」で決め打ち
しきい値を保存できる仕組みができたら追って対応
愛称
FIXしたい (AI広聴 (ai-kouchou)でよいか)
KouchouAI
修正対象
サービス愛称
リポジトリ名
GCPプロジェクト名
他あれば
デモページ
今回のプロダクトは各自治体や政党側でホスティングされる想定
annotakahiro.com にもデモ的なページを用意したい
レポートの発行者を誰ということにするか(画面左上などに表示される)
デジタル民主主義2030
お披露目系
１６日夜収録予定
西尾
試してみている、今週もっとやっていく予定
気づいたことはSlackで共有していく
根本
GovTechTokyo
ヒアリング日程調整中（カレンダーブロックしているところを候補日程として送っている）
広島県
1回目のミーティング 3/12（水）17:00-18:00
有賀さん、ひがしさん（CfJ）、根本で対応
「TTTCセットアップについて相談したい」という話
「3月中に3回やりたい」という要望
GitIDもらえたら限定アクセス出しますよ
Slack案内して、何かあればここに
ほか
こちらのログに集約していきます（今週来週で面談の予定）
自治体検討まとめ
安野
名前現時点版
日本語: 広聴AI
英語: KouchouAI
一般名詞感もある
が、競合もいないので取りに行ってもいいかもという気持ちではある
お披露目系
あとはSmartNews社へのライセンスの話待	ちではある
SN社が間に合わなかったとしても動画としては収録しておくと良いので収録しておく
案件系
広島県がどうなっているか知りたい to hiro41nemo10@gmail.com

メモ
今後の動き
内部、広島県で使うことでフィードバックが出てくるので、モグラ叩き的に課題を解決していく
読売の反AI社説: https://annyotakagmailcom.slack.com/archives/C076XTS142Z/p1741611445828049
行政の人に使ってもらえるイメージを持ってもらいたい -> パブコメ特化の機能を作るとよいのでは？
パブコメ用のフォーマットでレポートを出力する機能を作るなど
アテンションの集まってない自治体はそちらに課題を感じてそう(これはケアしないのも手)
何で試すべきか:
シン東京2050はパブコメとはちょっと違う、純粋にパブコメのデータがいい
開示請求かけたやつは3末以降の予定(認知戦の最前線のデータが手に入るのは熱い、しかも防衛側)
まとめと回答のセットのデータがあると比較可能になる

前回のアクション アイテム
（角野）READMEの記載
セットアップ手順
それぞれのアプリの役割
（安野）annoがSmartNewsに著作権確認
repoのorgは一旦今のままでリリース(将来的にはannoと切り離すが5月まで余裕がある)
（なのくろ）UI改修
sunburst廃止
散布図を2つにする
名称も全体像、濃いクラスタ（仮）にする(「クラスタ」より「かたまり」とかのほうがいいかも(nishio))
（なのくろ）濃いクラスタの定義(パラメータ設定)を分析者が事前に行う
GETパラメータで渡すのが良さそう
（角野）ドメイン発行
tttc-staging.takahiroannno.com


アクション アイテム
repo/その他リソースをKouchouAIに合わせる



2025年3月4日 | TTTC開発 weekly
参加者:   Hiroshi Nemoto Nasuka Sumino

共有事項
角野
docker化
docker compose upでローカルで立ち上げられるようになった
動作には支障ないが微妙に問題が残っていて、コンテナ環境でのみ一部の画像やメタ情報が取得できない（コンテナ利用することに起因する問題で、原因はわかっているが解決に少し時間がかかりそう）
このまま公開でOK、クリティカルではない
issueだけたてとく
python環境のインフラ構築
GCEで環境構築を実施
フロント側との疎通確認がまだ取れていないが、API単体ではGCE上で動作することは確認済み
ドメイン取得 -> LB設定 -> APIサーバーのhttps化を実現すれば疎通できる（はず）
takahiroanno.comのサブドメインを切ってDNS
tttcstaging.takahiroanno.com
Cloud Runでも試していたが、コンテナのライフサイクル的に厳しそう
リクエストの処理を完了する前にコンテナが落ちる
公開前に、追加で以下はやった方が良さそう
READMEの記載
最低限、セットアップ方法とそれぞれのserviceの役割くらいは書いた方が良さそう
細かいUI調整
階層クラスタの設定部分等
直接このPJとは関係ないですが金曜にこちらのイベントに登壇してきます
関西Kaggler会 交流会
TTTCの紹介・事例紹介（M1等）がメイン
shotokutaishiの紹介も少ししようと思ってます
西尾
特にこのプロジェクトでの作業はしていないが今後しそうなことの情報共有
Audrey+Glenが5月に来日するタイミングでサイボウズの青野社長と鼎談をするのだが、その時までに以前社内の研修で使ったTTTCをブラッシュアップしておくとAudreyにサイボウズでの事例として話して記事化できていいんじゃないという感じになってて、だったらタイミング的には「安野チームの作った最新版で生成し直して見ました」という流れが良さそうだなという気持ち
3/10以降の予定です
なのくろ
フロント開発は一旦落ち着いてきた感 is ある
濃いクラスタリングUI対応済み
export 機能対応済み
APIと接続できれば表示できる状態
APIへ疎通できないと表示できない
ビルド自体は一応通るように修正済み
安野
名前を考えたい
要件
とにかく死ぬほどわかりやすくしたい
老若男女が良さそう感を持つやつがいい
ちょっとダサくてもいい
残タスクを把握したい
なのくろさんの稼働に関して、SmartNewsが支払っている&著作権は持ちたいということなのでクリーンにやるならCLAの著作権者が機会経営とSmartNewsの共同著作になる
annoさんがSmartNewsと話すタスク
エネルギー関係の情報開示請求の現状について知りたい
開示請求は提出済
3/31 +2-3日までに開示請求可否の通知が出る（築地宛に郵送）
提出されたコメント数は41,421件より多いらしい（意見が41,421だがコメントはそれよりも多い）
紙できても美味しい？→1枚10円かかる
開示手数料
GTTの人とのヒアリングの状況について知りたい
候補確認させてください（3つぐらい送る想定）
3/17（月）17:30-18:30
3/19（水）10:30-12:00, 19:00-20:00
3/21（金）11:30-13:00
3/24（月）10:30-12:00
新slackに移行したい
ここに関係者（各政党、各自治体）を全員突っ込んでいく想定
https://join.slack.com/t/w1740803485-clv347541/shared_invite/zt-310tfm4r5-Mt4ju0ZEp4adwDtUNRm3eQ 
public/privateの区別をつけたい
publicなのは全世界に公開する向けとする
privateな安野slackもしばらく存続させる
privateで書き込むべきことがあればそこに書き込む
public側は完全中立にしたい。安野の政治活動系の話もしない。
「最初にこれを読んで同意して」的なペライチが必要そう(nishio)
故にエネルギー関連パブコメでの実験みたいな話はprivate側でやった方が安全そう
このあたり線引が難しいのでここでも少し揉みたい
オープンなデータorそうでないデータとかでしょうか？（根本）
将来的には１００％新slackの方で開発関連の話が行われている方が望ましい
ただし安野事務所関連で活用する話とかは安野事務所slackで行われていたい
docsは当面privateじゃないか(nishio)
publicはGitHub Issueでいいのでは
organizationはtakahiroannoなのか？新しいorgでは？(nano)
将来的にはそう(anno)
安野の独裁ではない構図にしたい(3~5月)
時間があればネクストステップ相談したい
基本はGTTヒアリングで出てきた問題を潰すのがよい
認知戦に強いよといえるとなおよい（エネルギーパブコメで目指す）
実験はして見たい、UMAPだと広がる可能性あるんだよなw(nishio)






前回のアクション アイテム
（なの）名前を考える（shotokutaishiでいいの？）
（なの）npm exportを実装する
（なの）ライセンスとコントリビュータアグリーメントの整理
コントリビュータアグリーメントは都知事戦のもので良さそう https://github.com/takahiroanno2024/poster-map/blob/main/CLA.md
ライセンスはTTTCを踏襲してAGPLにする
（角野）dockerで立ち上げる環境の整備
composeで立ち上げられるようにする
（なの）濃いクラスタのフロント実装
（角野）バックエンド（python）サーバーを立てる
ここまでやると、csvアップロード -> レポート出力までの一連の操作がWebアプリ上で完結する

メモ
	濃いクラスタのパラメータ設定が難しい問題(nishio)
		プリセットを決めることもデータセットによって異なるので難しい（？）
			気にせず上位10%, min=5でいいかも？
		レポート作成する人がdefaultを決めて、見る人はチェックボックスを押すだけで見れたらいいのでは
	plotlyのツールチップを改行する方法がわからない問題(nano)
	絞り込んでツリーにすると付箋みたいでかわいいけど、文字が見にくいな...(nishio)
	3x2の6通りあるのは難しいのでは(anno)
		4種類のビューの切り替え、1段階、がいいのでは
		ファーストビュー「散布図」→次が「濃いところだけ散布図」
	サンバースト必要か説
	階層を2階層にするか
defaultは2でよさそう
詳細設定から設定UIを削るのはしない(詳細設定は通常非表示だから、わかる人は切り替えて試したい)

アクション アイテム
（角野）READMEの記載
セットアップ手順
それぞれのアプリの役割
（安野）annoがSmartNewsに著作権確認
repoのorgは一旦今のままでリリース(将来的にはannoと切り離すが5月まで余裕がある)
（なのくろ）UI改修
sunburst廃止
散布図を2つにする
名称も全体像、濃いクラスタ（仮）にする(「クラスタ」より「かたまり」とかのほうがいいかも(nishio))
（なのくろ）濃いクラスタの定義(パラメータ設定)を分析者が事前に行う
GETパラメータで渡すのが良さそう
（角野）ドメイン発行
tttc-staging.takahiroannno.com




2025年2月25日 | TTTC開発 weekly
参加者:   Hiroshi Nemoto Nasuka Sumino

共有事項
角野
濃いクラスタ抽出の実装
前回のFBを踏まえたデモを実装
https://broadlistening.onrender.com/0218%E3%83%95%E3%82%A3%E3%83%BC%E3%83%89%E3%83%90%E3%83%83%E3%82%AF%E5%8F%8D%E6%98%A0
概要
「濃いクラスタだけ抽出する」チェックを押すと、最下層のクラスタは濃いクラスタだけ表示される
散布図も同様に濃いクラスタだけに変化する
クラスタの最下層に対して、更に元のargumentを追加
shotokutaishi向けにも上記を実装
クラスタの密度情報をpythonのresultファイルに追記
csvアップロード機能におけるバックエンドのpythonサーバーを実装
レポート作成のリクエストを受けて、レポート作成処理（anno-broadlistening）を実行する等
（その他の部分はなのくろさんが爆速で実装してくださったので、全体感についてはなのくろさんに記載いただけると助かります！ nanocloudx@gmail.com ）
一通り動作するようになったが、データの永続化周りに課題があるのでそのあたりを次に対応する想定
ローカルや、EC2等で動かす分には問題ないが、Cloud Run等で動作させるとレポートの結果ファイル(resultのjson)や状態管理を行っているファイルが失われる問題がある
外部storage（GCS等）等に上記を保存できるようにする想定
3/7~15あたりは言語処理学会参加などであまり動けません
なの
shotokutaishi リポジトリの作成
クラウドにホスティングするだけで使える状態を目指す
Node.js と Python が動く環境であればGoogleCloudでもAWSでも他でも可
環境URL貼っておきます（すみの）
管理画面: https://shotokutaishi-client-admin-135149897998.asia-northeast1.run.app/
レポート共有:https://shotokutaishi-client-135149897998.asia-northeast1.run.app/ 
フロントエンドとバックエンドを疎結合にしてAPIで会話するようにした
レポート作成者画面とレポート閲覧者画面を分離して用意した
CSVアップロード機能のUI実装済
濃いクラスタのフィルタリングは実装中
従来通りの npm export をサポートするか議題
現時点での shotokutaishi は未対応（サーバーありきでの設計）
python エンジニアがサーバーを建てずに確認したい需要がありそうという話
anno-broadlistening と shotokutaishi の UI を両方メンテするのはちょっと大変
→ shotokutaishi にも export 機能もたせちゃう？


安野メモ
(1)GTTの人とのアポ取り状況
いいたいことがたくさんあるようなのでユーザインタビュー的に話を聞く会
(2)リリース日程を決めてそれまでにやるべきネクストステップを固めたい
MVP1.0
ライセンスとコントリビュータアグリーメントをおいてリポジトリを公開
タスクを明文化すると外部の人の力を借りられる
Slackに招待するほどでもないがGitHubでコミュニケーションして協力できるといい
(3)とも関連
MVP2.0
政党に「こっちを使うといいよ」と言える形にしたい
3月中旬ぐらいに出せると良いと思う
Q: 自治体や政党がホストできたほうがいい？
OSSをホストしたいニーズはどれくらいあるのか、MUSTなのか？
こちらでホストして使わせる形にする？
export機能をつけるのか、出力されたHTMLをCtrl+Sでいいのでは？→これでいいならもうできてるとも言える
濃いクラスタはv2.0、これが従来のものとの違いのアピールポイント
バージョンアップしたら基本「CSVから処理し直して」
そのほうが政党側としても理解しやすい、政党側でマイグレーションは無理
我々の開発速度的にも、安定するまでは従来バージョンとの互換性は気にしないのがよい
(3) どうやって開発を加速させるか、外のボランティアのチカラを使えるか考えたい
(4) 攻撃されたパブコメの収集をできるとよさそう
安野方式が従来パブコメよりロバストであることを主張できると良い

メモ
shotokutaishiをpublicにするか？privateにするか？
publicにする前にライセンスまわりを整理する必要がある
整理するまではprivateにしておく

アクション アイテム
（なの）名前を考える（shotokutaishiでいいの？）
（なの）npm exportを実装する
（なの）ライセンスとコントリビュータアグリーメントの整理
コントリビュータアグリーメントは都知事戦のもので良さそう https://github.com/takahiroanno2024/poster-map/blob/main/CLA.md
ライセンスはTTTCを踏襲してAGPLにする
（角野）dockerで立ち上げる環境の整備
composeで立ち上げられるようにする
（なの）濃いクラスタのフロント実装
（角野）バックエンド（python）サーバーを立てる
ここまでやると、csvアップロード -> レポート出力までの一連の操作がWebアプリ上で完結する



2025年2月18日 | TTTC開発 weekly
参加者:   Hiroshi Nemoto Nasuka Sumino

共有事項
角野
クラスタ重心のみを散布図に可視化した結果の共有
以下のプロセスで、クラスタ重心のみの可視化を実施
HDBSCAN
-> クラスタ重心のみでUMAP
-> クラスタ毎のデータ件数に基づいて重心をバブルチャートで可視化
グラフのURL
https://broadlistening.onrender.com/%E3%83%90%E3%83%96%E3%83%AB%E3%83%81%E3%83%A3%E3%83%BC%E3%83%88_hdbscan
密度スライダーを用いた散布図表示
概要
1000クラスタでk-meansクラスタリング -> 表示するクラスタを密度に基づいてスライダーで調節できるようにしたデモを作成
モチベーション
HDBSCANでは濃いクラスタの抽出はできるが、網羅的なデータのマッピングはできない（捨てるデータが存在する）
データ捨てず、なおかつ濃い部分の示唆を可視化できないか？
-> 密度のスライダーを用意し、上位N％のクラスタのみを表示する機能を用意
スライダーを調節してもらうことで網、羅的に確認することも可能にしつつ、（2次元空間上で）濃い部分だけを可視化できるようにした
散布図で元データを表示するため、UMAP後の二次元空間上でクラスタリングしている
URL
https://broadlistening.onrender.com/%E6%9C%80%E4%B8%8B%E5%B1%A4%E3%81%AE%E5%AF%86%E5%BA%A6%E8%AA%BF%E7%AF%80
元データのtreemap上での表示
treemap上で元データを確認できるようにする筋を検証するため一応確認したが、データ点が増えすぎるとサブクラスタ（この場合はargument）の文字が読めなくなり厳しい
見せ方に工夫の余地はありそう
文字数制限する、改行する、argumentだけ色を変えるなど
できるかどうかは要検証

レポートの見せ方のオプションについて
論点
クラスタリング時の次元をどうするか？
選択肢
2次元
元の埋め込み次元
可視化方法をどうするか？
選択肢
treemap/sunburst
散布図
元のargument
クラスタの重心（元の埋め込み次元でクラスタリングする場合はこちらのみ利用可能）
色々試した結果として、以下の2つのオプションを用意するのが良さそうに感じている
①濃い示唆を得るためのレポート
位置づけ
意見が集中している部分から示唆を得るためのレポート
主に意見を分析する人間・分析結果を見て意思決定する人間が使う想定
クラスタリング
HDBSCAN + 元次元のクラスタリング
可視化方法
クラスタ重心のplot？
(treemap等は不要？)
濃い部分に着目している良さが薄れる
やるとしたら「100件のクラスタ」を元データにして再度クラスタリングしてtreeを作るとかかな〜(nishio)
これは「やるとするなら」の温度感なので、具体的なニーズが見えてきてからでいいかなという気持ち
②全データ点のマッピングを表示するレポート
位置づけ
全データ点がどのようなグループに分類されているかを漏れなく把握するためのレポート
意見を分析する人間・分析結果を見て意思決定する人間・意見を出した市民が見る想定
クラスタリング
2次元でクラスタリング
可視化方法
 元データを散布図で表示
treemap/sunburst
論点
①②をそれぞれどのような関係性で見せるか？
全部のデータをパーティショニングしたものはこうで、その中で濃い部分を
①について
パラメータはデフォルトで入れる
「市役所のおっちゃんが使えるビュー」であることを重視すると「高次元だから全然違う」よりは「濃い一部分を抽出したのがこちらです」の方が関係性がわかりやすくて理解しやすいのではないか(anno)
二回層で全データの階層クラスタリング + その中で濃いクラスタ抽出をするくらいであれば市役所のおっちゃんも理解できるのではないか？

政党と役所で違う
政党はワンイシューを見つけたいので濃いクラスタがみたい
テレビも面白い声を拾いたい
政党は「小さな声をちゃんと聞いているぞ」を出したい
網羅したい
市役所はまちまち
都は細かめにクラスタリングして既存の分類とのマッチングを(人力で？)頑張った
テレビも全体感を出したい
アイキャッチで使う10-15, 実務で使う数十、濃いクラスタが見えると良い（nemoto）
10-15から更に30程度にブレークダウンして深堀りできると良い
作業者が「見つけること」の支援と、知見を大衆に「伝えること」の支援がある、という話
濃いクラスタの一覧は「見つけること」の支援
「伝えること」の支援としては「全体像」からズームインして行く感じでないと唐突に小さいクラスタの話になって視聴者がついてこれないのではないか
まず②を実装していく
②のクラスタリングで、示唆の抽出が不十分などの具体的な声が出てきたら①の方向性を検討する

なのくろ
フロント開発は概ねできた感
https://broadlistening-report-135149897998.asia-northeast1.run.app/
追加でこれ欲しい！というのがあれば対応します
フッター(クレジット)は草案を作ってあります、必要であれば commit ready です
にしお
ちょっと実験したくらい
2/25の講演で言及した方がいいこと的なのがもしあれば(と思ったがそもそも安野さんの公演が直前にあるからいいか)




前回アクション アイテム
なのくろ：Vercelのアカウント
安野がクレカ情報をいれる
なのくろ: 階層クラスタのドリルダウン的な部分の実装をやる
角野:クラスタリングの実験
k-meansで1000クラスタ -> 密度の低いクラスタを捨てる -> マージしていく
HDBSCANベースでクラスタリング（密度の低いクラスタは捨てる） -> 階層のマージをしていく
上位階層だけデータを保持し、下層だけ濃いクラスタ抽出を採用する
2回別のアルゴリズムでクラスタリングする必要がある


ネクストステップ
②の実装
安野理想
階層は二段階の想定
e.g. 10クラスタ、1000クラスタで分類している
treemap等での二段階目のクラスタ表示は、濃いクラスタだけ表示したい
散布図上では、濃いクラスタの部分だけ色を微妙に変えることで、一段目のクラスタと二段目の濃いクラスタ部分を一画面で同時に把握できるようにしたい
スライダーだと市役所のおっちゃんは使えないかも説
ボタンひとつで濃いクラスタとそうでないmapを切り替えられるようにするくらいなら使えるのではないか？


とはいえこれは結構大変なのでは？遠目の理想では？早く政党に濃いクラスタへの注目を体験させたいのでは？
意外と簡単にできる？→Plotlyで簡単にできるかどうかの検証が必要そう
boldにした部分だけであれば、恐らくそこまで時間はかからないのでまずはこれを実装していく
問題ないか安野に確認する
csvアップロード
チーム安野内部用のものとして作る

メモ
密度のスライダー調節は、実務者にはあると良さそう




アクション アイテム




2025年2月6日 | TTTC開発 weekly
参加者:   Hiroshi Nemoto Nasuka Sumino


共有事項
階層クラスタリングについて（安野記載のものを転記しつつ追記）
良いところ
画面がだいぶモダンになった。軽くなった気もする
サンバースト、ツリーマップは面白い
見やすくなったし、デジタル民主主義2030感も出せているのでとても良い
改善したいところ
データ面
ツリー構造のクラスタで、AIだとあまりクラスタごとの違いが明瞭ではなかった
上のクラスタと下のクラスタに同じ名前がついていることもしばしば
プロンプティングで調整すればある程度は解決できそう(sumino)
例えば、上位階層のクラスタはより抽象的な名称にするように指示するなど
現状でも、上下で同一の名称にはしないように指示は記載している
ただし、そもそもサブクラスタが一つしか存在しないようなケースだとサブクラスタの名称をそのまま使うようになっている。見栄えは悪いが仕方ない？
このクラスタの中身についての文字レポートがあるといいのかもしれない
濃いクラスタ一覧が見たい気持ちがある
一番上の階層の次には濃いクラスタ一覧を見せてもいいのかもしれない？
ソフトウェア面
CSVアップロードがあると良さそう（大規模熟議の方と同様、ユーザーサイドで色々試せるので）
大規模熟議の方とうまく統合する道もあるのかもしれないと悩みはじめた（重要）
今後の進め方に影響してくると思うのでこのあたりは確認しておきたいです(sumino)
アプローチや目的は微妙に違うんだけど目指しているところは似ていたりする＆CSVアップロードなどのインフラ面は共通化できるので馬力が出るかもしれない？
ユースケース毎に、分析テンプレート的なものを用意できると良いかも？（sumino）
トピックごとの賛否を可視化するテンプレート、既存のTTTC的なテンプレート等
そもそも現状ではユースケースがそこまで定まりきっていない・適切な出力フォーマットも見えていないので、もう少し見えてきてからの話ではある
フロントエンド面
全体
解説文を表示するクラスタ階層を切り替えられるようにしたい（現状は最上位のみ表示）(sumino) 
サンバースト
文字が縦になると見にくい
すごい感の演出として、このビジュアライゼーションはあっても良さそう
ツリーマップ
有望な香りがする
どういう投稿があったのか、もう少し中身が知りたくなる
特に最下層の葉まで到達したときに何も見えない
ホバーしたときにクラスタの解説文を見れると良さそう？探索的にクラスタを見ていく時に中身の内容把握が早くなりそう(sumino)
同じことを思った 、サンバーストもホバーで説明出れば斜めで読みにくいとかを気にしなくて良くなりそう(nishio)
クラスタのタイトルが長すぎると、文字が小さくて見えなくなる（sumino）


角野
階層クラスタリングを実装
細かい部分の調整はまだ必要だが、バックエンド・フロントエンドともに、アウトプットを確認できるレベルの実装が終わっている状態
preview pageにレポートを記載
https://github.com/takahiroanno2024/anno-broadlistening/commit/6df2ac26620a6941a7f1c27c699657c931525dbd 
aipubcom/M1/衆院選（政策・後半）のレポートを出力
一旦角野のnetlify環境でホスティングしているが、他のオプションを検討したい
無料でやるならgithub pages？
ただしgithub上でソースが公開されるので内部previewには若干不向き
netlify（or 他のホスティングサービス）を機械経営で契約し、使い回す形にしても良いかも
これ今後いろいろなものを政党や地方自治体に非公開に見せたりもすると思うのでやり方整えといた方が良さげ(nishio)
クラスタリングアルゴリズムについて
現在はUMAPした空間上でk-meansを行っているが、UMAPによって歪んだ空間上でまとまりを作るため、本来は距離的に遠いはずのデータがまとめられてしまう欠点がある
-> 一応UMAPする前の空間でのクラスタリングを試してみたが、解釈が難しくなるため元の方が良さそう
k-meansで作成したクラスタを密度に応じて削減する件は未検証
「濃いクラスタ抽出」と比較すると、既存のクラスタアルゴリズムはクラスタ間の差分が薄くなっており、似たようなクラスタ名が生成されるケースがある
-> optionで、濃いクラスタ抽出ベースの階層クラスタリングを入れても良いかも
既存のアルゴリズムに対するトレードオフとして、以下がある
捨てるデータが出てくる
計算量が多く実行に時間がかかる（マシンによっては動かないかも）
それぞれのクラスタの説明文(aipubcom)
濃いクラスタ抽出
https://shotokutaishi-preview.netlify.app/aupubcom_dense/ 
階層クラスタ（最下層）
https://docs.google.com/spreadsheets/d/1bPx-SDRLJyyGBvKARLD6jKLkgihBhRfC8va8kEAdn4Q/edit?usp=sharing 
プロンプトに微妙な文言（高山さんが指摘されていたやつ）が入っていたので修正
コミット履歴に入ってしまっており、何か事後処理をすべきか（炎上リスクがどの程度ありそうか）を確認したい
Google Driveでプロジェクト用のディレクトリを切りたい
データ等の置き場にしたい
安野アカウントでそれ用のディレクトリを切ってもらうのが良さそう？
西尾
Code for Japanのチャンネルで「Dockerfileを改善したから見て！」という人yoshinori77が出現(知らない人)
見てみたけど影響範囲が広いからそのままmergeするのもどうかなという感じ
一方でOSSとして外部の人のコントリビュートを受け入れる姿勢を示すのも大事か〜という気持ち
角野さんと少し議論していくつかのissueに分割し、実験的ブランチとしてマージした
「エンドユーザがDockerHubからimageをpullしてrunするだけでつかえる」を目標にする
だが今すぐはできない(pullしたあとの自前データの入れ方が未整備)
pushの仕方も未整備(我々がローカルで作業すると公開してはいけないデータをうっかり混入するかもしれないのでCIでpushするようにしたい)
なので「エンドユーザの利便性」の前にまず「CIする」をサブゴールとし、CI実験ブランチで上記の仕組みを整えていく
今後はCode for JapanのチャンネルではなくGithubのIssuesで議論するように誘導するのでCode for Japanをウォッチしなくても大丈夫

なのくろ
階層型クラスタリング向けフロントエンドを実装
既存の next-app を使わずにフルスクラッチした
もはやTTTCではない
考えうる限りでは既存より遥かに高速になっているはず...！
デザインは都知事選の資料を参考にそれっぽい感じにした
実施する自治体の情報を流し込めるようにした、今後便利かなと
表示文言について
ブロードリスニングの解説欄を用意したのでちゃんと書きたい
分析手順の説明とかちゃんと書きたい
デプロイ先について
main, develop, PRそれぞれで確認環境が構築されると嬉しい
個人的には Vercel 契約してもらえると嬉しいかも（↑を全部自動でやってくれる）
今後のリポジトリ運用とか
もはやTTTCではないってのが実際そうなので...
(Special thanks Talk to the City なのは間違いない)
next-app と report の２つが存在するのは正直紛らわしいかも
hierarchical_report 版は別repoでよいのでは？説
別repoを立ててリブランディングしていく
tttcはREADMEでreferenceしておく & AGPLライセンスにする必要はある
名前を決めないといけない
コードネーム: shoutokutaishi


論点
ホスティング先どうする？
Gcloud
Netlify
Vercel
ネクストステップの優先順位
csvアップロードで（非エンジニアでも）shotokutaishiを回せるようにしたい




決定事項



メモ




アクション アイテム
なのくろ：Vercelのアカウント
安野がクレカ情報をいれる
なのくろ: 階層クラスタのドリルダウン的な部分の実装をやる
角野:クラスタリングの実験
k-meansで1000クラスタ -> 密度の低いクラスタを捨てる -> マージしていく
HDBSCANベースでクラスタリング（密度の低いクラスタは捨てる） -> 階層のマージをしていく
上位階層だけデータを保持し、下層だけ濃いクラスタ抽出を採用する
2回別のアルゴリズムでクラスタリングする必要がある



2025年1月28日 | TTTC開発 weekly
参加者:   Hiroshi Nemoto Nasuka Sumino


共有事項
角野
階層クラスタのレポート出力機能実装に着手
階層クラスタ: 5, 10, 20, 40, 100のように、クラスタを階層的にサブクラスタに分割したもの
当初は「濃いクラスタ抽出」の実装を検討していたが、その後検討した上で以下の理由により階層クラスタを実装する方向にシフトした
クラスタ数を増やせば「濃いクラスタ抽出」に近い示唆を得られる
詳細を分析したい/ざっくり概要を見たい場合のどちらのユースケースでも利用できる
現在実装に着手している状況
python側は今週前半には実装完了する見込み
（別途フロント側の実装が必要）
デモアプリ
本格的な開発に着手する前に、streamlitで実装イメージを内部共有するためのデモを作成
https://broadlistening.onrender.com/%E9%9A%8E%E5%B1%A4%E3%82%AF%E3%83%A9%E3%82%B9%E3%82%BF%E3%83%AA%E3%83%B3%E3%82%B0
階層クラスタの見せ方はこのデモではツリーマップで実装しているが、sunburstなど他の見せ方もありそう（&データ構造的にも変わらない）ので、ここの見せ方はもうちょい検討する
参考: https://plotly.com/javascript/sunburst-charts/ 
クラスタリングアルゴリズムについて、オリジナルと新アルゴリズムでは以下の違いがある
オリジナル
スペクトラルクラスタリングを利用
新アルゴリズム
k-meansでクラスタを構築 -> umapの２次元平面上でのクラスタ中心の距離に基づいてクラスタをマージ
TODO: 過去の衆院選データに対して、濃いクラスタリングと上記のk-meansのどちらも試した上でアウトプットを確認すると良さそう？
クラスタリングやマージ方法には他にもオプションがあるが、現状は計算量の少ない上記で行っている
階層クラスタの次にやっていくことについて確認したい
階層クラスタ進め方
k-meansベースで一旦実装を進める
クラスタリングアルゴリズムの比較をしたい(anno)
であれば、追加でHDBSCANでも階層クラスタリングを実行し、k-meansベースのものと出力を比較する
角野さんなのくろさんはk-meansベースで進めると良いと思う(nishio)
HDBSCANでk-meansの結果と同じフォーマットで出すのはできると思う

共通の過去のデータに対して適用し、アウトプットを比較する
M1データ: 広島がやりたいのはマーケティング的なのでこれに近い
衆院選・政策クエリ: 政局よりは政策の方が良さそうという考え
aipubcom?: まさにパブコメ自体


レポートが出力できた段階でホスティングする
やると良さそうなこと
パブコメデモの作成？
過去のパブコメについて、ブロードリスニングツール使って要約するデモを作成する
そもそも既存のTTTCの出力フォーマットで十分なのかという部分から確認が必要（場合によっては、パブコメ用の新規フォーマットを用意する）
aipubcomブランチに入れてるデータでいけるかも(nishio)
-> このデータでレポートを出力する
取り急ぎ、古川さんとディスカッションをセットするのが良さそう？
過去のパブコメに対してTTTCでアウトプットを出力しつつ、どういった要素が足りないか・使えるものにするためにどのようなフォーマットにしていくかを議論する
トレーサビリティ強化
X API経由のデータについて元ポストを辿れるようにする
環境構築の簡易化
プロジェクトゴールである、「東京都がやったモデルをエンジニアなしで地方自治体が実行出来るようにする」を達成する上で重要
とりあえず、docker化・quickstartの整備あたりを行うのが良さそう？
quickstartの方が費用対効果が高そうなので、まずそちらからやっていくのが良さそう
(nishio)Dockerは詳しくないのでわからない、環境を揃えることができればquickstartのドキュメントはかきやすくはなると思う、レポートができた後、多くの人が他人にレポートを非公開でシェアするところで躓くと思うのでそのガイドが必要そうかなと思っている(どんな選択肢があるだろうか...)
ドキュメントの置き場所はGitHubにdocs/とか作ってMarkdownで良いかな？
フロントの軽量化
2万件〜程度の件数になると重くなる問題を解決したい
ただ、今後階層クラスタのレポートが主流になる場合は解決する意義が薄くなる（そのあたりの様子を見たあとに着手するほうが良さそう
重たい主原因が「散布図をクラスタごとに書くと30万個DOMがある！」なので階層クラスタリングで格段に改善しうる
引き続き旧形式のレポートのニーズが高いのであれば優先度は高い。その場合は、GTTでやっていたように全画面地図をトップのもの以外は削減するオプションをつけるのが良さそう
西尾
もはやTTTCではないのだから新しい名前をつけた方がいいのでは
聖徳太子、わりと伝わりやすかったらしい

安野




メモ
スケジュール
2/9 広島 MTG
2/11 玉木さんyoutube
2/20 吉村さん

アクション アイテム




2025年1月20日
現状の課題
最優先
１）東京都モデルを広島県で出来るようにする
HDBSCANの濃いクラスタを集めるやり方を可視化できるようにする
＝行政職員が見ている結果とユーザーが見ている結果を同一にする
クラスタ数の最適化をやりやすくする


データを入れてから処理を実行するまでの一連の流れをエンジニアなしで出来るようにする
２）Grounding、Sensemaking

プロジェクト概要

プロジェクトの前提
Talk to the Cityは都知事選の時から実際に活用が進んできた”最も社会実装に近いプロダクト”になっている
対日本テレビでは選挙特番に活用
対東京都ではシン東京２０５０に活用され、実際の行政の現場でパブコメの新しいスタンダードにできるレベルの成果を出せた
１）今まで拾い上げられていないような声を拾えた
量は２桁変わった
質としても、パブコメでは得られない１０代〜３０代の声をより多く拾えた感触
２）行政職員のコストも（初回である大変さはあるものの）、大量の文章を読み解くコストは下げられそうな所感があった
３）結果、行政が気づけていなかったアジェンダを発見できた
”ブロードリスニング”を体現しており、社会からのニーズも引続き強い
広島県など他地域でも広りそうな気配を感じている
宮坂副知事も「東京で上手くいったら東京モデルとして広げたい」と発現
フィルターバブルの問題が山積する中、『意見の全体像のマップを描くこと』は非常に社会からの要請も強い
ただし、改善ポイントは山積している
データ収集面
どのように幅広いデータを収集するか？　方法論を確立できていない
なんとなく日本だとヤフコメ、YouTube、X、直接投稿の４つのミックスが有用そう
東京都では１０００万円で１万件のコメントを収集した
それらのコメントを集めるため、メディア側のコンテンツも必要そう（生配信ワークショップなど）
アルゴリズム面
disinformationや多数派工作にどう対応するべきか
フィルターの強化が有り得そう
（角野追加）
多数派工作について、ユーザーidが特定できるものについては、id単位で発言をまとめて処理できる機能があっても良いかも
disinformationについては、geminiのgrounding等を活用したファクトチェック機能を実装しても良いかも（参考。これがどこまでワークするかは要検証）
少数意見（N=5~30)だけれども学びのある意見の塊を抽出するためにはどうするか？
西尾さんが試行錯誤中のHDBSCANの手法
https://scrapbox.io/nishio/%E9%AB%98%E6%AC%A1%E5%85%83%E3%83%87%E3%83%BC%E3%82%BF%E5%88%86%E6%9E%90%E5%8B%89%E5%BC%B7%E4%BC%9A
（角野追加）
そもそも誰をターゲットにするかによって最適なアウトプットの形式は変わるのではないか
自治体・政党
政策・施策の策定に反映するために、どのような論点があるかを知りたい・どのような声が多くあがっているのかを知りたい
-> 濃いクラスタ抽出や、sensemaking的なフォーマットでの示唆分析が有効そう
市民
ざっくりどんな論点・意見があるのかを知りたい
-> 今のフォーマットでも一定は満たせていそう？
自分の出した意見がどのように反映されているのかを知りたい
-> 
フロントエンド面
スケーラビリティの問題
インターフェースを洗練させたい問題
複数の可視化方法を入れ替える等
インタラクション面
途中結果を適宜表示できると良い
導入面
エンジニアがいなければそもそも活用を開始できない
クラスタ数の調整など、難易度がやや高い
継続的にデータが増え続ける中で回し続けるようなCIの仕組みが揃っていない
費用面
東京都ではコストパーポスト（１コメントもらうのにいくら広告宣伝費をかけたのか）が１,０００円程度になっている
他の手段と比較して安くならなければならないだろう
（角野追加）トレーサビリティ面
X APIなど、幾つかのAPIで取得・分析したデータに関するトレーサビリティが低い
プロジェクトのゴールの共有
PMFとりたい
東京都がやったモデルをエンジニアなしで地方自治体が実行出来るようにする
どこかの地方自治体が我々のバックアップなしでパブコメをこの形で６ヶ月以内にやれれば勝ちである
ネクストステップ
現状の課題を並べて優先順位をつける
優先順位順に開発をしてゆく
